<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-11T00:00:00Z">2025-09-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrEnv: Transparently Share Serverless Execution Environments Across
  Different Functions and Nodes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ μFork: Supporting POSIX fork Within a Single-Address-Space OS <span class="chip">SOSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Alistair Kressel, Hugo Lefeuvre, Pierre Olivier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-address-space operating systems have well-known lightweightness
benefits that result from their central design idea: the kernel and
applications share a unique address space. This model makes these operating
systems (OSes) incompatible by design with a large class of software:
multiprocess POSIX applications. Indeed, the semantics of the primitive used to
create POSIX processes, fork, are inextricably tied to the existence of
multiple address spaces.
  Prior approaches addressing this issue trade off lightweightness,
compatibility and/or isolation. We propose {\mu}Fork, a single-address-space
operating system design supporting POSIX fork on modern hardware without
compromising on any of these key objectives. {\mu}Fork emulates POSIX processes
({\mu}processes) and achieves fork by creating for the child a copy of the
parent {\mu}process' memory at a different location within a single address
space. This approach presents two challenges: relocating the child's absolute
memory references (pointers), as well as providing user/kernel and
{\mu}processes isolation without impacting lightweightness. We address them
using CHERI. We implement {\mu}Fork and evaluate it upon three real-world
use-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS
worker warm-up. {\mu}Fork outperforms previous work and traditional monolithic
OSes on key lightweightness metrics by an order of magnitude, e.g. it can offer
a fork-bound FaaS function throughput 24% higher than that of a monolithic OS,
and can fork a {\mu}process in 54{\mu}s, 3.7x faster than a traditional fork.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear at SOSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Deterministic Sub-0.5 us Response on Linux through Interrupt
  Isolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03855v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03855v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyi Zhou, Zhili Liu, Shancong Zhang, Jiemin Li, Dengke Du, Mengke Sun, Zhiqiang Wang, Hongyan Liu, Guokai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time responsiveness in Linux is often constrained by interrupt
contention and timer handling overhead, making it challenging to achieve
sub-microsecond latency. This work introduces an interrupt isolation approach
that centralizes and minimizes timer interrupt interference across CPU cores.
By enabling a dedicated API to selectively invoke timer handling routines and
suppress non-critical inter-processor interrupts, our design significantly
reduces jitter and response latency. Experiments conducted on an ARM-based
multicore platform demonstrate that the proposed mechanism consistently
achieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT
configurations. These results highlight the potential of interrupt isolation as
a lightweight and effective strategy for deterministic real-time workloads in
general-purpose operating systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A neural drift-plus-penalty algorithm for network power allocation and
  routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Rashwan, Keith Briggs, Chris Budd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The drift-plus-penalty method is a Lyapunov optimisation technique commonly
applied to network routing problems. It reduces the original stochastic
planning task to a sequence of greedy optimizations, enabling the design of
distributed routing algorithms which stabilize data queues while simultaneously
optimizing a specified penalty function. While drift-plus-penalty methods have
desirable asymptotic properties, they tend to incur higher network delay than
alternative control methods, especially under light network load. In this work,
we propose a learned variant of the drift-plus-penalty method that can preserve
its theoretical guarantees, while being flexible enough to learn routing
strategies directly from a model of the problem. Our approach introduces a
novel mechanism for learning routing decisions and employs an optimal
transport-based method for link scheduling. Applied to the joint task of
transmit-power allocation and data routing, the method achieves consistent
improvements over common baselines under a broad set of scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ObjectReact: Learning Object-Relative Control for Visual Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2025; 23 pages including appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Based Data-Assisted Port-Hamiltonian Control for Free-Floating
  Space Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Eslami, Maryam Babazadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generic data-assisted control architecture within the port-Hamiltonian
framework is proposed, introducing a physically meaningful observable that
links conservative dynamics to all actuation, dissipation, and disturbance
channels. A robust, model-based controller combined with a high-gain
decentralized integrator establishes large robustness margins and strict
time-scale separation, ensuring that subsequent learning cannot destabilize the
primary dynamics. Learning, selected for its generalizability, is then applied
to capture complex, unmodeled effects, despite inherent delay and transient
error during adaptation. Formal Lyapunov analysis with explicit stability
bounds guarantees convergence under bounded learning errors. The structured
design confines learning to the simplest part of the dynamics, enhancing data
efficiency while preserving physical interpretability. The approach is generic,
with a free-floating space manipulator orientation control task, including
integrated null-space collision avoidance, serving as a case study to
demonstrate robust tracking performance and applicability to broader robotic
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object
  Bagging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zhou, Jiaming Qi, Hongmin Wu, Chen Wang, Yizhou Chen, Zeqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Spontaneous Stop-and-Go Traffic Waves: A Bifurcation Perspective
  of A Dynamical Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suzhou Huang, Jian Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a discrete-time dynamical system in a car-following context. The
system was recently introduced to parsimoniously model human driving behavior
based on utility maximization. The parameters of the model were calibrated
using vehicle trajectory data from the Sugiyama experiment. It was shown that
such a system can accurately reproduce the observed collective phenomena of a
more elaborate experiment by Tadaki et al. Once the heterogeneity and noise are
switched off, the model defines a map of the corresponding discrete-time
dynamical system. We first perform a bifurcation analysis of the map by
studying the stability of its limit solutions: a free-flow fixed point and a
stop-and-go quasi-periodic orbit. When the vehicle density is varied, our model
displays a bifurcation diagram qualitatively similar to those found in a class
of optimal velocity models based on an ordinary differential equation approach,
including regimes where one or both of the limit solutions are stable. In a 2D
bifurcation diagram we further demonstrate that imposing a vehicle
density-dependent speed advisory can dissipate the stop-and-go quasi-periodic
orbit. This in turn lays the mathematical foundation for a simple, yet
effective proposal [1] to tame stop-and-go waves, improving traffic flow and
smoothness simultaneously via variable speed advisory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Spontaneous Stop-and-Go Traffic Waves: A Computational Mechanism
  Design Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Shen, Qi Dai, Suzhou Huang, Dimitar Filev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well known that stop-and-go waves can be generated spontaneously in
traffic even without bottlenecks. Can such undesirable traffic patterns,
induced by intrinsic human driving behaviors, be tamed effectively and
inexpensively? Taking advantage of emerging connectivity and autonomy
technologies, we envision a simple yet realistic traffic control system to
achieve this goal. To prove the concept, we design such a system to suppress
these waves while maximizing traffic throughput in the Tadaki setting: a
circular road with varying number of vehicles. We first introduce our driver
behavior model and demonstrate how our calibrated human driving agents can
closely reproduce the observed human driving patterns in the original Tadaki
experiment. We then propose a simple control system mediated via connected
automated vehicles (CAV) whose ideal speed parameter is treated as a
system-level control variable adapted to the local vehicle density of the
traffic. The objective of the control system is set up as a tradeoff:
maximizing throughput while minimizing traffic oscillation. Following
computational mechanism design, we search for the optimal control policy as a
function of vehicle density and the tradeoff attitude parameter. This can be
done by letting all vehicles play a simulated game of CAV-modulated traffic
under such a control system. Our simulation results show that the improvements
in traffic efficiency and smoothness are substantial. Finally, we envision how
such a traffic control system can be realized in an environment with smart
vehicles connected to a smart infrastructure or via a scheme of variable speed
advisory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Robust and Reliable Designs Using the
  Compromised Design Support Problem: A Case Study in Hot Rod Rolling Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Ghasemzadeh, H M Dilshad Alam Digonta, Anand Balu Nellippallil, Anton van Beek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Design under uncertainty is a challenging problem, as a systems performance
can be highly sensitive to variations in input parameters and model
uncertainty. A conventional approach to addressing such problems is robust
optimization, which seeks to enhance design performance by reducing sensitivity
to uncertainty. Alternatively, reliability-based design focuses on optimizing
performance while ensuring that failure constraints are satisfied with a
specified probability. While both methods are well established, their
integration into multi-objective and multi-stakeholder decision-making
frameworks remains a challenging problem. In this study, we extend the
Compromise Decision Support Problem (cDSP) framework to incorporate
reliability-based design considerations and evaluate its performance in
comparison to the conventional robust-based cDSP formulation. The developed
framework has been validated on a multidisciplinary hot rod rolling process
including parametric and model uncertainties. The results compare the predicted
performance under robust and reliable scenarios, validating the efficiency of
the approach in managing uncertainties for complex, multidisciplinary systems.
Specifically, we found that the two methods exhibit markedly different
performance when the predicted performance follows a non-normal distribution, a
situation that arises in non-linear systems with parametric uncertainty. Based
on this insight, we offer guidance to designers on the conditions under which
each method is most appropriate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient and Secure Cloud Control Systems: Advances,
  Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasir Ali, Tayyab Manzoor, Huan Yang, Asif Ali, Yuanqing Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Networked Control Systems (NCSs) have been instrumental in realizing fully
connected and responsive intelligent environments within the context of
real-time virtual control and management. However, traditional NCSs face
considerable challenges in handling the vast amounts of data generated by
large-scale control applications, particularly in terms of data acquisition,
storage, and computational processing. To address these challenges, the
emergence of cloud computing and advancements in control theory have empowered
the new paradigm known as Cloud Control Systems (CCSs). Recently, CCSs have
received substantial attention from industries for their potential properties,
such as large-scale data management, complex computations, and data-centric
optimized decisions. This study presents an extensive review of recent progress
in CCSs spanning over multiple studies published between 2012 and 2025.
Specifically, the focus is on providing a taxonomy of the current findings in
CCS research, encompassing various perspectives, such as its efficient
implementations in industrial automation, security and privacy considerations,
and cloud-based control techniques. Each category is examined in depth through
selected state-of-the-art analyses of different approaches and contrasting
methodologies. Furthermore, we discuss future directions aimed at designing
more efficient and practical CCSs. The insights gained from this study can help
researchers, practitioners, and decision-makers in their domain for effective
CCS design and deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Voltage Synchronization and Proportional Current Sharing of Grid-Forming
  Inverters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianxi Tang, Li Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most previously proposed controllers are analyzed in the
small-signal/quasi-steady regime rather than large-signal or transient
stability for grid-forming inverters (GFMI). Additionally, methods that presume
system-wide data--global measurements and complete grid-model knowledge--are
challenging to realize in practice and unsuitable for large-scale operation.
Moreover, proportional current sharing is rarely embedded into them. The whole
system is a high-order, nonlinear differential system, making analysis
intractable without principled simplifications. Hence, contraction stability
analysis in GFMI is proposed to guarantee the large-signal stability.
Furthermore, a contraction-based controller is proposed to synchronize GFMI.
Additionally, this paper proposes integrating an auxiliary virtual-impedance
layer into the contraction-based controller to achieve proportional current
sharing, while the GFMI retains global stability and voltage synchronization. A
dispatchable virtual oscillator control (dVOC), also known as the
Andronov--Hopf oscillator (AHO) is used to validate the proposed contraction
stability analysis and contraction-based controller with virtual-impedance. It
is proved that the complex multi-converter system can achieve output-feedback
contraction under large-signal operation. Therefore, without requiring
system-wide data, the proposed method offers voltage synchronization,
decentralized stability conditions for the transient stability of AHO and
proportional current sharing, beyond prior small-signal, quasi-steady analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The role of communication delays in the optimal control of spatially
  invariant systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Ballotta, Juncal Arbelaiz, Vijay Gupta, Luca Schenato, Mihailo R. Jovanović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study optimal proportional feedback controllers for spatially invariant
systems when the controller has access to delayed state measurements received
from different spatial locations. We analyze how delays affect the spatial
locality of the optimal feedback gain leveraging the problem decoupling in the
spatial frequency domain. For the cases of expensive control and small delay,
we provide exact expressions of the optimal controllers in the limit for
infinite control weight and vanishing delay, respectively. In the expensive
control regime, the optimal feedback control law decomposes into a delay-aware
filtering of the delayed state and the optimal controller in the delay-free
setting. Under small delays, the optimal controller is a perturbation of the
delay-free one which depends linearly on the delay. We illustrate our
analytical findings with a reaction-diffusion process over the real line and a
multi-agent system coupled through circulant matrices, showing that delays
reduce the effectiveness of optimal feedback control and may require each
subsystem within a distributed implementation to communicate with farther-away
locations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>{\copyright} 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Occupancy-aware Trajectory Planning for Autonomous Valet Parking in
  Uncertain Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Nawaz, Faizan M. Tariq, Sangjae Bae, David Isele, Avinash Singh, Nadia Figueroa, Nikolai Matni, Jovin D'sa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of a 8-bit Wallace Tree Multiplier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Biswas, Jimmy Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAN-Therm: A Lightweight Battery Thermal Model Using Kolmogorov-Arnold
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyoraj Mallick, Sanchita Ghosh, Tanushree Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery management systems (BMSs) rely on real-time estimation of battery
temperature distribution in battery cells to ensure safe and optimal operation
of Lithium-ion batteries (LIBs). However, physical BMS often suffers from
memory and computational resource limitations required by highfidelity models.
Temperature prediction using physics-based models becomes challenging due to
their higher computational time. In contrast, machine learning based approaches
offer faster predictions but demand larger memory overhead. In this work, we
develop a lightweight and efficient Kolmogorov-Arnold networks (KAN) based
thermal model, KAN-Therm, to predict the core temperature of a cylindrical
battery. We have compared the memory overhead and computation costs of our
method with Multi-layer perceptron (MLP), recurrent neural network (RNN), and
long shortterm memory (LSTM) network. Our results show that the proposed
KAN-Therm model exhibit the best prediction accuracy with the least memory
overhead and computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Control of an SIR Model with Noncompliance as a Social Contagion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chloe Ngo, Christian Parkinson, Weinan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and study a compartmental model for epidemiology with human
behavioral effects. Specifically, our model incorporates governmental
prevention measures aimed at lowering the disease infection rate, but we split
the population into those who comply with the measures and those who do not
comply and therefore do not receive the reduction in infectivity. We then allow
the attitude of noncompliance to spread as a social contagion parallel to the
disease. We derive the reproductive ratio for our model and provide stability
analysis for the disease-free equilibria. We then propose a control scenario
wherein a policy-maker with access to control variables representing disease
prevention mandates, treatment efforts, and educational campaigns aimed at
encouraging compliance minimizes a cost functional incorporating several cost
concerns. We characterize optimal controls via the Pontryagin optimality
principle and present simulations which demonstrate the behavior of the control
maps in several different parameter regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Equivalence of Koopman Eigenfunctions and Commuting Symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02437v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02437v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Jiang, Yan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Koopman operator framework offers a way to represent a nonlinear system
as a linear one. The key to this simplification lies in the identification of
eigenfunctions. While various data-driven algorithms have been developed for
this problem, a theoretical characterization of Koopman eigenfunctions from
geometric properties of the flow is still missing. This paper provides such a
characterization by establishing an equivalence between a set of Koopman
eigenfunctions and a set of commuting symmetries -- both assumed to span the
tangent spaces at every point on a simply connected open set. Based on this
equivalence, we build an explicit and convergent formula for the principal
Koopman eigenfunctions defined on the region of attraction of a locally
asymptotically stable equilibrium point, thereby offering a constructive
formula to compute Koopman eigenfunctions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Reachability with Scenario Optimization and the Holdout
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Dietrich, Rosalyn Devonport, Stephen Tu, Murat Arcak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reachability analysis is an important method in providing safety guarantees
for systems with unknown or uncertain dynamics. Due to the computational
intractability of exact reachability analysis for general nonlinear,
high-dimensional systems, recent work has focused on the use of probabilistic
methods for computing approximate reachable sets. In this work, we advocate for
the use of a general purpose, practical, and sharp method for data-driven
reachability: the holdout method. Despite the simplicity of the holdout method,
we show -- on several numerical examples including scenario-based reach tubes
-- that the resulting probabilistic bounds are substantially sharper and
require fewer samples than existing methods for data-driven reachability.
Furthermore, we complement our work with a discussion on the necessity of
probabilistic reachability bounds. We argue that any method that attempts to
de-randomize the bounds, by converting the guarantees to hold
deterministically, requires (a) an exponential in state-dimension amount of
samples to achieve non-vacuous guarantees, and (b) extra assumptions on the
dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Performance Signal Design for Optical OFDM Systems using
  Variational Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam N. Luong, Chuyen T. Nguyen, Thanh V. Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter proposes a design of low peak-to-average power ratio (PAPR), low
symbol error rate (SER), and high data rate signal for optical orthogonal
frequency division multiplexing (OFDM) systems. The proposed design leverages a
variational autoencoder (VAE) incorporating gradual loss learning to jointly
optimize the geometry and probability of the constellation's symbols. This not
only enhances mutual information (MI) but also effectively reduces the PAPR
while maintaining a low SER for reliable transmission. We evaluate the
performance of the proposed VAE-based design by comparing the MI, SER, and PAPR
against existing techniques. Simulation results demonstrate that the proposed
method achieves a considerably lower PAPR while maintaining superior SER and MI
performance for a wide range of SNRs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tannenbaum's gain-margin optimization meets Polyak's heavy-ball
  algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuwei Wu, Jie Chen, Mihailo R. Jovanović, Tryphon T. Georgiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper highlights an apparent, yet relatively unknown link, between
algorithm design in optimization theory and control synthesis in robust
control. Specifically, quadratic optimization can be recast as a regulation
problem within the frame of $H_\infty$ control. From this vantage point, the
optimality of Polyak's fastest heavy-ball algorithm can be ascertained as a
solution to a gain margin optimization problem. The approach is independent of
Polyak's original and brilliant argument, and relies on foundational work by
Tannenbaum who introduced and solved gain margin optimization via
Nevanlinna-Pick interpolation theory. The link between first-order optimization
methods and robust control sheds new light into the limits of algorithmic
performance of such methods, and suggests a framework where similar
computational tasks can be systematically studied and algorithms optimized. In
particular, it raises the question as to whether periodically scheduled
algorithms can achieve faster rates for quadratic optimization, in a manner
analogous to periodic control that extends gain margin beyond that of
time-invariant control. This turns out not to be the case, due to the analytic
obstruction of a transmission zero that is inherent in causal schemes.
Interestingly, this obstruction can be removed with implicit algorithms, cast
as feedback regulation problems with causal, but not strictly causal dynamics,
thereby devoid of the transmission zero at infinity and able to achieve
superior convergence rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fundamental Convergence Rate Bound for Gradient Based Online
  Optimization Algorithms with Exact Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Xinting Wu, Ian R. Petersen, Iman Shames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider algorithms with integral action for solving online
optimization problems characterized by quadratic cost functions with a
time-varying optimal point described by an $(n-1)$th order polynomial. Using a
version of the internal model principle, the optimization algorithms under
consideration are required to incorporate a discrete time $n$-th order
integrator in order to achieve exact tracking. By using results on an optimal
gain margin problem, we obtain a fundamental convergence rate bound for the
class of linear gradient based algorithms exactly tracking a time-varying
optimal point. This convergence rate bound is given by $
\left(\frac{\sqrt{\kappa} - 1 }{\sqrt{\kappa} + 1}\right)^{\frac{1}{n}}$, where
$\kappa$ is the condition number for the set of cost functions under
consideration. Using our approach, we also construct algorithms which achieve
the optimal convergence rate as well as zero steady-state error when tracking a
time-varying optimal point.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Automatic Control</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecture <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combating the Memory Walls: Optimization Pathways for Long-Context
  Agentic LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wu, Can Xiao, Jiayi Nie, Xuan Guo, Binglei Lou, Jeffrey T. H. Wong, Zhiwen Mo, Cheng Zhang, Przemyslaw Forys, Wayne Luk, Hongxiang Fan, Jianyi Cheng, Timothy M. Jones, Rika Antonova, Robert Mullins, Aaron Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of a 8-bit Wallace Tree Multiplier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Biswas, Jimmy Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Optimization Accelerator Framework for Multistate Ising
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chirag Garg, Sayeef Salahuddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ising Machines are emerging hardware architectures that efficiently solve
NP-Hard combinatorial optimization problems. Generally, combinatorial problems
are transformed into quadratic unconstrained binary optimization (QUBO) form,
but this transformation often complicates the solution landscape, degrading
performance, especially for multi-state problems. To address this challenge, we
model spin interactions as generalized boolean logic function to significantly
reduce the exploration space. We demonstrate the effectiveness of our approach
on graph coloring problem using probabilistic Ising solvers, achieving similar
accuracy compared to state-of-the-art heuristics and machine learning
algorithms. It also shows significant improvement over state-of-the-art
QUBO-based Ising solvers, including probabilistic Ising and simulated
bifurcation machines. We also design 1024-neuron all-to-all connected
probabilistic Ising accelerator on FPGA with the proposed approach that shows
~10000x performance acceleration compared to GPU-based Tabucol heuristics and
reducing physical neurons by 1.5-4x over baseline Ising frameworks. Thus, this
work establishes superior efficiency, scalability and solution quality for
multi-state optimization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 page main text, 4 main figures, 2 main table, 3 page supplementary,
  10 supplementary figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonia: A Multi-Agent Reinforcement Learning Approach to Data
  Placement and Migration in Hybrid Storage Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rakesh Nadig, Vamanan Arulchelvan, Rahul Bera, Taha Shahroodi, Gagandeep Singh, Andreas Kakolyris, Mohammad Sadrosadati, Jisung Park, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid storage systems (HSS) integrate multiple storage devices with diverse
characteristics to deliver high performance and capacity at low cost. The
performance of an HSS highly depends on the effectiveness of two key policies:
(1) the data-placement policy, which determines the best-fit storage device for
incoming data, and (2) the data-migration policy, which dynamically rearranges
stored data (i.e., prefetches hot data and evicts cold data) across the devices
to sustain high HSS performance. Prior works optimize either data placement or
data migration in isolation, which leads to suboptimal HSS performance.
Unfortunately, no prior work tries to optimize both policies together.
  Our goal is to design a holistic data-management technique that optimizes
both data-placement and data-migration policies to fully exploit the potential
of an HSS, and thus significantly improve system performance. We propose
Harmonia, a multi-agent reinforcement learning (RL)-based data-management
technique that employs two lightweight autonomous RL agents, a data-placement
agent and a data-migration agent, that adapt their policies for the current
workload and HSS configuration while coordinating with each other to improve
overall HSS performance.
  We evaluate Harmonia on real HSS configurations with up to four heterogeneous
storage devices and seventeen data-intensive workloads. On
performance-optimized (cost-optimized) HSS with two storage devices, Harmonia
outperforms the best-performing prior approach by 49.5% (31.7%) on average. On
an HSS with three (four) devices, Harmonia outperforms the best-performing
prior work by 37.0% (42.0%) on average. Harmonia's performance benefits come
with low latency (240ns for inference) and storage overheads (206 KiB in DRAM
for both RL agents combined). We will open-source Harmonia's implementation to
aid future research on HSS.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Theory and Practice of Concurrency in the
  Entity-Component-System Pattern 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.15264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.15264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Redmond, Jonathan Castello, José Manuel Calderón Trilla, Lindsey Kuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version (with appendices) of the OOPSLA 2025
  paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Data Structures and Algorithms <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Additive Approximation Schemes for Low-Dimensional Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashanti Anderson, Ainesh Bakshi, Samuel B. Hopkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of fitting low-dimensional embeddings to
high-dimensional data. In particular, we study the $k$-Euclidean Metric
Violation problem ($\textsf{$k$-EMV}$), where the input is $D \in
\mathbb{R}^{\binom{n}{2}}_{\geq 0}$ and the goal is to find the closest vector
$X \in \mathbb{M}_{k}$, where $\mathbb{M}_k \subset
\mathbb{R}^{\binom{n}{2}}_{\geq 0}$ is the set of all $k$-dimensional Euclidean
metrics on $n$ points, and closeness is formulated as the following
optimization problem, where $\| \cdot \|$ is the entry-wise $\ell_2$ norm: \[
  \textsf{OPT}_{\textrm{EMV}} = \min_{X \in \mathbb{M}_{k} } \Vert D - X
\Vert_2^2\,.\] Cayton and Dasgupta [CD'06] showed that this problem is NP-Hard,
even when $k=1$. Dhamdhere [Dha'04] obtained a $O(\log(n))$-approximation for
$\textsf{$1$-EMV}$ and leaves finding a PTAS for it as an open question
(reiterated recently by Lee [Lee'25]). Although $\textsf{$k$-EMV}$ has been
studied in the statistics community for over 70 years, under the name
"multi-dimensional scaling", there are no known efficient approximation
algorithms for $k > 1$, to the best of our knowledge.
  We provide the first polynomial-time additive approximation scheme for
$\textsf{$k$-EMV}$. In particular, we obtain an embedding with objective value
$\textsf{OPT}_{\textrm{EMV}} + \varepsilon \Vert D\Vert_2^2$ in $(n\cdot
B)^{\mathsf{poly}(k, \varepsilon^{-1})}$ time, where each entry in $D$ can be
represented by $B$ bits. We believe our algorithm is a crucial first step
towards obtaining a PTAS for $\textsf{$k$-EMV}$. Our key technical contribution
is a new analysis of correlation rounding for Sherali-Adams / Sum-of-Squares
relaxations, tailored to low-dimensional embeddings. We also show that our
techniques allow us to obtain additive approximation schemes for two related
problems: a weighted variant of $\textsf{$k$-EMV}$ and $\ell_p$ low-rank
approximation for $p>2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximizing social welfare among EF1 allocations at the presence of two
  types of agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Ma, Yong Chen, Guangting Chen, Mingyang Gong, Guohui Lin, An Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the fair allocation of indivisible items to $n$ agents to maximize
the utilitarian social welfare, where the fairness criterion is envy-free up to
one item and there are only two different utility functions shared by the
agents. We present a $2$-approximation algorithm when the two utility functions
are normalized, improving the previous best ratio of $16 \sqrt{n}$ shown for
general normalized utility functions; thus this constant ratio approximation
algorithm confirms the APX-completeness in this special case previously shown
APX-hard. When there are only three agents, i.e., $n = 3$, the previous best
ratio is $3$ shown for general utility functions, and we present an improved
and tight $\frac 53$-approximation algorithm when the two utility functions are
normalized, and a best possible and tight $2$-approximation algorithm when the
two utility functions are unnormalized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version appears in ISAAC 2025; 20 pages in this full
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Approximation Guarantees and Hardness Results for MNL-Driven
  Product Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Segev, Gidi Steinberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address open computational questions regarding the market
share ranking problem, recently introduced by Derakhshan et al. (2022). Their
modelling framework incorporates the extremely popular Multinomial Logit (MNL)
choice model, along with a novel search-based consider-then-choose paradigm. In
a nutshell, the authors devised a Pandora's-Box-type search model, where
different customer segments sequentially screen through a ranked list of
products, one position after the other, forming their consideration set by
including all products viewed up until terminating their inspection procedure.
Subsequently, a purchasing decision out of this set is made based on a joint
MNL choice model.
  Our main contribution consists in devising a polynomial-time approximation
scheme for the market share ranking problem, utilizing fresh technical
developments and analytical ideas, in conjunction with revising the original
insights of Derakhshan et al. (2022). Along the way, we introduce a black-box
reduction, mapping general instances of the market share ranking problem into
``bounded ratio'' instances, showing that this result directly leads to an
elegant and easily-implementable quasi-PTAS. Finally, to provide a complete
computational characterization, we prove that the market share ranking problem
is strongly $\mathrm{NP}$-hard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplineSketch: Even More Accurate Quantiles with Error Guarantees <span class="chip">SIGMOD'26</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksander Łukasiewicz, Jakub Tětek, Pavel Veselý
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space-efficient streaming estimation of quantiles in massive datasets is a
fundamental problem with numerous applications in data monitoring and analysis.
While theoretical research led to optimal algorithms, such as the
Greenwald-Khanna algorithm or the KLL sketch, practitioners often use other
sketches that perform significantly better in practice but lack theoretical
guarantees. Most notably, the widely used $t$-digest has unbounded worst-case
error.
  In this paper, we seek to get the best of both worlds. We present a new
quantile summary, SplineSketch, for numeric data, offering near-optimal
theoretical guarantees, namely uniformly bounded rank error, and outperforming
$t$-digest by a factor of 2-20 on a range of synthetic and real-world datasets.
To achieve such performance, we develop a novel approach that maintains a
dynamic subdivision of the input range into buckets while fitting the input
distribution using monotone cubic spline interpolation. The core challenge is
implementing this method in a space-efficient manner while ensuring strong
worst-case guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGMOD'26</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compressibility Measures and Succinct Data Structures for Piecewise
  Linear Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07827v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07827v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Ferragina, Filippo Lari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of deriving compressibility measures for Piecewise
Linear Approximations (PLAs), i.e., error-bounded approximations of a set of
two-dimensional increasing data points using a sequence of segments. Such
approximations are widely used tools in implementing many learned data
structures, which mix learning models with traditional algorithmic design
blocks to exploit regularities in the underlying data distribution, providing
novel and effective space-time trade-offs. We introduce the first lower bounds
to the cost of storing PLAs in two settings, namely compression and indexing.
We then compare these compressibility measures to known data structures, and
show that they are asymptotically optimal up to a constant factor from the
space lower bounds. Finally, we design the first data structures for the
aforementioned settings that achieve the space lower bounds plus small additive
terms, which turn out to be succinct in most practical cases. Our data
structures support the efficient retrieval and evaluation of a segment in the
(compressed) PLA for a given $x$-value, which is a core operation in any
learned data structure relying on PLAs. As a result, our paper offers the first
theoretical analysis of the maximum compressibility achievable by PLA-based
learned data structures, and provides novel storage schemes for PLAs offering
strong theoretical guarantees while also suggesting simple and efficient
practical implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 36th International Symposium on
  Algorithms and Computation (ISAAC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nyldon Factorization of Thue-Morse Words and Fibonacci Words <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaisei Kishi, Kazuki Kai, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Nyldon factorization is a string factorization that is a non-decreasing
product of Nyldon words. Nyldon words and Nyldon factorizations are recently
defined combinatorial objects inspired by the well-known Lyndon words and
Lyndon factorizations. In this paper, we investigate the Nyldon factorization
of several words. First, we fully characterize the Nyldon factorizations of the
(finite) Fibonacci and the (finite) Thue-Morse words. Moreover, we show that
there exists a non-decreasing product of Nyldon words that is a factorization
of the infinite Thue-Morse word.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A full version of our conference paper accepted for SPIRE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enumeration Algorithms for Conjunctive Queries with Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.03712v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.03712v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaleen Deep, Xiao Hu, Paraschos Koutris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the enumeration of query results for an important subset of
CQs with projections, namely star and path queries. The task is to design data
structures and algorithms that allow for efficient enumeration with delay
guarantees after a preprocessing phase. Our main contribution is a series of
results based on the idea of interleaving precomputed output with further join
processing to maintain delay guarantees, which maybe of independent interest.
In particular, for star queries, we design combinatorial algorithms that
provide instance-specific delay guarantees in linear preprocessing time. These
algorithms improve upon the currently best known results. Further, we show how
existing results can be improved upon by using fast matrix multiplication. We
also present new results involving tradeoff between preprocessing time and
delay guarantees for enumeration of path queries that contain projections.
Boolean matrix multiplication is an important query that can be expressed as a
CQ with projection where the join attribute is projected away. Our results can
therefore also be interpreted as sparse, output-sensitive matrix multiplication
with delay guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamic, Self-balancing k-d Tree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Russell A. Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The original description of the k-d tree recognized that rebalancing
techniques, used for building an AVL tree or a red-black tree, are not
applicable to a k-d tree, because these techniques involve cyclic exchange of
tree nodes, which destroys the sorted order of the k-d tree. For this reason, a
static, balanced k-d tree is often built from all of the k-dimensional data en
masse. However, it is possible to build a dynamic k-d tree that self-balances
when necessary after insertion or deletion of each k-dimensional datum. This
article describes insertion, deletion, and rebalacing algorithms for a dynamic,
self-balancing k-d tree, and measures their performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene
  Evaluation <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuiko Uchida, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the ICCV 2025 UniLight Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lifting the Winding Number: Precise Discontinuities in Neural Fields for
  Physics Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chang, Mengfei Liu, Zhecheng Wang, Peter Yichen Chen, Eitan Grinspun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cutting thin-walled deformable structures is common in daily life, but poses
significant challenges for simulation due to the introduced spatial
discontinuities. Traditional methods rely on mesh-based domain representations,
which require frequent remeshing and refinement to accurately capture evolving
discontinuities. These challenges are further compounded in reduced-space
simulations, where the basis functions are inherently geometry- and
mesh-dependent, making it difficult or even impossible for the basis to
represent the diverse family of discontinuities introduced by cuts.
  Recent advances in representing basis functions with neural fields offer a
promising alternative, leveraging their discretization-agnostic nature to
represent deformations across varying geometries. However, the inherent
continuity of neural fields is an obstruction to generalization, particularly
if discontinuities are encoded in neural network weights.
  We present Wind Lifter, a novel neural representation designed to accurately
model complex cuts in thin-walled deformable structures. Our approach
constructs neural fields that reproduce discontinuities precisely at specified
locations, without baking in the position of the cut line. Crucially, our
approach does not embed the discontinuity in the neural network's weights,
opening avenues to generalization of cut placement.
  Our method achieves real-time simulation speeds and supports dynamic updates
to cut line geometry during the simulation. Moreover, the explicit
representation of discontinuities makes our neural field intuitive to control
and edit, offering a significant advantage over traditional neural fields,
where discontinuities are embedded within the network's weights, and enabling
new applications that rely on general cut placement.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-10T00:00:00Z">2025-09-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Deterministic Sub-0.5 us Response on Linux through Interrupt
  Isolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyi Zhou, Zhili Liu, Shancong Zhang, Jiemin Li, Dengke Du, Mengke Sun, Zhiqiang Wang, Hongyan Liu, Guokai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time responsiveness in Linux is often constrained by interrupt
contention and timer handling overhead, making it challenging to achieve
sub-microsecond latency. This work introduces an interrupt isolation approach
that centralizes and minimizes timer interrupt interference across CPU cores.
By enabling a dedicated API to selectively invoke timer handling routines and
suppress non-critical inter-processor interrupts, our design significantly
reduces jitter and response latency. Experiments conducted on an ARM-based
multicore platform demonstrate that the proposed mechanism consistently
achieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT
configurations. These results highlight the potential of interrupt isolation as
a lightweight and effective strategy for deterministic real-time workloads in
general-purpose operating systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Unknown Input Observer Design with Relaxed Conditions:
  Theory and Application to Vehicle Platooning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Zhao, Guitao Yang, Thomas Parisini, Boli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing observers for linear systems with both known and unknown inputs is
an important problem in several research contexts, for example, fault diagnosis
and fault-tolerant control, and cyber-secure control systems, and presents
significant challenges in distributed state estimation due to the limited
sensing capabilities of individual nodes. Existing methods typically impose an
individual input-to-output rank condition on each estimator node, which
severely restricts applicability in practical applications. This paper presents
a novel distributed unknown-input observer design scheme based on a geometric
approach under much weaker assumptions than the ones available in the
literature. By leveraging the properties of the $(C, A)$-invariant (conditioned
invariant) subspace at each node, our methodology aims at reconstructing
portions of the system state that remain unaffected by local unknown inputs,
while integrating these estimates via a network-based information exchange. A
case study on vehicle platoon control shows the effectiveness of the proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks
  with Entropy Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Ansarifard, Mostafa Rahmani, Mohit K. Sharma, Kishor C. Joshi, George Exarchakos, Alister Burr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive MIMO systems rely on accurate Channel State Information (CSI)
feedback to enable high-gain beam-forming. However, the feedback overhead
scales linearly with the number of antennas, presenting a major bottleneck.
While recent deep learning methods have improved CSI compression, most overlook
the impact of quantization and entropy coding, limiting their practical
deployability. In this work, we propose an end-to-end CSI compression framework
that integrates a Spatial Correlation-Guided Attention Mechanism with
quantization and entropy-aware training. Our model effectively exploits the
spatial correlation among the antennas, thereby learning compact,
entropy-optimized latent representations for efficient coding. This reduces the
required feedback bitrates without sacrificing reconstruction accuracy, thereby
yielding a superior rate-distortion trade-off. Experiments show that our method
surpasses existing end-to-end CSI compression schemes, exceeding benchmark
performance by an average of 21.5% on indoor datasets and 18.9% on outdoor
datasets. The proposed framework results in a practical and efficient CSI
feedback scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TANGO: Traversability-Aware Navigation with Local Metric Control for
  Topological Goals <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Podgorski, Sourav Garg, Mehdi Hosseinzadeh, Lachlan Mares, Feras Dayoub, Ian Reid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual navigation in robotics traditionally relies on globally-consistent 3D
maps or learned controllers, which can be computationally expensive and
difficult to generalize across diverse environments. In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers. Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles. We address key
limitations of previous methods by continuously predicting local trajectory
using monocular depth and traversability estimation, and incorporating an
auto-switching mechanism that falls back to a baseline controller when
necessary. The system operates using foundational models, ensuring open-set
applicability without the need for domain-specific fine-tuning. We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability. Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments. The source code is
made publicly available: https://github.com/podgorki/TANGO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Graph Learning for Power System Reconfigurations: Transfer
  Across Topology Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Anna Scaglione, Sandy Miguel, Daniel Arnold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses a fundamental challenge in applying deep learning to
power systems: developing neural network models that transfer across
significant system changes, including networks with entirely different
topologies and dimensionalities, without requiring training data from unseen
reconfigurations. Despite extensive research, most ML-based approaches remain
system-specific, limiting real-world deployment. This limitation stems from a
dual barrier. First, topology changes shift feature distributions and alter
input dimensions due to power flow physics. Second, reconfigurations redefine
output semantics and dimensionality, requiring models to handle
configuration-specific outputs while maintaining transferable feature
extraction. To overcome this challenge, we introduce a Universal Graph
Convolutional Network (UGCN) that achieves transferability to any
reconfiguration or variation of existing power systems without any prior
knowledge of new grid topologies or retraining during implementation. Our
approach applies to both transmission and distribution networks and
demonstrates generalization capability to completely unseen system
reconfigurations, such as network restructuring and major grid expansions.
Experimental results across power system applications, including false data
injection detection and state forecasting, show that UGCN significantly
outperforms state-of-the-art methods in cross-system zero-shot transferability
of new reconfigurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis and Control of Acoustic Emissions from Marine Energy Converters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqin He, Max Malyi, Jonathan Shek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the mitigation of acoustic emissions from tidal
current converters (TCCs) through optimized control strategies to enhance power
generation efficiency while minimizing environmental impacts on marine life. A
MATLAB/Simulink-based model of a Tidal Current Conversion System (TCCS) was
developed to simulate the effects of variable control parameters, including
switching frequencies, maximum power point tracking (MPPT) coefficients, and
the elimination of the gearbox, on underwater noise levels. Acoustic emissions
were quantified in terms of sound pressure levels (SPLs), and their potential
impacts on marine mammals and fish were evaluated against species-specific
auditory thresholds for temporary and permanent hearing threshold shifts. The
results indicate that adjusting control parameters can significantly reduce
SPLs, with the removal of the gearbox yielding the greatest noise reduction.
The study identifies operational conditions under which marine species are at
risk of auditory damage and proposes control strategies to mitigate these risks
without compromising energy output. These findings contribute to the
understanding of how control system modifications can balance the efficiency of
marine energy systems with ecological considerations, offering guidance for the
design and operation of environmentally compliant TCCs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute
  Implementations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ron F. Del Rosario, Klaudia Krawiecka, Christian Schroeder de Witt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Model (LLM) agents become increasingly capable of
automating complex, multi-step tasks, the need for robust, secure, and
predictable architectural patterns is paramount. This paper provides a
comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic
design that separates strategic planning from tactical execution. We explore
the foundational principles of P-t-E, detailing its core components - the
Planner and the Executor - and its architectural advantages in predictability,
cost-efficiency, and reasoning quality over reactive patterns like ReAct
(Reason + Act). A central focus is placed on the security implications of this
design, particularly its inherent resilience to indirect prompt injection
attacks by establishing control-flow integrity. We argue that while P-t-E
provides a strong foundation, a defense-in-depth strategy is necessary, and we
detail essential complementary controls such as the Principle of Least
Privilege, task-scoped tool access, and sandboxed code execution. To make these
principles actionable, this guide provides detailed implementation blueprints
and working code references for three leading agentic frameworks: LangChain
(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing
the P-t-E pattern is analyzed, highlighting unique features like LangGraph's
stateful graphs for re-planning, CrewAI's declarative tool scoping for
security, and AutoGen's built-in Docker sandboxing. Finally, we discuss
advanced patterns, including dynamic re-planning loops, parallel execution with
Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop
(HITL) verification, to offer a complete strategic blueprint for architects,
developers, and security engineers aiming to build production-grade, resilient,
and trustworthy LLM agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal control of stochastic networks of $M/M/\infty$ queues with
  linear costs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Pugliese Carratelli, Ioannis Lestas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an arbitrary network of $M/M/\infty$ queues with controlled
transitions between queues. We consider optimal control problems where the
costs are linear functions of the state and inputs over a finite or infinite
horizon. We provide in both cases an explicit characterization of the optimal
control policies. We also show that these do not involve state feedback, but
they depend on the network topology and system parameters. The results are also
illustrated with various examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission to the Conference on Decision and Control 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How can a geothermal storage system be optimally integrated into a local
  district? A case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ueli Schilt, Somesh Vijayananda, Sarah Schneeberger, Manuel Meyer, Santhosh Iyyakkunnel, Pascal Marc Vecsei, Philipp Schuetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving net-zero targets requires the phase-out of fossil-based heating. A
major challenge is the seasonal mismatch between renewable heat supply and
demand. District heating networks often dispose of excess heat in summer and
rely on fossil backups in winter. Large-scale thermal energy storage offers a
solution by storing surplus summer heat for use during winter, thus reducing
the need for fossil fuels. This study investigates the feasibility of a
large-scale thermal storage system at a power production site that supplies a
large district heating network in the city of Bern, Switzerland. Specifically,
the study examines the potential of a geothermal storage system to offset
fossil fuel heat generation in winter by utilising heat stored during the
summer months. Using a Python-based multi-energy system model, we simulate the
optimal operation of the geothermal storage system with respect to cost and
emissions, considering both supply and demand on an hourly basis over one year.
Multi-objective optimisation is applied to generate a Pareto-optimal front. The
results show that the geothermal storage system eliminates the requirement of 8
GWh of gas-powered heat supply and increases the waste heat utilisation by 20%,
therefore lowering emissions. This effect is further increased when combined
with an expansion of the district heating network, as individual,
emission-heavy heaters are replaced by low-emission heat from the district
heating network. The findings presented in this study can prove useful when
evaluating similar systems across Switzerland.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 1 table. 2025 CISBAT conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phase-Coordinated Multi-Agent Circular Formation Control with
  Non-Concentric Boundary Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Singh, Anoop Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of collective circular motion control for
unicycle agents, with the objective of achieving phase coordination of their
velocity vectors while ensuring that their trajectories remain confined within
a prescribed non-concentric circular boundary. To accommodate such nonuniform
motion constraints, we build upon our earlier work and extend the use of Mobius
transformation to a multi-agent framework. The Mobius transformation maps two
nonconcentric circles to concentric ones, thereby converting spatially
nonuniform constraints into uniform ones in the transformed plane. Leveraging
this property, we introduce the notion of a phase-shifted order parameter,
along with the associated concepts of Mobius phase-shift coupled
synchronization and balancing, which characterize the phase-coordinated
patterns studied in this paper. We establish an equivalence between the
unicycle dynamics in the original and transformed planes under the Mobius
transformation and its inverse, and show that synchronization is preserved
across both planes, whereas balancing is generally not. Distributed control
laws are then designed in the transformed plane using barrier Lyapunov
functions, under the assumption of an undirected and connected communication
topology among agents. These controllers are subsequently mapped back to the
original plane to obtain the linear acceleration and turn-rate control inputs
applied to the actual agents. Both simulations and experimental results are
provided to illustrate the proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast
  Marching Tree for Dynamic Replanning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soheil Espahbodini Nia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planning in dynamic environments remains a core challenge in robotics,
especially as autonomous systems are deployed in unpredictable spaces such as
warehouses and public roads. While algorithms like Fast Marching Tree
(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their
single-pass design prevents path revisions which are essential for real-time
adaptation. On the other hand, full replanning is often too computationally
expensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching
Tree algorithm that enables efficient and consistent replanning in dynamic
environments. We revisit the neighbor selection rule of FMT$^{*}$ and
demonstrate that a minimal change overcomes its single-pass limitation,
enabling the algorithm to update cost-to-come values upon discovering better
connections without sacrificing asymptotic optimality or computational
efficiency. By maintaining a cost-ordered priority queue and applying a
selective update condition that uses an expanding neighbor to identify and
trigger the re-evaluation of any node with a potentially suboptimal path,
FMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the
environment evolves. This targeted strategy preserves the inherent efficiency
of FMT$^{*}$ while enabling robust adaptation to changes in obstacle
configuration. FMT$^{x}$ is proven to recover an asymptotically optimal
solution after environmental changes. Experimental results demonstrate that
FMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more
swiftly to dynamic events with lower computational overhead and thus offering a
more effective solution for real-time robotic navigation in unpredictable
worlds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 8 figures, 2 tables, submitted to the International Journal
  of Robotics Research (IJRR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness of quantum algorithms: Worst-case fidelity bounds and
  implications for design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Berberich, Tobias Fellner, Robert L. Kosut, Christian Holm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Errors occurring on noisy hardware pose a key challenge to reliable quantum
computing. Existing techniques such as error correction, mitigation, or
suppression typically separate the error handling from the algorithm analysis
and design. In this paper, we develop an alternative, algorithm-centered
framework for understanding and improving the robustness against errors. For a
given quantum algorithm and error model, we derive worst-case fidelity bounds
which can be explicitly computed to certify the robustness. We consider general
error models including coherent and (Markovian) incoherent errors and allowing
for set-based error descriptions to address uncertainty or time-dependence in
the errors. Our results give rise to guidelines for robust algorithm design and
compilation by optimizing our theoretical robustness measure. Numerical results
on algorithm analysis and robust optimization demonstrate the practicality of
the framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SKYLINK: Scalable and Resilient Link Management in LEO Satellite Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanja de Sombre, Arash Asadi, Debopam Bhattacherjee, Deepak Vasisht, Andrea Ortiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of space-based services has established LEO satellite
networks as a promising option for global broadband connectivity.
Next-generation LEO networks leverage inter-satellite links (ISLs) to provide
faster and more reliable communications compared to traditional bent-pipe
architectures, even in remote regions. However, the high mobility of
satellites, dynamic traffic patterns, and potential link failures pose
significant challenges for efficient and resilient routing. To address these
challenges, we model the LEO satellite network as a time-varying graph
comprising a constellation of satellites and ground stations. Our objective is
to minimize a weighted sum of average delay and packet drop rate. Each
satellite independently decides how to distribute its incoming traffic to
neighboring nodes in real time. Given the infeasibility of finding optimal
solutions at scale, due to the exponential growth of routing options and
uncertainties in link capacities, we propose SKYLINK, a novel fully distributed
learning strategy for link management in LEO satellite networks. SKYLINK
enables each satellite to adapt to the time-varying network conditions,
ensuring real-time responsiveness, scalability to millions of users, and
resilience to network failures, while maintaining low communication overhead
and computational complexity. To support the evaluation of SKYLINK at global
scale, we develop a new simulator for large-scale LEO satellite networks. For
25.4 million users, SKYLINK reduces the weighted sum of average delay and drop
rate by 29% compared to the bent-pipe approach, and by 92% compared to
Dijkstra. It lowers drop rates by 95% relative to k-shortest paths, 99%
relative to Dijkstra, and 74% compared to the bent-pipe baseline, while
achieving up to 46% higher throughput. At the same time, SKYLINK maintains
constant computational complexity with respect to constellation size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Planning Strategy for Building a Heterogeneous Smart EM Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arianna Benoni, Marco Salucci, Baozhu Li, Andrea Massa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a planning strategy for the deployment of smart
electromagnetic entities (SEEs) to enhance the wireless coverage and the
Quality-of-Service (QoS) in large urban areas. The integration of different
technological solutions such as integrated access-and-backhaul nodes (IABs),
smart repeaters (SRs), and electromagnetic skins (EMSs) is here addressed to
enable an effective and efficient implementation of the concept of Smart
Electromagnetic Environment (SEME). By combining the features of such
heterogeneous SEEs and optimizing their number, positions, orientations, and
configuration, the electromagnetic (EM) coverage in a set of
Regions-of-Interest (RoIs) of outdoor scenarios is recovered and/or enhanced
subject to installation costs and energy consumption requirements. Numerical
validations from real-world scenarios are reported to assess the effectiveness
of the proposed planning scheme as well as to show the potentialities of an
heterogeneous deployment of SEMEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AP-observation Automata for Abstraction-based Verification of
  Continuous-time Systems (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasinee Pruekprasert, Clovis Eberhart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge in abstraction-based verification and control under complex
specifications such as Linear Temporal Logic (LTL) is that abstract models
retain significantly less information than their original systems. This issue
is especially true for continuous-time systems, where the system state
trajectories are split into intervals of discrete actions, and satisfaction of
atomic propositions is abstracted to a whole time interval. To tackle this
challenge, this work introduces a novel translation from LTL specifications to
AP-observation automata, a particular type of B\"uchi automata specifically
designed for abstraction-based verification. Based on this automaton, we
present a game-based verification algorithm played between the system and the
environment, and an illustrative example for abstraction-based system
verification under several LTL specifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of the paper under the same title
  accepted for presentation at the 22nd International Colloquium on Theoretical
  Aspects of Computing (ICTAC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resilient Global Practical Fixed-Time Cooperative Output Regulation of
  Uncertain Nonlinear Multi-Agent Systems Subject to Denial-of-Service Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenji Cao, Lu Liu, Zehua Ye, Dan Zhang, Gang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of resilient global practical fixed-time
cooperative output regulation of uncertain nonlinear multi-agent systems
subject to denial-of-service attacks. A novel distributed resilient adaptive
fixed-time control strategy is proposed, which consists of a novel distributed
resilient fixed-time observer with a chain of nonlinear filters and a novel
distributed resilient adaptive fixed-time controller. It is shown that the
problem of resilient global practical fixed-time cooperative output regulation
can be solved by the proposed control strategy. More specifically, the proposed
{distributed} control strategy ensures the global boundedness of all the
signals in the resulting closed-loop system and the global convergence of the
regulated outputs to a {tunable} residual set in a fixed time. A simulation
example is finally provided to illustrate the efficacy of the proposed control
strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game-Theoretic Resilience Framework for Cyber-Physical Microgrids using
  Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S Krishna Niketh, Sagar Babu Mitikiri, V Vignesh, Vedantham Lakshmi Srinivas, Mayukha Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on cyber physical infrastructure in modern power
systems has amplified the risk of targeted cyber attacks, necessitating robust
and adaptive resilience strategies. This paper presents a mathematically
rigorous game theoretic framework to evaluate and enhance microgrid resilience
using a combination of quantitative resilience metrics Load Served Ratio LSR,
Critical Load Resilience CLR, Topological Survivability Score TSS, and DER
Resilience Score DRS. These are integrated into a unified payoff matrix using
the Analytic Hierarchy Process AHP to assess attack defense interactions. The
framework is formalized as a finite horizon Markov Decision Process MDP with
formal convergence guarantees and computational complexity bounds. Three case
studies are developed 1. static attacks analyzed via Nash equilibrium, 2.
severe attacks incorporating high impact strategies, and 3. adaptive attacks
using Stackelberg games, regret matching, softmax heuristics, and Multi Agent Q
Learning. Rigorous theoretical analysis provides convergence proofs with
explicit rates , PAC learning sample complexity bounds, and computational
complexity analysis. The framework is tested on an enhanced IEEE 33bus
distribution system with DERs and control switches, demonstrating the
effectiveness of adaptive and strategic defenses in improving cyber physical
resilience with statistically significant improvements of 18.7% 2.1% over
static approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed
  Task Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirabhra Mandal, Aamodh Suresh, Carlos Nieto-Granda, Sonia Martínez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a problem of multi-agent exploration with behaviorally heterogeneous
robots. Each robot maps its surroundings using SLAM and identifies a set of
areas of interest (AoIs) or frontiers that are the most informative to explore
next. The robots assess the utility of going to a frontier using Behavioral
Entropy (BE) and then determine which frontier to go to via a distributed task
assignment scheme. We convert the task assignment problem into a
non-cooperative game and use a distributed algorithm (d-PBRAG) to converge to
the Nash equilibrium (which we show is the optimal task allocation solution).
For unknown utility cases, we provide robust bounds using approximate rewards.
We test our algorithm (which has less communication cost and fast convergence)
in simulation, where we explore the effect of sensing radii, sensing accuracy,
and heterogeneity among robotic teams with respect to the time taken to
complete exploration and path traveled. We observe that having a team of agents
with heterogeneous behaviors is beneficial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-Efficient Online Control Policy Learning with Real-Time Recursive
  Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Zhang, James Avtges, Todd D. Murphey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven control methods need to be sample-efficient and lightweight,
especially when data acquisition and computational resources are limited --
such as during learning on hardware. Most modern data-driven methods require
large datasets and struggle with real-time updates of models, limiting their
performance in dynamic environments. Koopman theory formally represents
nonlinear systems as linear models over observables, and Koopman
representations can be determined from data in an optimization-friendly setting
with potentially rapid model updates. In this paper, we present a highly
sample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning
(RKL). We identify sufficient conditions for model convergence and provide
formal algorithmic analysis supporting our claim that RKL is lightweight and
fast, with complexity independent of dataset size. We validate our method on a
simulated planar two-link arm and a hybrid nonlinear hardware system with soft
actuators, showing that real-time recursive Koopman model updates improve the
sample efficiency and stability of data-driven controller synthesis --
requiring only <10% of the data compared to benchmarks. The high-performance
C++ codebase is open-sourced. Website:
https://www.zixinatom990.com/home/robotics/corl-2025-recursive-koopman-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Optimization of Computation Offloading and Resource Allocation in
  ISAC-assisted SAGIN-based IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sooyeob Jung, Seongah Jeong, Jinkyu Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letters, an energy-efficient integrated sensing and communication
(ISAC) for space-air-ground integrated network (SAGIN)-based Internet of Things
(IoT) systems is proposed to facilitate wide coverage and real-time 6G
services. For processing a sizable data collected at a IoT device, a hybrid
edge computing scheme is applied with the cloudlets mounted at autonomous
aerial vehicle (AAV) and low earth orbit (LEO) satellite, where the AAV with
multiple antennas performs uplink sensing of the nearby target. With the aim of
minimizing the total AAV's energy consumption, we optimize the duration of
training and data phase and the bit allocation coupled with the offloading
ratio under the constraints for offloading and sensing. Via simulations, the
superiority of the proposed algorithm is verified to be pronounced with the
sufficient mission time and the high sensing performance constraint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multivariable Current Controller for Enhancing Dynamic Response and Grid
  Synchronization Stability of IBRs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Yazdani, Ali Maleki, Saeed Lotfifard, Ali Saberi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a multivariable current control strategy for
inverter-based resources (IBRs) based on optimal control theory to enhance
their dynamic performance and grid synchronization stability. The structure of
the implemented multiple-input, multiple-output (MIMO) controller closely
resembles that of the commonly used conventional single-input, single-output
(SISO) PI controllers for IBRs. As a result, it requires only minor adjustments
to conventional vector current control schemes, thereby facilitating its
straightforward adoption. Time-domain simulations and analytical analysis
demonstrate the superior performance of the developed method under various
conditions and use case scenarios, such as weak power systems and uncertain
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Local Voltage Control for Active Distribution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana Vieira Fernandes, Soummya Kar, Carlos Santos Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution networks face challenges from the increasing deployment of
Distributed Energy Resources (DERs) and the emergence of bidirectional power
flows. We propose a decentralized Volt/VAr control method based on a
saddle-point reformulation and consensus+innovation (C+I) updates. Each agent
at a controllable bus computes and enforces its own set-points using only
neighbor communication. Our method embeds passive buses directly, preserves
network physics through a linearized Jacobian model, and avoids any supervisory
nodes. Simulation results on a modified CIGRE low-voltage network show voltage
stability improvement within operational limits, indicating the viability of a
fully decentralized (edge-based) Volt/VAr control solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE SmartGridComm'25 - 2025 IEEE International
  Conference on Communications, Control, and Computing Technologies for Smart
  Grids (SmartGridComm)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of Reliable and Resilient Electric Power Systems for Wide-Body
  All-Electric Aircraft 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mona Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve net-zero emissions by 2050, all-electric transportation is a
promising option. In the U.S., the transportation sector contributes the
largest share (29 percent) of greenhouse gas emissions. While electric vehicles
are approaching maturity, aviation is only beginning to develop electrified
aircraft for commercial flights. More than 75 percent of aviation emissions
come from large aircraft, and this impact will worsen with 4-5 percent annual
air travel growth. Aircraft electrification has led to two types: more electric
aircraft (MEA) and all-electric aircraft (AEA). A MEA replaces subsystems such
as hydraulics with electric alternatives, whereas an AEA uses electrically
driven subsystems and provides thrust fully from electrochemical energy units
(EEUs). For wide-body AEA, thrust demand is about 25 MW plus 1 MW for
non-thrust loads, creating major challenges for electric power system (EPS)
design. Achieving maximum power density requires minimizing mass and volume.
Increasing voltage into the kilovolt range using medium-voltage direct current
(MVDC) is a feasible option to enhance power transfer. Consequently, designing
an MVDC EPS for wide-body AEA is critical. Because EPS failures could
jeopardize passenger safety, reliability and resilience are essential. This
chapter presents a load-flow model for DC systems to determine power flows in
both normal and single-contingency conditions, followed by analysis of optimal
MVDC EPS architectures. A complete EPS for wide-body AEA is introduced, with
EEUs and non-propulsion loads located, distances estimated, and flow studies
performed. Multiple architectures are evaluated for reliability, power density,
power loss, and cost to identify optimal solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regularization in Data-driven Predictive Control: A Convex Relaxation
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Shang, Yang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the role of regularization in data-driven predictive
control (DDPC) through the lens of convex relaxation. Using a bi-level
optimization framework, we model system identification as an inner problem and
predictive control as an outer problem. Within this framework, we show that
several regularized DDPC formulations, including l1-norm penalties,
projection-based regularizers, and a newly introduced causality-based
regularizer, can be viewed as convex relaxations of their respective bi-level
problems. This perspective clarifies the conceptual links between direct and
indirect data-driven control and highlights how regularization implicitly
enforces system identification. We further propose an optimality-based variant,
O-DDPC, which approximately solves the inner problem with all identification
constraints via an iterative algorithm. Numerical experiments demonstrate that
O-DDPC outperforms existing regularized DDPC by reducing both bias and variance
errors. These results indicate that further benefits may be obtained by
applying system identification techniques to pre-process the trajectory library
in nonlinear settings. Overall, our analysis contributes to a unified convex
relaxation view of regularization in DDPC and sheds light on its strong
empirical performance beyond linear time-invariant systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic
  Paradigm for Defense and Dominance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya-Ting Yang, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyber warfare has become a central element of modern conflict, especially
within multi-domain operations. As both a distinct and critical domain, cyber
warfare requires integrating defensive and offensive technologies into coherent
strategies. While prior research has emphasized isolated tactics or fragmented
technologies, a holistic understanding is essential for effective resource
deployment and risk mitigation. Game theory offers a unifying framework for
this purpose. It not only models attacker-defender interactions but also
provides quantitative tools for equilibrium analysis, risk assessment, and
strategic reasoning. Integrated with modern AI techniques, game-theoretic
models enable the design and optimization of strategies across multiple levels
of cyber warfare, from policy and strategy to operations, tactics, and
technical implementations. These models capture the paradoxical logic of
conflict, where more resources do not always translate into greater advantage,
and where nonlinear dynamics govern outcomes. To illustrate the approach, this
chapter examines RedCyber, a synthetic cyber conflict, demonstrating how
game-theoretic methods capture the interdependencies of cyber operations. The
chapter concludes with directions for future research on resilience,
cros-echelon planning, and the evolving role of AI in cyber warfare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient High-Order Participation Factor Computation via
  Batch-Structured Tensor Contraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Sajjadi, Kaiyang Huang, Kai Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Participation factors (PFs) quantify the interaction between system modes and
state variables, and they play a crucial role in various applications such as
modal analysis, model reduction, and control design. With increasing system
complexity, especially due to power electronic devices and renewable
integration, the need for scalable and high-order nonlinear PF (NPF)
computation has become more critical. This paper presents an efficient
tensor-based method for calculating NPFs up to an arbitrary order. Traditional
computation of PFs directly from normal form theory is computationally
expensive -- even for second-order PFs -- and becomes infeasible for higher
orders due to memory constraints. To address this, a tensor contraction-based
approach is introduced that enables the calculation of high-order PFs using a
batching strategy. The batch sizes are dynamically determined based on the
available computational resources, allowing scalable and memory-efficient
computation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Inverse Reinforcement Learning for Identifying
  Pareto-Efficient Coordination -- A Distributionally Robust Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Snow, Vikram Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent inverse reinforcement learning (IRL) aims to identify
Pareto-efficient behavior in a multi-agent system, and reconstruct utility
functions of the individual agents. Motivated by the problem of detecting UAV
coordination, how can we construct a statistical detector for Pareto-efficient
behavior given noisy measurements of the decisions of a multi-agent system?
This paper approaches this IRL problem by deriving necessary and sufficient
conditions for a dataset of multi-agent system dynamics to be consistent with
Pareto-efficient coordination, and providing algorithms for recovering utility
functions which are consistent with the system dynamics. We derive an optimal
statistical detector for determining Pareto-efficient coordination from noisy
system measurements, which minimizes Type-I statistical detection error. Then,
we provide a utility estimation algorithm which minimizes the worst-case
estimation error over a statistical ambiguity set centered at empirical
observations; this min-max solution achieves distributionally robust IRL, which
is crucial in adversarial strategic interactions. We illustrate these results
in a detailed example for detecting Pareto-efficient coordination among
multiple UAVs given noisy measurement recorded at a radar. We then reconstruct
the utility functions of the UAVs in a distributionally robust sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreejeet Maity, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Centralized and Distributed Frameworks in Unknown Input
  Observer Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Zhao, Guitao Yang, Peng Li, Boli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State estimation for linear time-invariant systems with unknown inputs is a
fundamental problem in various research domains. In this article, we establish
conditions for the design of unknown input observers (UIOs) from a geometric
approach perspective. Specifically, we derive a necessary and sufficient
geometric condition for the existence of a centralized UIO. Compared to
existing results, our condition offers a more general design framework,
allowing designers the flexibility to estimate partial information of the
system state. Furthermore, we extend the centralized UIO design to distributed
settings. In contrast to existing distributed UIO approaches, which require
each local node to satisfy the rank condition regarding the unknown input and
output matrices, our method accommodates cases where a subset of nodes does not
meet this requirement. This relaxation significantly broadens the range of
practical applications. Simulation results are provided to demonstrate the
effectiveness of the proposed design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Stölzle, Sonal Santosh Baberwal, Daniela Rus, Shirley Coyle, Cosimo Della Santina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating Brain-Machine Interfaces into non-clinical applications like
robot motion control remains difficult - despite remarkable advancements in
clinical settings. Specifically, EEG-based motor imagery systems are still
error-prone, posing safety risks when rigid robots operate near humans. This
work presents an alternative pathway towards safe and effective operation by
combining wearable EEG with physically embodied safety in soft robots. We
introduce and test a pipeline that allows a user to move a soft robot's end
effector in real time via brain waves that are measured by as few as three EEG
channels. A robust motor imagery algorithm interprets the user's intentions to
move the position of a virtual attractor to which the end effector is
attracted, thanks to a new Cartesian impedance controller. We specifically
focus here on planar soft robot-based architected metamaterials, which require
the development of a novel control architecture to deal with the peculiar
nonlinearities - e.g., non-affinity in control. We preliminarily but
quantitatively evaluate the approach on the task of setpoint regulation. We
observe that the user reaches the proximity of the setpoint in 66% of steps and
that for successful steps, the average response time is 21.5s. We also
demonstrate the execution of simple real-world tasks involving interaction with
the environment, which would be extremely hard to perform if it were not for
the robot's softness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, presented at 7th IEEE-RAS International Conference on Soft
  Robotics (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Electricity Demand and Grid Impacts of AI Data Centers: Challenges and
  Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chen, Xiaoyang Wang, Ana Colacelli, Matt Lee, Le Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of artificial intelligence (AI) is driving an unprecedented
increase in the electricity demand of AI data centers, raising emerging
challenges for electric power grids. Understanding the characteristics of AI
data center loads and their interactions with the grid is therefore critical
for ensuring both reliable power system operation and sustainable AI
development. This paper provides a comprehensive review and vision of this
evolving landscape. Specifically, this paper (i) presents an overview of AI
data center infrastructure and its key components, (ii) examines the key
characteristics and patterns of electricity demand across the stages of model
preparation, training, fine-tuning, and inference, (iii) analyzes the critical
challenges that AI data center loads pose to power systems across three
interrelated timescales, including long-term planning and interconnection,
short-term operation and electricity markets, and real-time dynamics and
stability, and (iv) discusses potential solutions from the perspectives of the
grid, AI data centers, and AI end-users to address these challenges. By
synthesizing current knowledge and outlining future directions, this review
aims to guide research and development in support of the joint advancement of
AI data centers and power systems toward reliable, efficient, and sustainable
operation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Optimal Orders for Entanglement Swapping in Path Graphs: A
  Greedy Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14040v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14040v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Sy Mai, Abderrahim Amlou, Amar Abane, Abdella Battou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of finding an optimal order for entanglement
swapping in a heterogeneous path of quantum repeaters so as to maximize the
path throughput defined as the delivery rate of end-to-end entanglements. The
primary difficulty in addressing this problem lies in the vast array of
possible swapping orders for large paths and the complexity of the expected
throughput, which depends on the attributes of each node and edge along the
path, as well as the order of swapping. To cope with these issues, we first
propose simple approximations in estimating the swapping outcome between two
entanglement distributions that can run in constant time, thereby providing an
efficient approach for evaluating and comparing different swapping orders,
allowing us to solve the problem exactly for small paths. Second, as the number
of possible orders grows exponentially with the number of repeaters in the
path, we develop an efficient heuristic based on the greedy selection of nodes
to sequentially perform swaps according to their swapping scores, defined as
the expected number of entanglements resulting from their swaps. The scores are
local but dynamic in the sense that they depend not just on the entanglement
distributions available on the path but also on prior swapping decisions.
Finally, we illustrate the efficiency and effectiveness of our proposed model
and approach through extensive experimentation conducted using a general
quantum network simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Guarantees of Model-free Policy Gradient Methods for LQR
  with Stochastic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Song, Andrea Iannelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy gradient (PG) methods are the backbone of many reinforcement learning
algorithms due to their good performance in policy optimization problems. As a
gradient-based approach, PG methods typically rely on knowledge of the system
dynamics. If this is not available, trajectory data can be utilized to
approximate first-order information. When the data are noisy, gradient
estimates become inaccurate and a study that investigates uncertainty
estimation and the analysis of its propagation through the algorithm is
currently missing. To address this, our work focuses on the Linear Quadratic
Regulator (LQR) problem for systems subject to additive stochastic noise. After
briefly summarizing the state of the art for cases with a known model, we focus
on scenarios where the system dynamics are unknown, and approximate gradient
information is obtained using zeroth-order optimization techniques. We analyze
the theoretical properties by computing the error in the estimated gradient and
examining how this error affects the convergence of PG algorithms.
Additionally, we provide global convergence guarantees for various versions of
PG methods, including those employing adaptive step sizes and variance
reduction techniques, which help increase the convergence rate and reduce
sample complexity. This study contributed to characterizing robustness of the
study of the robustness of model-free PG methods, aiming to identify their
limitations in the presence of stochastic noise and proposing improvements to
enhance their applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Power-Gas Infrastructure Planning under Weather-induced Supply and
  Demand Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23509v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23509v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahman Khorramfar, Dharik Mallapragada, Saurabh Amin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing economy-wide decarbonization strategies based on decarbonizing
the power grid via variable renewable energy (VRE) expansion and
electrification of end-uses requires new approaches for energy infrastructure
planning that consider, among other factors, weather-induced uncertainty in
demand and VRE supply. An energy planning model that fails to account for these
uncertainties can hinder the intended transition efforts to a low-carbon grid
and increase the risk of supply shortage especially during extreme weather
conditions. Here, we consider the generation and transmission expansion problem
of joint power-gas infrastructure and operations planning under the uncertainty
of both demand and renewable supply. We propose two distributionally robust
optimization approaches based on moment (MDRO) and Wasserstein distance (WDRO)
ambiguity sets to endogenize these uncertainties and account for the change in
the underlying distribution of these parameters that is caused by the climate
change, among other factors. Furthermore, our model considers the risk-aversion
of the energy planners in the modeling framework via the conditional
value-at-risk (CVaR) metric. An equivalent mixed-integer linear programming
(MILP) reformulation of both modeling frameworks is presented, and a
computationally efficient approximation scheme to obtain near-optimal solutions
is proposed. We demonstrate the resulting DRO planning models and solution
strategy via a New England case study under different levels of end-use
electrification and decarbonization targets. Our experiments systematically
explore different modeling aspects and compare the DRO models with stochastic
programming (SP) results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational Concept of the Psyche (in Russian) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Kolonin, Vladimir Kryukov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The article provides an overview of approaches to modeling the human psyche
in the perspective of building an artificial one. Based on the review, a
concept of cognitive architecture is proposed, where the psyche is considered
as an operating system of a living or artificial subject, including a space of
needs that determines its life meanings in connection with stimuli from the
external world, and intelligence as a decision-making system for actions in
relation to this world in order to satisfy these needs. Based on the concept, a
computational formalization is proposed for creating artificial intelligence
systems through learning from experience in the space of a space of needs,
taking into account their biological or existential significance for an
intelligent agent. Thus, the problem of building general artificial
intelligence as a system for making optimal decisions in the space of
agent-specific needs under conditions of uncertainty is formalized, with
maximization of success in achieving goals, minimization of existential risks
and maximization of energy efficiency. A minimal experimental implementation of
the model is also provided.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, in Russian, 2 figures, submitted to Neuroinformatics-2025
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Resilience-Aware Control in Multi-Robot Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03120v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03120v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haejoon Lee, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring resilient consensus in multi-robot systems with misbehaving agents
remains a challenge, as many existing network resilience properties are
inherently combinatorial and globally defined. While previous works have
proposed control laws to enhance or preserve resilience in multi-robot
networks, they often assume a fixed topology with known resilience properties,
or require global state knowledge. These assumptions may be impractical in
physically-constrained environments, where safety and resilience requirements
are conflicting, or when misbehaving agents share inaccurate state information.
In this work, we propose a distributed control law that enables each robot to
guarantee resilient consensus and safety during its navigation without fixed
topologies using only locally available information. To this end, we establish
a sufficient condition for resilient consensus in time-varying networks based
on the degree of non-misbehaving or normal agents. Using this condition, we
design a Control Barrier Function (CBF)-based controller that guarantees
resilient consensus and collision avoidance without requiring estimates of
global state and/or control actions of all other robots. Finally, we validate
our method through simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and will appear at 2025 IEEE Conference on Decision and
  Control (CDC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-Stage Optimization of Open-loop Stable Limit Cycles with Smooth,
  Symbolic Derivatives <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Saud Ul Hassan, Christian Hubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-loop stable limit cycles are foundational to legged robotics, providing
inherent self-stabilization that minimizes the need for computationally
intensive feedback-based gait correction. While previous methods have primarily
targeted specific robotic models, this paper introduces a general framework for
rapidly generating limit cycles across various dynamical systems, with the
flexibility to impose arbitrarily tight stability bounds. We formulate the
problem as a single-stage constrained optimization problem and use Direct
Collocation to transcribe it into a nonlinear program with closed-form
expressions for constraints, objectives, and their gradients.
  Our method supports multiple stability formulations. In particular, we tested
two popular formulations for limit cycle stability in robotics: (1) based on
the spectral radius of a discrete return map, and (2) based on the spectral
radius of the monodromy matrix, and tested five different
constraint-satisfaction formulations of the eigenvalue problem to bound the
spectral radius. We compare the performance and solution quality of the various
formulations on a robotic swing-leg model, highlighting the Schur decomposition
of the monodromy matrix as a method with broader applicability due to weaker
assumptions and stronger numerical convergence properties.
  As a case study, we apply our method on a hopping robot model, generating
open-loop stable gaits in under 2 seconds on an Intel Core i7-6700K, while
simultaneously minimizing energy consumption even under tight stability
constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Conference on Robotics and Automation
  (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tunable Thresholds and Frequency Encoding in a Spiking NOD Controller 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Xul Belaustegui, Alessio Franci, Naomi Ehrich Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Nonlinear Opinion Dynamics (S-NOD) is an excitable decision-making
model inspired by the spiking dynamics of neurons. S-NOD enables the design of
agile decision-making that can rapidly switch between decision options in
response to a changing environment. In S-NOD, decisions are represented by
discrete opinion spikes that evolve in continuous time. Here, we extend
previous analysis of S-NOD and explore its potential as a nonlinear controller
with a tunable balance between robustness and responsiveness to input. We
identify and provide necessary conditions for the bifurcation that determines
the onset of periodic opinion spiking. We leverage this analysis to
characterize the tunability of the input-output threshold for opinion spiking
as a function of the model basal sensitivity and the tunable dependence of
opinion spiking frequency on input magnitude above the threshold. We conclude
with a discussion of S-NOD as a new neuromorphic control block and its
extension to distributed spiking controllers.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecture <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing Cryptographic Software via Typed Assembly Language (Extended
  Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixin Song, Tingzhen Dong, Kosi Nwabueze, Julian Zanders, Andres Erbsen, Adam Chlipala, Mengjia Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Authors of cryptographic software are well aware that their code should not
leak secrets through its timing behavior, and, until 2018, they believed that
following industry-standard constant-time coding guidelines was sufficient.
However, the revelation of the Spectre family of speculative execution attacks
injected new complexities.
  To block speculative attacks, prior work has proposed annotating the
program's source code to mark secret data, with hardware using this information
to decide when to speculate (i.e., when only public values are involved) or not
(when secrets are in play). While these solutions are able to track secret
information stored on the heap, they suffer from limitations that prevent them
from correctly tracking secrets on the stack, at a cost in performance.
  This paper introduces SecSep, a transformation framework that rewrites
assembly programs so that they partition secret and public data on the stack.
By moving from the source-code level to assembly rewriting, SecSep is able to
address limitations of prior work. The key challenge in performing this
assembly rewriting stems from the loss of semantic information through the
lengthy compilation process. The key innovation of our methodology is a new
variant of typed assembly language (TAL), Octal, which allows us to address
this challenge. Assembly rewriting is driven by compile-time inference within
Octal. We apply our technique to cryptographic programs and demonstrate that it
enables secure speculation efficiently, incurring a low average overhead of
$1.2\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter
  1.58-bit LLM Inference <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlun Zhang, Xinyu Li, Shimpei Ando, Kentaro Yoshioka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy
efficiency for CNNs by eliminating runtime weight updates. However, their
scalability to Large Language Models (LLMs) is fundamentally constrained by
their vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA
series - demands more than 1,000 cm2 of silicon area even in advanced CMOS
nodes. This paper presents BitROM, the first CiROM-based accelerator that
overcomes this limitation through co-design with BitNet's 1.58-bit quantization
model, enabling practical and efficient LLM inference at the edge. BitROM
introduces three key innovations: 1) a novel Bidirectional ROM Array that
stores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator
optimized for ternary-weight computations; and 3) an integrated Decode-Refresh
(DR) eDRAM that supports on-die KV-cache management, significantly reducing
external memory access during decoding. In addition, BitROM integrates
LoRA-based adapters to enable efficient transfer learning across various
downstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit
density of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over
prior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%
reduction in external DRAM access, further enhancing deployment efficiency for
LLMs in edge applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ASP-DAC 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoVeriFix: Automatically Correcting Errors and Enhancing Functional
  Correctness in LLM-Generated Verilog Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Tan, Xiangchen Meng, Zijun Jiang, Yangdi Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities in
generating software code for high-level programming languages such as Python
and C++. However, their application to hardware description languages, such as
Verilog, is challenging due to the scarcity of high-quality training data.
Current approaches to Verilog code generation using LLMs often focus on
syntactic correctness, resulting in code with functional errors. To address
these challenges, we present AutoVeriFix, a novel Python-assisted two-stage
framework designed to enhance the functional correctness of LLM-generated
Verilog code. In the first stage, LLMs are employed to generate high-level
Python reference models that define the intended circuit behavior. In the
second stage, these Python models facilitate the creation of automated tests
that guide the generation of Verilog RTL implementations. Simulation
discrepancies between the reference model and the Verilog code are iteratively
used to identify and correct errors, thereby improving the functional accuracy
and reliability of the LLM-generated Verilog code. Experimental results
demonstrate that our approach significantly outperforms existing
state-of-the-art methods in improving the functional correctness of generated
Verilog code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FASE: FPGA-Assisted Syscall Emulation for Rapid End-to-End Processor
  Performance Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzhen Meng, Xiuzhuang Chen, Hongjun Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI workloads and domain-specific architectures has
led to increasingly diverse processor microarchitectures, whose design
exploration requires fast and accurate performance validation. However,
traditional workflows defer validation process until RTL design and SoC
integration are complete, significantly prolonging development and iteration
cycle.
  In this work, we present FASE framework, FPGA-Assisted Syscall Emulation, the
first work for adapt syscall emulation on FPGA platforms, enabling complex
multi-thread benchmarks to directly run on the processor design without
integrating SoC or target OS for early-stage performance validation. FASE
introduces three key innovations to address three critical challenges for
adapting FPGA-based syscall emulation: (1) only a minimal CPU interface is
exposed, with other hardware components untouched, addressing the lack of a
unified hardware interface in FPGA systems; (2) a Host-Target Protocol (HTP) is
proposed to minimize cross-device data traffic, mitigating the low-bandwidth
and high-latency communication between FPGA and host; and (3) a host-side
runtime is proposed to remotely handle Linux-style system calls, addressing the
challenge of cross-device syscall delegation.
  Experiments ware conducted on Xilinx FPGA with open-sourced RISC-V SMP
processor Rocket. With single-thread CoreMark, FASE introduces less than 1%
performance error and achieves over 2000x higher efficiency compared to Proxy
Kernel due to FPGA acceleration. With complex OpenMP benchmarks, FASE
demonstrates over 96% performance validation accuracy for most single-thread
workloads and over 91.5% for most multi-thread workloads compared to full SoC
validation, significantly reducing development complexity and time-to-feedback.
All components of FASE framework are released as open-source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 19 figures, to be submitted to IEEE TCAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aurora: Architecting Argonne's First Exascale Supercomputer for
  Accelerated Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin S. Allen, James Anchell, Victor Anisimov, Thomas Applencourt, Abhishek Bagusetty, Ramesh Balakrishnan, Riccardo Balin, Solomon Bekele, Colleen Bertoni, Cyrus Blackworth, Renzo Bustamante, Kevin Canada, John Carrier, Christopher Chan-nui, Lance C. Cheney, Taylor Childers, Paul Coffman, Susan Coghlan, Michael D'Mello, Murali Emani, Kyle G. Felker, Sam Foreman, Olivier Franza, Longfei Gao, Marta García, María Garzarán, Balazs Gerofi, Yasaman Ghadar, Neha Gupta, Kevin Harms, Väinö Hatanpää, Brian Holland, Carissa Holohan, Brian Homerding, Khalid Hossain, Louise Huot, Huda Ibeid, Joseph A. Insley, Sai Jayanthi, Hong Jiang, Wei Jiang, Xiao-Yong Jin, Jeongnim Kim, Christopher Knight, Kalyan Kumaran, JaeHyuk Kwack, Ti Leggett, Ben Lenard, Chris Lewis, Nevin Liber, Johann Lombardi, Raymond M. Loy, Ye Luo, Bethany Lusch, Nilakantan Mahadevan, Victor A. Mateevitsi, Gordon McPheeters, Ryan Milner, Vitali A. Morozov, Servesh Muralidharan, Tom Musta, Mrigendra Nagar, Vikram Narayana, Marieme Ngom, Anthony-Trung Nguyen, Nathan Nichols, Aditya Nishtala, James C. Osborn, Michael E. Papka, Scott Parker, Saumil S. Patel, Adrian C. Pope, Sucheta Raghunanda, Esteban Rangel, Paul M. Rich, Silvio Rizzi, Kris Rowe, Varuni Sastry, Adam Scovel, Filippo Simini, Haritha Siddabathuni Som, Patrick Steinbrecher, Rick Stevens, Xinmin Tian, Peter Upton, Thomas Uram, Archit K. Vasan, Álvaro Vázquez-Mayagoitia, Kaushik Velusamy, Brice Videau, Venkatram Vishwanath, Brian Whitney, Timothy J. Williams, Michael Woodacre, Sam Zeltner, Gengbin Zheng, Huihuo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aurora is Argonne National Laboratory's pioneering Exascale supercomputer,
designed to accelerate scientific discovery with cutting-edge architectural
innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center
GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth
Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named
Ponte Vecchio) on each compute node. Aurora also integrates the Distributed
Asynchronous Object Storage (DAOS), a novel exascale storage solution, and
leverages Intel's oneAPI programming environment. This paper presents an
in-depth exploration of Aurora's node architecture, the HPE Slingshot
interconnect, the supporting software ecosystem, and DAOS. We provide insights
into standard benchmark performance and applications readiness efforts via
Aurora's Early Science Program and the Exascale Computing Project.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 10 figures. Submitted to J. Supercomputing</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate Algorithms for Verifying Differential Privacy with Gaussian
  Distributions <span class="chip">CCS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishnu Bhusal, Rohit Chadha, A. Prasad Sistla, Mahesh Viswanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The verification of differential privacy algorithms that employ Gaussian
distributions is little understood. This paper tackles the challenge of
verifying such programs by introducing a novel approach to approximating
probability distributions of loop-free programs that sample from both discrete
and continuous distributions with computable probability density functions,
including Gaussian and Laplace. We establish that verifying
$(\epsilon,\delta)$-differential privacy for these programs is \emph{almost
decidable}, meaning the problem is decidable for all values of $\delta$ except
those in a finite set. Our verification algorithm is based on computing
probabilities to any desired precision by combining integral approximations,
and tail probability bounds. The proposed methods are implemented in the tool,
DipApprox, using the FLINT library for high-precision integral computations,
and incorporate optimizations to enhance scalability. We validate {\ourtool} on
fundamental privacy-preserving algorithms, such as Gaussian variants of the
Sparse Vector Technique and Noisy Max, demonstrating its effectiveness in both
confirming privacy guarantees and detecting violations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extended abstract appears in CCS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing Cryptographic Software via Typed Assembly Language (Extended
  Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixin Song, Tingzhen Dong, Kosi Nwabueze, Julian Zanders, Andres Erbsen, Adam Chlipala, Mengjia Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Authors of cryptographic software are well aware that their code should not
leak secrets through its timing behavior, and, until 2018, they believed that
following industry-standard constant-time coding guidelines was sufficient.
However, the revelation of the Spectre family of speculative execution attacks
injected new complexities.
  To block speculative attacks, prior work has proposed annotating the
program's source code to mark secret data, with hardware using this information
to decide when to speculate (i.e., when only public values are involved) or not
(when secrets are in play). While these solutions are able to track secret
information stored on the heap, they suffer from limitations that prevent them
from correctly tracking secrets on the stack, at a cost in performance.
  This paper introduces SecSep, a transformation framework that rewrites
assembly programs so that they partition secret and public data on the stack.
By moving from the source-code level to assembly rewriting, SecSep is able to
address limitations of prior work. The key challenge in performing this
assembly rewriting stems from the loss of semantic information through the
lengthy compilation process. The key innovation of our methodology is a new
variant of typed assembly language (TAL), Octal, which allows us to address
this challenge. Assembly rewriting is driven by compile-time inference within
Octal. We apply our technique to cryptographic programs and demonstrate that it
enables secure speculation efficiently, incurring a low average overhead of
$1.2\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dependent-Type-Preserving Memory Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulette Koronkevich, William J. Bowman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted and received second place at the Student Research
  Competition at Principles of Programming Languages 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Verified Compilation of Floating-point Optimization in
  Scientific Computing Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.09019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.09019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohit Tekriwal, John Sarracino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hiord#: An Approach to the Specification and Verification of
  Higher-Order (C)LP Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Ciccalè, Daniel Jurjo-Rivas, Jose F. Morales, Pedro López-García, Manuel V. Hermenegildo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher-order constructs enable more expressive and concise code by allowing
procedures to be parameterized by other procedures. Assertions allow expressing
partial program specifications, which can be verified either at compile time
(statically) or run time (dynamically). In higher-order programs, assertions
can also describe higher-order arguments. While in the context of (constraint)
logic programming ((C)LP), run-time verification of higher-order assertions has
received some attention, compile-time verification remains relatively
unexplored. We propose a novel approach for statically verifying higher-order
(C)LP programs with higher-order assertions. Although we use the Ciao assertion
language for illustration, our approach is quite general and we believe is
applicable to similar contexts. Higher-order arguments are described using
predicate properties -- a special kind of property which exploits the (Ciao)
assertion language. We refine the syntax and semantics of these properties and
introduce an abstract criterion to determine conformance to a predicate
property at compile time, based on a semantic order relation comparing the
predicate property with the predicate assertions. We then show how to handle
these properties using an abstract interpretation-based static analyzer for
programs with first-order assertions by reducing predicate properties to
first-order properties. Finally, we report on a prototype implementation and
evaluate it through various examples within the Ciao system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Theory and Practice of Logic Programming
  (TPLP)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Data Structures and Algorithms <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dorst-Smeulders Coding for Arbitrary Binary Words <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro De Luca, Gabriele Fici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A binary word is Sturmian if the occurrences of each letter are balanced, in
the sense that in any two factors of the same length, the difference between
the number of occurrences of the same letter is at most 1. In digital geometry,
Sturmian words correspond to discrete approximations of straight line segments
in the Euclidean plane. The Dorst-Smeulders coding, introduced in 1984, is a
4-tuple of integers that uniquely represents a Sturmian word $w$, enabling its
reconstruction using $|w|$ modular operations, making it highly efficient in
practice. In this paper, we present a linear-time algorithm that, given a
binary input word $w$, computes the Dorst-Smeulders coding of its longest
Sturmian prefix. This forms the basis for computing the Dorst-Smeulders coding
of an arbitrary binary word $w$, which is a minimal decomposition (in terms of
the number of factors) of $w$ into Sturmian words, each represented by its
Dorst-Smeulders coding. This coding could be leveraged in compression schemes
where the input is transformed into a binary word composed of long Sturmian
segments. Although the algorithm is conceptually simple and can be implemented
in just a few lines of code, it is grounded in a deep analysis of the
structural properties of Sturmian words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, presented at SPIRE 2025 (proceedings upcoming)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Checking and producing word attractors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marie-Pierre Béal, Maxime Crochemore, Giuseppe Romana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The article focuses on word (or string) attractors, which are sets of
positions related to the text compression efficiency of the underlying word.
The article presents two combinatorial algorithms based on Suffix automata or
Directed Acyclic Word Graphs. The first algorithm decides in linear time
whether a set of positions on the word is an attractor of the word. The second
algorithm generates an attractor for a given word in a greedy manner. Although
this problem is NP-hard, the algorithm is efficient and produces very small
attractors for several well-known families of words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 3 tables, 2 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enumeration kernels for Vertex Cover and Feedback Vertex Set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marin Bougeret, Guilherme C. M. Gomes, Vinicius F. dos Santos, Ignasi Sau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enumerative kernelization is a recent and promising area sitting at the
intersection of parameterized complexity and enumeration algorithms. Its study
began with the paper of Creignou et al. [Theory Comput. Syst., 2017], and
development in the area has started to accelerate with the work of Golovach et
al. [J. Comput. Syst. Sci., 2022]. The latter introduced polynomial-delay
enumeration kernels and applied them in the study of structural
parameterizations of the \textsc{Matching Cut} problem and some variants. Few
other results, mostly on \textsc{Longest Path} and some generalizations of
\textsc{Matching Cut}, have also been developed. However, little success has
been seen in enumeration versions of \textsc{Vertex Cover} and \textsc{Feedback
Vertex Set}, some of the most studied problems in kernelization. In this paper,
we address this shortcoming. Our first result is a polynomial-delay enumeration
kernel with $2k$ vertices for \textsc{Enum Vertex Cover}, where we wish to list
all solutions with at most $k$ vertices. This is obtained by developing a
non-trivial lifting algorithm for the classical crown decomposition reduction
rule, and directly improves upon the kernel with $\mathcal{O}(k^2)$ vertices
derived from the work of Creignou et al. Our other result is a polynomial-delay
enumeration kernel with $\mathcal{O}(k^3)$ vertices and edges for \textsc{Enum
Feedback Vertex Set}; the proof is inspired by some ideas of Thomass\'e [TALG,
2010], but with a weaker bound on the kernel size due to difficulties in
applying the $q$-expansion technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. Accepted at IPEC2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FeynmanDD: Quantum Circuit Analysis with Classical Decision Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Wang, Bin Cheng, Longxiang Yuan, Zhengfeng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications of decision diagrams in quantum circuit analysis have been an
active research area. Our work introduces FeynmanDD, a new method utilizing
standard and multi-terminal decision diagrams for quantum circuit simulation
and equivalence checking. Unlike previous approaches that exploit patterns in
quantum states and operators, our method explores useful structures in the path
integral formulation, essentially transforming the analysis into a counting
problem. The method then employs efficient counting algorithms using decision
diagrams as its underlying computational engine. Through comprehensive
theoretical analysis and numerical experiments, we demonstrate FeynmanDD's
capabilities and limitations in quantum circuit analysis, highlighting the
value of this new BDD-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures, 7 tables. Published in the Proceedings of CAV
  2025. Code available at https://github.com/cqs-thu/feynman-decision-diagram</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyuan Gong, Tongyang Li, Xinzhao Wang, Zhiyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Permutation-Avoiding FFT-Based Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Venkovic, Hartwig Anzt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast Fourier Transform (FFT) libraries are widely used for evaluating
discrete convolutions. Most FFT implementations follow some variant of the
Cooley-Tukey framework, in which the transform is decomposed into butterfly
operations and index-reversal permutations. While butterfly operations dominate
the floating-point operation count, the memory access patterns induced by
index-reversal permutations significantly degrade the FFT's arithmetic
intensity. When performing discrete convolution, the three sets of
index-reversal permutations which occur in FFT-based implementations using
Cooley-Tukey frameworks cancel out, thus paving the way to implementations free
of any permutation. To the best of our knowledge, such permutation-free
variants of FFT-based discrete convolution are not commonly used in practice,
making such kernels worth investigating. Here, we look into such
permutation-avoiding convolution procedures for multi-dimensional cases within
a general radix Cooley-Tukey framework. We perform numerical experiments to
benchmark the algorithms presented against state-of-the-art FFT-based
convolution implementations. Our results suggest that developers of FFT
libraries should consider supporting permutation-avoiding convolution kernels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 22 tables, 2 figures, 22 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compressibility Measures and Succinct Data Structures for Piecewise
  Linear Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Ferragina, Filippo Lari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of deriving compressibility measures for Piecewise
Linear Approximations (PLAs), i.e., error-bounded approximations of a set of
two-dimensional increasing data points using a sequence of segments. Such
approximations are widely used tools in implementing many learned data
structures, which mix learning models with traditional algorithmic design
blocks to exploit regularities in the underlying data distribution, providing
novel and effective space-time trade-offs. We introduce the first lower bounds
to the cost of storing PLAs in two settings, namely compression and indexing.
We then compare these compressibility measures to known data structures, and
show that they are asymptotically optimal up to a constant factor from the
space lower bounds. Finally, we design the first data structures for the
aforementioned settings that achieve the space lower bounds plus small additive
terms, which turn out to be succinct in most practical cases. Our data
structures support the efficient retrieval and evaluation of a segment in the
(compressed) PLA for a given $x$-value, which is a core operation in any
learned data structure relying on PLAs. As a result, our paper offers the first
theoretical analysis of the maximum compressibility achievable by PLA-based
learned data structures, and provides novel storage schemes for PLAs offering
strong theoretical guarantees while also suggesting simple and efficient
practical implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 36th International Symposium on
  Algorithms and Computation (ISAAC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subset Sum in Near-Linear Pseudopolynomial Time and Polynomial Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.04726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.04726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thejas Radhika Sajith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a multiset $A = \{a_1, \dots, a_n\}$ of positive integers and a target
integer $t$, the Subset Sum problem asks if there is a subset of $A$ that sums
to $t$. Bellman's [1957] classical dynamic programming algorithm runs in
$O(nt)$ time and $O(t)$ space. Since then, much work has been done to reduce
both the time and space usage.
  Notably, Bringmann [SODA 2017] uses a two-step color-coding technique to
obtain a randomized algorithm that runs in $\tilde{O}(n+t)$ time and
$\tilde{O}(t)$ space. Jin, Vyas and Williams [SODA 2021] build upon the
algorithm given by Bringmann, using a clever algebraic trick first seen in
Kane's Logspace algorithm, to obtain an $\tilde{O}(nt)$ time and
$\tilde{O}(\log(nt))$ space randomized algorithm. A SETH-based lower-bound
established by Abboud et al. [SODA 2019] shows that Bringmann's algorithm is
likely to have near-optimal time complexity.
  We build on the techniques used by Jin et al. to obtain a randomized
algorithm running in $\tilde{O}(n+t)$ time and $\tilde{O}(n^2 + n \log^2 t)$
space, resulting in an algorithm with near-optimal runtime that also runs in
polynomial space. We use a multipoint evaluation-based approach to speed up a
bottleneck step in their algorithm.
  We also provide a simple polynomial space deterministic algorithm that runs
in $\tilde{O}(n^2t)$ time and $\tilde{O}(n \log^2 t)$ space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages; v2: correction in the deterministic algorithm, main
  randomized algorithm unchanged</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the twin-width of near-regular graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Heinrich, Ferdinand Ihringer, Simon Raßmann, Lena Volk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Twin-width is a recently introduced graph parameter based on the repeated
contraction of near-twins. It has shown remarkable utility in algorithmic and
structural graph theory, as well as in finite model theory -- particularly
since first-order model checking is fixed-parameter tractable when a witness
certifying small twin-width is provided. However, the behavior of twin-width in
specific graph classes, particularly cubic graphs, remains poorly understood.
While cubic graphs are known to have unbounded twin-width, no explicit cubic
graph of twin-width greater than 4 is known.
  This paper explores this phenomenon in regular and near-regular graph
classes. We show that extremal graphs of bounded degree and high twin-width are
asymmetric, partly explaining their elusiveness. Additionally, we establish
bounds for circulant and d-degenerate graphs, and examine strongly regular
graphs, which exhibit similar behavior to cubic graphs. Our results include
determining the twin-width of Johnson graphs over 2-sets, and cyclic Latin
square graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Induced Minor Models. I. Structural Properties and Algorithmic
  Consequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Bousquet, Clément Dallard, Maël Dumas, Claire Hilaire, Martin Milanič, Anthony Perez, Nicolas Trotignon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A graph $H$ is said to be an induced minor of a graph $G$ if $H$ can be
obtained from $G$ by a sequence of vertex deletions and edge contractions.
Equivalently, $H$ is an induced minor of $G$ if there exists an induced minor
model of $H$ in $G$, that is, a collection of pairwise disjoint subsets of
vertices of $G$ labeled by the vertices of $H$, each inducing a connected
subgraph in $G$, such that two vertices of $H$ are adjacent if and only if
there is an edge in $G$ between the corresponding subsets.
  In this paper, we investigate structural properties of induced minor models,
including bounds on treewidth and chromatic number of the subgraphs induced by
minimal induced minor models. It is known that for some graphs $H$, testing
whether a given graph $G$ contains $H$ as an induced minor is an NP-complete
problem. Nevertheless, as algorithmic applications of our structural results,
we make use of recent developments regarding tree-independence number to show
that if $H$ is the $4$-wheel, the $5$-vertex complete graph minus an edge, or a
complete bipartite graph $K_{2,q}$, then there is a polynomial-time algorithm
to find in a given graph $G$ an induced minor model of $H$ in $G$, if there is
one. We also develop an alternative polynomial-time algorithm for recognizing
graphs that do not contain $K_{2,3}$ as an induced minor, which revolves around
the idea of detecting the induced subgraphs whose presence is forced when the
input graph contains $K_{2,3}$ as an induced minor, using the so-called
shortest path detector. It turns out that all these induced subgraphs are
Truemper configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Section 6 contains the main results of the first version of this
  preprint (arXiv:2402.08332v1), which has been superseded by the current
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical-Computational Trade-offs for Recursive Adaptive Partitioning
  Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Shuo Tan, Jason M. Klusowski, Krishnakumar Balasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models based on recursive adaptive partitioning such as decision trees and
their ensembles are popular for high-dimensional regression as they can
potentially avoid the curse of dimensionality. Because empirical risk
minimization (ERM) is computationally infeasible, these models are typically
trained using greedy algorithms. Although effective in many cases, these
algorithms have been empirically observed to get stuck at local optima. We
explore this phenomenon in the context of learning sparse regression functions
over $d$ binary features, showing that when the true regression function $f^*$
does not satisfy Abbe et al. (2022)'s Merged Staircase Property (MSP), greedy
training requires $\exp(\Omega(d))$ to achieve low estimation error.
Conversely, when $f^*$ does satisfy MSP, greedy training can attain small
estimation error with only $O(\log d)$ samples. This dichotomy mirrors that of
two-layer neural networks trained with stochastic gradient descent (SGD) in the
mean-field regime, thereby establishing a head-to-head comparison between
SGD-trained neural networks and greedy recursive partitioning estimators.
Furthermore, ERM-trained recursive partitioning estimators achieve low
estimation error with $O(\log d)$ samples irrespective of whether $f^*$
satisfies MSP, thereby demonstrating a statistical-computational trade-off for
greedy training. Our proofs are based on a novel interpretation of greedy
recursive partitioning using stochastic process theory and a coupling technique
that may be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computing Tree Decompositions with Small Independence Number 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09993v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09993v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Dallard, Fedor V. Fomin, Petr A. Golovach, Tuukka Korhonen, Martin Milanič
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The independence number of a tree decomposition is the maximum of the
independence numbers of the subgraphs induced by its bags. The
tree-independence number of a graph is the minimum independence number of a
tree decomposition of it. Several NP-hard graph problems, like maximum weight
independent set, can be solved in time n^{O(k)} if the input n-vertex graph is
given together with a tree decomposition of independence number k. Yolov, in
[SODA 2018], gave an algorithm that, given an n-vertex graph G and an integer
k, in time n^{O(k^3)} either constructs a tree decomposition of G whose
independence number is O(k^3) or correctly reports that the tree-independence
number of G is larger than k.
  In this paper, we first give an algorithm for computing the tree-independence
number with a better approximation ratio and running time and then prove that
our algorithm is, in some sense, the best one can hope for. More precisely, our
algorithm runs in time 2^{O(k^2)} n^{O(k)} and either outputs a tree
decomposition of G with independence number at most $8k$, or determines that
the tree-independence number of G is larger than k. This implies 2^{O(k^2)}
n^{O(k)}-time algorithms for various problems, like maximum weight independent
set, parameterized by the tree-independence number k without needing the
decomposition as an input. Assuming Gap-ETH, an n^{\Omega(k)} factor in the
running time is unavoidable for any approximation algorithm for the
tree-independence number.
  Our second result is that the exact computation of the tree-independence
number is para-NP-hard: We show that for every constant k \ge 4 it is NP-hard
to decide if a given graph has the tree-independence number at most k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Small corrections were made</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tight Lieb-Robinson Bound for approximation ratio in Quantum Annealing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Braida, Simon Martiel, Ioan Todinca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum annealing (QA) holds promise for optimization problems in quantum
computing, especially for combinatorial optimization. This analog framework
attracts attention for its potential to address complex problems. Its
gate-based homologous, QAOA with proven performance, has brought lots of
attention to the NISQ era. Several numerical benchmarks try to classify these
two metaheuristics however, classical computational power highly limits the
performance insights. In this work, we introduce a new parametrized version of
QA enabling a precise 1-local analysis of the algorithm. We develop a tight
Lieb-Robinson bound for regular graphs, achieving the best-known numerical
value to analyze QA locally. Studying MaxCut over cubic graph as a benchmark
optimization problem, we show that a linear-schedule QA with a 1-local analysis
achieves an approximation ratio over 0.7020, outperforming any known 1-local
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Single-Choice Prophet Inequalities from Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.07945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.07945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviad Rubinstein, Jack Z. Wang, S. Matthew Weinberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the single-choice Prophet Inequality problem when the gambler is
given access to samples. We show that the optimal competitive ratio of $1/2$
can be achieved with a single sample from each distribution. When the
distributions are identical, we show that for any constant $\varepsilon > 0$,
$O(n)$ samples from the distribution suffice to achieve the optimal competitive
ratio ($\approx 0.745$) within $(1+\varepsilon)$, resolving an open problem of
Correa, D\"utting, Fischer, and Schewior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears in Innovations in Theoretical Computer Science (ITCS) 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharp Online Hardness for Large Balanced Independent Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Dhawan, Eren C. Kızıldağ, Neeladri Maitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the algorithmic problem of finding large $\gamma$-balanced
independent sets in dense random bipartite graphs; an independent set is
$\gamma$-balanced if a $\gamma$ proportion of its vertices lie on one side of
the bipartition. In the sparse regime, Perkins and Wang established tight
bounds within the low-degree polynomial (LDP) framework, showing a
factor-$1/(1-\gamma)$ statistical-computational gap via the Overlap Gap
Property (OGP) framework tailored for stable algorithms. However, these
techniques do not appear to extend to the dense setting. For the related large
independent set problem in dense random graph, the best known algorithm is an
online greedy procedure that is inherently unstable, and LDP algorithms are
conjectured to fail even in the "easy" regime where greedy succeeds. We show
that the largest $\gamma$-balanced independent set in dense random bipartite
graphs has size $\alpha:=\frac{\log_b n}{\gamma(1-\gamma)}$ whp, where $n$ is
the size of each bipartition, $p$ is the edge probability, and $b=1/(1-p)$. We
design an online algorithm that achieves $(1-\epsilon)(1-\gamma)\alpha$ whp for
any $\epsilon>0$. We complement this with a sharp lower bound, showing that no
online algorithm can achieve $(1+\epsilon)(1-\gamma)\alpha$ with nonnegligible
probability. Our results suggest that the same factor-$1/(1-\gamma)$ gap is
also present in the dense setting, supporting its conjectured universality.
While the classical greedy procedure on $G(n,p)$ is straightforward, our
algorithm is more intricate: it proceeds in two stages, incorporating a
stopping time and suitable truncation to ensure that $\gamma$-balancedness-a
global constraint-is met despite operating with limited information. Our lower
bound utilizes the OGP framework; we build on a recent refinement of this
framework for online models and extend it to the bipartite setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages; abstract shortened due to arxiv restrictions; this version
  includes a new result involving online algorithms which allow future queries</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-Part: high fidelity and structure coherent shape decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, Chunchao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating 3D shapes at part level is pivotal for downstream applications
such as mesh retopology, UV mapping, and 3D printing. However, existing
part-based generation methods often lack sufficient controllability and suffer
from poor semantically meaningful decomposition. To this end, we introduce
X-Part, a controllable generative model designed to decompose a holistic 3D
object into semantically meaningful and structurally coherent parts with high
geometric fidelity. X-Part exploits the bounding box as prompts for the part
generation and injects point-wise semantic features for meaningful
decomposition. Furthermore, we design an editable pipeline for interactive part
generation. Extensive experimental results show that X-Part achieves
state-of-the-art performance in part-level shape generation. This work
establishes a new paradigm for creating production-ready, editable, and
structurally sound 3D assets. Codes will be released for public research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via
  Camera and Visual Difference Prediction <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng Cai, Robert Wanat, Rafal Mantiuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGGRAPH Asia 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-09T00:00:00Z">2025-09-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Systems and Control <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Markov Decision Process Model for Intrusion Tolerance Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Kreidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formulate and analyze a simplest Markov decision process model for
intrusion tolerance problems, assuming that (i) each attack proceeds through
one or more steps before the system's security fails, (ii) defensive responses
that target these intermediate steps may only sometimes thwart the attack and
(iii) reset responses that are sensible upon discovering an attack's completion
may not always recover from the security failure. The analysis shows that, even
in the ideal case of perfect detectors, it can be sub-optimal in the long run
to employ defensive responses while under attack; that is, depending on attack
dynamics and response effectiveness, the total overhead of ongoing defensive
countermeasures can exceed the total risk of intermittent security failures.
The analysis similarly examines the availability loss versus the risk reduction
of employing preemptive resets, isolating key factors that determine whether
system recovery is best initiated reactively or proactively. We also discuss
model extensions and related work looking towards intrusion tolerance
applications with (i) imperfect or controllable detectors, (ii) multiple types
of attacks, (iii) continuous-time dynamics or (iv) strategic attackers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures, unpublished/rejected manuscript circa 2010</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partitioning and Self-organization of Distributed Generation in Large
  Distribution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr Al Faiya, Stephen McArthur, Ivana Kockar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution networks will experience more installations of distributed
generation (DG) that is unpredictable and stochastic in nature. Greater
distributed control and intelligence will allow challenges such as voltage
control to be handled effectively. The partitioning of power networks into
smaller clusters provides a method to split the control problem into manageable
sub-problems. This paper presents a community detection-based partitioning
technique for distribution networks considering local DGs, allowing them to be
grouped and controlled in a distributed manner by using local signals and
measurements. This method also allows each community to control the voltage
using only neighboring DGs, and for each community to self-organize to reflect
varying DG conditions and to maintain stable control. Simulations demonstrate
that the partitioning of the large distribution network is effective, and each
community is able to self-organize and to regulate the voltage independently
using only its local DGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE General Meeting, 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Topic Projected Opinion Dynamics for Resource Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashil Wankhede, Nirabhra Mandal, Sonia Martínez, Pavankumar Tallapragada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a model of opinion formation on resource allocation among multiple
topics by multiple agents, who are subject to hard budget constraints. We
define a utility function for each agent and then derive a projected dynamical
system model of opinion evolution assuming that each agent myopically seeks to
maximize its utility subject to its constraints. Inter-agent coupling arises
from an undirected social network, while inter-topic coupling arises from
resource constraints. We show that opinions always converge to the equilibrium
set. For special networks with very weak antagonistic relations, the opinions
converge to a unique equilibrium point. We further show that the underlying
opinion formation game is a potential game. We relate the equilibria of the
dynamics and the Nash equilibria of the game and characterize the unique Nash
equilibrium for networks with no antagonistic relations. Finally, simulations
illustrate our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, accepted for presentation in IEEE Conference on
  Decision and Control (CDC), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedback Linearization-based Guidance Law for Guaranteed Interception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Dorsey, Ankit Goel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an input-output feedback linearization (IOL)-based
guidance law to ensure interception in a pursuer-evader engagement scenario. A
point-mass dynamic model for both the pursuer and the evader is considered. An
IOL guidance law is derived using range and line-of-sight (LOS) rate
measurements. It is found that the range-based IOL guidance law exhibits a
singularity under certain conditions. To address this issue, a fuzzy logic
system is employed to smoothly blend the IOL guidance with the classical
proportional guidance law, thereby avoiding the singularity. In contrast, the
LOS-based IOL guidance law is free of singularities but suffers from divergence
issues due to angle-related complications. To resolve this, a simple correction
function is introduced to ensure consistent interception behavior. Results from
Monte Carlo simulations indicate that both modifications of the IOL guidance
laws cause interception with control limits applied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensor Management in Multi-Stage Stochastic Control Problems with
  Imperfect State Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Kreidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technological advancements in miniaturization and wireless communications are
yielding more affordable and versatile sensors and, in turn, more applications
in which a network of sensors can be actively managed to best support overall
decision-making objectives. We propose modeling the opportunity for sensor
management within multi-stage stochastic control problems with imperfect state
information. Such formulations inherently assume the state of the modeled
environment cannot be accessed directly but instead the controller can observe
only noisy measurements of the state and, therefore, at each decision stage
some form of state estimation is required before a control is actuated. The
notion of sensor management arises when the modeled controls not only affect
the subsequent evolution of the state but can also affect the nature of future
measurements and, hence, the quality of state estimates that drive future
control decisions. In principle, the optimal strategy for any appropriately
modeled multi-stage stochastic control problem with imperfect state information
(with or without opportunity for sensor management) is the solution to a
dynamic program; in practice, the computational requirements are typically
prohibitive yet dynamic programming methods are still useful to guide the
development of effective suboptimal strategies. In this spirit, we model the
opportunity for sensor management within small-scale examples of two
well-studied dynamic programming formulations, namely (1) the
finite-state/finite-action Partially-Observable Markov Decision Process
(PO-MDP) and (2) the Linear-Quadratic-Gaussian Regulator (LQGR). These examples
admit solvable dynamic programs and confirm how the interplay between sensing
and acting is a natural by-product of a dynamic programming solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 9 figures, unpublished/unreviewed manuscript circa 2002</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filtering in Multivariate Systems with Quantized Measurements using a
  Gaussian Mixture-Based Indicator Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel L. Cedeño, Rodrigo A. González, Boris I. Godoy, Juan C. Agüero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the problem of state estimation in multivariable dynamic
systems with quantized outputs, a common scenario in applications involving
low-resolution sensors or communication constraints. A novel method is proposed
to explicitly construct the probability mass function associated with the
quantized measurements by approximating the indicator function of each region
defined by the quantizer using Gaussian mixture models. Unlike previous
approaches, this technique generalizes to any number of quantized outputs
without requiring case-specific numerical solutions, making it a scalable and
efficient solution. Simulation results demonstrate that the proposed filter
achieves high accuracy in state estimation, both in terms of fidelity of the
filtering distributions and mean squared error, while maintaining significantly
reduced computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been acepted for presentation in the 64th IEEE
  Conference on Decision and Control. 6 pages, 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swarm-optimized Adaptive Augmentation of Missile Autopilot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Dorsey, Parham Oveissi, Jeffrey D. Barton, Ankit Goel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of optimizing a missile autopilot. In
particular, the paper investigates the application of an online learning
technique to learn and optimize the gains of a three-loop topology autopilot
for a planar missile modeled with nonlinear dynamics and nonlinear aerodynamics
forces and moments. The classical autopilot for a missile is based on a
three-loop topology, where each loop consists of tunable proportional gains. An
adaptive three-loop autopilot is constructed by augmenting the classical
autopilot's fixed-gain controllers with a learning-based controller, which is
recursively optimized using retrospective cost optimization. Numerical
simulations show that online learning improves the tracking performance of the
classical autopilot in both nominal and off-nominal interception scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault Tolerant Control of a Quadcopter using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muzaffar Habib, Adnan Maqsood, Adnan Fayyaz ud Din
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel reinforcement learning (RL)-based control
framework aimed at enhancing the safety and robustness of the quadcopter, with
a specific focus on resilience to in-flight one propeller failure. Addressing
the critical need of a robust control strategy for maintaining a desired
altitude for the quadcopter to safe the hardware and the payload in physical
applications. The proposed framework investigates two RL methodologies Dynamic
Programming (DP) and Deep Deterministic Policy Gradient (DDPG), to overcome the
challenges posed by the rotor failure mechanism of the quadcopter. DP, a
model-based approach, is leveraged for its convergence guarantees, despite high
computational demands, whereas DDPG, a model-free technique, facilitates rapid
computation but with constraints on solution duration. The research challenge
arises from training RL algorithms on large dimensions and action domains. With
modifications to the existing DP and DDPG algorithms, the controllers were
trained not only to cater for large continuous state and action domain and also
achieve a desired state after an inflight propeller failure. To verify the
robustness of the proposed control framework, extensive simulations were
conducted in a MATLAB environment across various initial conditions and
underscoring its viability for mission-critical quadcopter applications. A
comparative analysis was performed between both RL algorithms and their
potential for applications in faulty aerial systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>e-ISSN: 1946-3901, ISSN: 1946-3855,
  https://www.sae.org/publications/technical-papers/content/01-18-01-0006/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prescribed-Time Event-Triggered Control for Matrix-Scaled Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny K P, Rakesh R Warier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proposes a distributed control method for matrix-scaled
multi-agent networks aimed at achieving convergence within a user-defined time
frame. The control law of each individual agent relies only on information from
neighboring agents and is updated at discrete intervals determined by
state-dependent triggering functions, reducing the frequency of agent
interactions. To this end, first, the controller is augmented with a
time-varying gain. Then, the dynamics of the closed-loop system over the
finite-time interval is transformed into an infinite-time frame using time
scaling. Lyapunov-based analysis is employed to derive suitable triggering
conditions that guarantee the asymptotic convergence of the time-transformed
system, thereby ensuring the prescribed-time convergence of the original
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-chip microwave sensing of quasiparticles in tantalum superconducting
  circuits on silicon for scalable quantum technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Poorgholam-Khanjari, Paniz Foshat, Mingqi Zhang, Valentino Seferai, Martin Weides, Kaveh Delfanazari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance and scalability of superconducting quantum circuits are
fundamentally constrained by non-equilibrium quasiparticles, which induce
microwave losses that limit resonator quality factors and qubit coherence
times. Understanding and mitigating these excitations is therefore central to
advancing scalable quantum technologies. Here, we demonstrate on-chip microwave
sensing of quasiparticles in high-Q {\alpha}-tantalum coplanar waveguide
resonators on silicon, operated in the single-photon regime.
Temperature-dependent measurements reveal persistent non-equilibrium
quasiparticles at millikelvin temperatures, producing a measurable suppression
of the internal quality factor (Qi) relative to theoretical expectations. By
benchmarking across materials, we find that the quasiparticle density in
{\alpha}-Ta is approximately one-third that of NbN at equivalent normalised
temperatures (T/Tc), directly correlating with reduced microwave loss. Our
methodology establishes a scalable platform for probing quasiparticle dynamics
and points towards new routes for engineering superconducting circuits with
improved coherence, with impact on qubit readout resonators, kinetic-inductance
detectors, and emerging quantum processors and sensors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A kernel-based approach to physics-informed nonlinear system
  identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cesare Donati, Martina Mammarella, Giuseppe C. Calafiore, Fabrizio Dabbene, Constantino Lagoa, Carlo Novara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a kernel-based framework for physics-informed nonlinear
system identification. The key contribution is a structured methodology that
extends kernel-based techniques to seamlessly integrate partially known
physics-based models, improving parameter estimation and overall model
accuracy. The proposed method enhances traditional modeling approaches by
integrating a parametric model, which provides physical interpretability, with
a kernel-based function, which accounts for unmodelled dynamics. The two
model's components are identified from data simultaneously, minimizing a
suitable cost that balances the relative importance of the physical and the
black-box parts of the model. Additionally, nonlinear state smoothing is
employed to address scenarios involving state-space models with not fully
measurable states. Numerical simulations on an experimental benchmark system
demonstrate the effectiveness of the proposed approach, with performance
comparisons against state-of-the-art identification techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion
  Control? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gavin Tao, Yinuo Wang, Jinzhao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end reinforcement learning for motion control promises unified
perception-action policies that scale across embodiments and tasks, yet most
deployed controllers are either blind (proprioception-only) or rely on fusion
backbones with unfavorable compute-memory trade-offs. Recurrent controllers
struggle with long-horizon credit assignment, and Transformer-based fusion
incurs quadratic cost in token length, limiting temporal and spatial context.
We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a
selective state-space backbone that applies state-space duality (SSD) to enable
both recurrent and convolutional scanning with hardware-aware streaming and
near-linear scaling. Proprioceptive states and exteroceptive observations
(e.g., depth tokens) are encoded into compact tokens and fused by stacked
SSD-Mamba2 layers. The selective state-space updates retain long-range
dependencies with markedly lower latency and memory use than quadratic
self-attention, enabling longer look-ahead, higher token resolution, and stable
training under limited compute. Policies are trained end-to-end under curricula
that randomize terrain and appearance and progressively increase scene
complexity. A compact, state-centric reward balances task progress, energy
efficiency, and safety. Across diverse motion-control scenarios, our approach
consistently surpasses strong state-of-the-art baselines in return, safety
(collisions and falls), and sample efficiency, while converging faster at the
same compute budget. These results suggest that SSD-Mamba2 provides a practical
fusion backbone for scalable, foresightful, and efficient end-to-end motion
control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Dynamic Programming for the Optimal Control Problem with an
  Ellipsoidal Target Set and Its Statistical Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjun Eom, Gyunghoon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses an extended class of optimal control problems where a
target for a system state has the form of an ellipsoid rather than a fixed,
single point. As a computationally affordable method for resolving the extended
problem, we present a revised version of the differential dynamic programming
(DDP), termed the differential dynamic programming with ellipsoidal target set
(ETS-DDP). To this end, the problem with an ellipsoidal target set is
reformulated into an equivalent form with the orthogonal projection operator,
yielding that the resulting cost functions turn out to be discontinuous at some
points. As the DDP usually requires the differentiability of cost functions, in
the ETS-DDP formulation we locally approximate the (nonsmooth) cost functions
to smoothed ones near the path generated at the previous iteration, by
utilizing the explicit form of the orthogonal projection operator. Moreover, a
statistical inference method is also presented for designing the ellipsoidal
target set, based on data on admissible target points collected by expert
demonstrations. Via a simulation on autonomous parking of a vehicle, it is seen
that the proposed ETS-DDP efficiently derives an admissible state trajectory
while running much faster than the point-targeted DDP, at the expense of
optimality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25th International Conference on Control, Automation and Systems
  (ICCAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Non-Conservative Contingency Planning for Autonomous Vehicles
  via Online Learning-Based Reachable Set Barriers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Lei Zheng, Shuzhi Sam Ge, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles must navigate dynamically uncertain environments while
balancing the safety and driving efficiency. This challenge is exacerbated by
the unpredictable nature of surrounding human-driven vehicles (HVs) and
perception inaccuracies, which require planners to adapt to evolving
uncertainties while maintaining safe trajectories. Overly conservative planners
degrade driving efficiency, while deterministic approaches may encounter
serious issues and risks of failure when faced with sudden and unexpected
maneuvers. To address these issues, we propose a real-time contingency
trajectory optimization framework in this paper. By employing event-triggered
online learning of HV control-intent sets, our method dynamically quantifies
multi-modal HV uncertainties and refines the forward reachable set (FRS)
incrementally. Crucially, we enforce invariant safety through FRS-based barrier
constraints that ensure safety without reliance on accurate trajectory
prediction of HVs. These constraints are embedded in contingency trajectory
optimization and solved efficiently through consensus alternative direction
method of multipliers (ADMM). The system continuously adapts to the
uncertainties in HV behaviors, preserving feasibility and safety without
resorting to excessive conservatism. High-fidelity simulations on highway and
urban scenarios, as well as a series of real-world experiments demonstrate
significant improvements in driving efficiency and passenger comfort while
maintaining safety under uncertainty. The project page is available at
https://pathetiue.github.io/frscp.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electric Vehicle Routing Problem with Time Windows and Station-based or
  Route-based Charging Options 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tran Trung Duc, Vu Duc Minh, Nguyen Ngoc Doanh, Pham Gia Nguyen, Laurent El Ghaoui, Ha Minh Hoang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Electric Vehicle Routing Problem with Time Windows and Station-based or
Route-based Charging Options addresses fleet optimization incorporating both
conventional charging stations and continuous wireless charging infrastructure.
This paper extends Schneider et al.'s foundational EVRP-TW model with arc-based
dynamic wireless charging representation, partial coverage modeling, and
hierarchical multi-objective optimization prioritizing fleet minimization.
Computational experiments on Schneider benchmark instances demonstrate
substantial operational benefits, with distance and time improvements ranging
from 0.7% to 35.9% in secondary objective components. Analysis reveals that 20%
wireless coverage achieves immediate benefits, while 60% coverage delivers
optimal performance across all test instances for infrastructure investment
decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A smart fridge with AI-enabled food computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khue Nong Thuc, Khoa Tran Nguyen Anh, Tai Nguyen Huy, Du Nguyen Hao Hong, Khanh Dinh Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of Things (IoT) plays a crucial role in enabling seamless
connectivity and intelligent home automation, particularly in food management.
By integrating IoT with computer vision, the smart fridge employs an ESP32-CAM
to establish a monitoring subsystem that enhances food management efficiency
through real-time food detection, inventory tracking, and temperature
monitoring. This benefits waste reduction, grocery planning improvement, and
household consumption optimization. In high-density inventory conditions,
capturing partial or layered images complicates object detection, as
overlapping items and occluded views hinder accurate identification and
counting. Besides, varied angles and obscured details in multi-layered setups
reduce algorithm reliability, often resulting in miscounts or
misclassifications. Our proposed system is structured into three core modules:
data pre-processing, object detection and management, and a web-based
visualization. To address the challenge of poor model calibration caused by
overconfident predictions, we implement a variant of focal loss that mitigates
over-confidence and under-confidence in multi-category classification. This
approach incorporates adaptive, class-wise error calibration via temperature
scaling and evaluates the distribution of predicted probabilities across
methods. Our results demonstrate that robust functional calibration
significantly improves detection reliability under varying lighting conditions
and scalability challenges. Further analysis demonstrates a practical,
user-focused approach to modern food management, advancing sustainable living
goals through reduced waste and more informed consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Event-Triggered MPC for Linear Parameter-Varying Systems with
  State Delays, Actuator Saturation and Disturbances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiping Zhong, Wanlin Lu, Langwen Zhang, Ziyang Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a unified adaptive event-triggered model predictive
control (ETMPC) scheme for linear parameter-varying (LPV) systems subject to
state delays, actuator saturation, and external disturbances. In existing
studies, only a limited number of ETMPC methods have attempted to address
either state delays or actuator saturation, and even these few methods
typically lack co-design optimization between adaptive event-triggering
mechanisms and the control law. To overcome these limitations, this paper
presents a Lyapunov-Krasovskii-based adaptive ETMPC strategy that enables the
co-design optimization of both the triggering mechanism and the controller.
Specifically, the event-triggering parameter matrix is adaptively optimized by
embedding an internal adaptive variable within the Lyapunov-Krasovskii-like
function. Furthermore, the actuator saturation nonlinearity is transformed into
a convex hull representation. The infinite-horizon robust optimization problem
is reformulated as a convex optimization problem with linear matrix inequality
(LMI) constraints. Invariant set constraints are introduced to ensure recursive
feasibility, and mean-square input-to-state stability (ISS) under multiple
uncertainties is rigorously established. Simulations on an industrial electric
heating system validate the proposed method's effectiveness in reducing
communication load.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anti-Disturbance Hierarchical Sliding Mode Controller for Deep-Sea
  Cranes with Adaptive Control and Neural Network Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zuo, Shujie Wu, Yuzhe Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address non-linear disturbances and uncertainties in complex marine
environments, this paper proposes a disturbance-resistant controller for
deep-sea cranes. The controller integrates hierarchical sliding mode control,
adaptive control, and neural network compensation techniques. By designing a
global sliding mode surface, the dynamic coordination between the driving and
non-driving subsystems is achieved, ensuring overall system stability. The
subsystem surfaces reduce oscillations and enhance tracking accuracy. Adaptive
control dynamically adjusts system parameters, enhancing robustness against
external uncertainties, while the neural network compensates for time-varying
disturbances through real-time learning. The stability of the control scheme is
verified on the basis of Lyapunov theory. The simulation results demonstrate
that, compared to traditional PID control, the proposed controller exhibits
significant advantages in trajectory tracking accuracy, response speed, and
disturbance rejection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Frequency Control for Multi-Area Power Systems Considering
  Transient Frequency Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiemin Mo, Tao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High penetration of renewable energy sources intensifies frequency
fluctuations in multi-area power systems, challenging both stability and
operational safety. This paper proposes a novel distributed frequency control
method that ensures transient frequency safety and enforces generation capacity
constraints, while achieving steady-state frequency restoration and optimal
economic operation. The method integrates a feedback optimization (FO)-based
controller and a safety corrector. The FO-based controller generates reference
setpoints by solving an optimization problem, driving the system to the steady
state corresponding to the optimal solution of this problem. The safety
corrector then modifies these references using control barrier functions to
maintain frequencies within prescribed safe bounds during transients while
respecting capacity constraints. The proposed method combines low computational
burden with improved regulation performance and enhanced practical
applicability. Theoretical analysis establishes optimality, asymptotic
stability, and transient frequency safety for the closed-loop system.
Simulation studies show that, compared with conventional FO-based schemes, the
method consistently enforces frequency safety and capacity limits, achieves
smaller frequency deviations and faster recovery, thereby demonstrating its
practical effectiveness and advantages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-knowledge fusion driven frequency security assessment: A robust
  framework for renewable-dominated power grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurun Zhang, Wei Yao, Yutian Lan, Hang Shuai, Shanyang Wei, Wei Gan, Chao Duan, Jinyu Wen, Shijie Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frequency security is critical for power grids, as deviations can trigger
widespread outages and result in substantial economic losses. However, modern
renewable-dominated power grids face an increased risk of insecurity due to low
inertia and nonlinear frequency responses. To mitigate these risks, robust
pre-fault frequency security assessment (FSA) is critical, which enables grid
operators to implement preventive control strategies. We propose a
data-knowledge fusion framework to achieve intelligent FSA in actual power
grids. First, we classify FSA domain knowledge into two distinct categories:
(1) physics-guided knowledge directs the neural network pre-training process,
ensuring that the fusion model's predictions consistent with frequency response
mechanisms, and (2) physics-constrained knowledge establishes quantitative
relationship on predictions, which forces them within theoretical ranges
defined by domain knowledge. Furthermore, we develop a dual-channel neural
network architecture to simultaneously capture both local and global
characteristics of the power system. Finally, we introduce a data-knowledge
fusion training algorithm that integrates guided learning with constrained
network architecture to enhance model reliability and generalization. Case
studies on China's Yunnan Provincial Power Grid validate the superior
performance of our framework: it reduces average prediction error to 1.26% (a
49.2% reduction over data-driven methods), and maintains 97.60% accuracy in
untrained scenarios (3.85% higher than data-driven methods), therefore
satisfies the accuracy, reliability, and generalization requirements for actual
power grids. The proposed methodology establishes a new paradigm for enhancing
robustness of FSA in power grids, with potential application to cross-domain
security assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Leader-Follower Consensus for Uncertain Multiagent Systems
  with Time-Triggered Switching of the Communication Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armel Koulong, Ali Pakniyat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A distributed adaptive control strategy is developed for heterogeneous
multiagent systems in nonlinear Brunovsky form with \({\pd}\)-dimensional
$n^{\text{th}}$-order dynamics, operating under time-triggered switching
communication topologies. The approach uses repulsive potential functions to
ensure agent-agent and obstacle safety, while neural network estimators
compensate for system uncertainties and disturbances. A high-order control
barrier function framework is then employed to certify the positive invariance
of the safe sets and the boundedness of the proposed control inputs. The
resulting distributed control and adaptive laws, together with dwell-time
requirements for topology transitions, achieve leader-following consensus. This
integrated design provides synchronized formation and robust disturbance
rejection in evolving network configurations, and its effectiveness is
demonstrated through numerical simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Joint submission paper MECC-JDSMC. Accepted for the 2025 Modeling,
  Estimation and Control Conference (MECC). Currently under review by the ASME
  Journal of Dynamic Systems, Measurement, and Control (JDSMC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Linear Pricing Mechanism for Load Management in Day-Ahead Retail
  Energy Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillippe K. Phanivong, Duncan S. Callaway
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulators and utilities have been exploring hourly retail electricity
pricing, with several existing programs providing day-ahead hourly pricing
schedules. At the same time, customers are deploying distributed energy
resources and smart energy management systems that have significant flexibility
and can optimally follow price signals. In aggregate, these optimally
controlled loads can create congestion management issues for distribution
system operators (DSOs). In this paper, we describe a new linear pricing
mechanism for day-ahead retail electricity pricing that provides a signal for
customers to follow to mitigate over-consumption while still consuming energy
at hours that are preferential for system performance. We show that by
broadcasting a linear price designed for price-signal control of
cost-optimizing loads, we can shape customer load profiles to provide
congestion management without the need for bi-directional communication or
customer bidding programs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnergyNet Explained: Internetification of Energy Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Birgersson, Marc A. Weiss, Jimmy Chen, Daniel Kammen, Tomas Kåberger, Franklin Carrero-Martínez, Joakim Wernberg, Michael Menser, Newsha K. Ajami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In developing EnergyNet we have leveraged and are extending lessons from
telecom's shift from a centralized, circuit-switched phone system to
decentralized, packet-switched data networks. EnergyNet utilizes 1) an Energy
Router that enforces galvanic separation and utilizes software-controlled
energy flows over a DC backplane, 2) Energy Local and Wide Area Networks
(ELAN/EWAN) based on DC microgrids that interconnect through an open Energy
Protocol (EP), and 3) a control plane comprised of the Energy Router Operating
System (EROS) and EP Server which is managed at operator scale through an
Energy Network Management System (ENMS). We distinguish the architectural
contribution (Tier-1 including components, interfaces, and operating model)
from expected outcomes contingent on adoption (Tier-2). The latter includes
local-first autonomy with global interoperability, near-real-time operation
with local buffering, removal of EV-charging bottlenecks, freed grid capacity
for data centers and industrial electrification, as well as a trend toward low,
predictable, fixed-cost clean energy. Evidence from early municipal
demonstrators illustrates feasibility and migration paths. The contribution is
a coherent, open, and testable blueprint for software-defined, decentralized
energy distribution, aligning power-systems engineering with networking
principles and offering a practical route from legacy, synchronous grids to
resilient, digitally routed energy distribution systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Admission Control for Inelastic Traffic on a Link Shared by
  Deadline-Driven Elastic Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Kreidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider a (logical) link between two distributed data centers with available
bandwidth designated for both deadline-driven elastic traffic, such as for
scheduled synchronization services, and profitable inelastic traffic, such as
for real-time streaming services. Admission control in this setting is cast as
a stochastic shortest path problem, with state space derived from
(discretization of) the elastic flow's size/deadline and action space
corresponding to alternative subsets of admitted inelastic flows: the
probabilistic model expresses uncertainty in both the link's available
bandwidth and the inelastic flows' offered loads, while the objective function
captures both congestion avoidance and the option to specify a desired minimum
elastic rate. Its solution is shown to (i) balance the accumulation of
instantaneous inelastic reward with the risk of missing the elastic deadline
and (ii) exhibit a degree of robustness to link & flow modeling errors that is
tunable via choice of the desired minimum elastic rate. Also discussed are
state augmentations that befit urgent or non-interruptible inelastic traffic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures, unpublished/rejected manuscript circa 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTM Performance Under Stressing Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Jessen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proliferation of new classes of airspace participants, including uncrewed and
advanced aerial mobility vehicles, necessitates the development and deployment
of novel airspace management solutions, such as the Unmanned Traffic Management
(UTM) system and the Provider of Services to UAM (PSU) Network. The efficacy of
such systems has been demonstrated on multiple occasions via real-world
deployments in limited test environments, however exploration of system
behavior under stressing conditions requires the development of appropriate
modeling and simulation (M&S) environments. Autonomy Networks for Advanced
Mobility at Lincoln Laboratory (ANAMLL) is a virtual Systems Integration
Laboratory (SIL) designed to host federated autonomy networks, such as a UTM or
PSU Network, and to enable test and validation at scales not available in
real-world deployments. As an example of ANAMLL's utility, we explore the
performance of a representative UTM network during a stressing demand scenario.
In a close examination of the demand scenario, ANAMLL demonstrates a UTM system
demand point at which in-flight replanning can no longer be accomplished within
an allowable time window. In a second analysis of the same scenario, ANAMLL
demonstrates the impact of network connectivity performance on end-user
airspace access.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Learning and Coverage of Unknown Fields Using Random-Feature
  Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Du, Ruoyu Lin, Yanning Shen, Magnus Egerstedt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a framework for multi-robot systems to perform
simultaneous learning and coverage of the domain of interest characterized by
an unknown and potentially time-varying density function. To overcome the
limitations of Gaussian Process (GP) regression, we employ Random Feature GP
(RFGP) and its online variant (O-RFGP) that enables online and incremental
inference. By integrating these with Voronoi-based coverage control and Upper
Confidence Bound (UCB) sampling strategy, a team of robots can adaptively focus
on important regions while refining the learned spatial field for efficient
coverage. Under mild assumptions, we provide theoretical guarantees and
evaluate the framework through simulations in time-invariant scenarios.
Furthermore, its effectiveness in time-varying settings is demonstrated through
additional simulations and a physical experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planar Juggling of a Devil-Stick using Discrete VHCs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Khandelwal, Ranjan Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planar juggling of a devil-stick using impulsive inputs is addressed using
the concept of discrete virtual holonomic constraints (DVHC). The location of
the center-of-mass of the devil-stick is specified in terms of its orientation
at the discrete instants when impulsive control inputs are applied. The
discrete zero dynamics (DZD) resulting from the choice of DVHC provides
conditions for stable juggling. A control design that enforces the DVHC and an
orbit stabilizing controller are presented. The approach is validated in
simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence of Batch Asynchronous Stochastic Approximation With
  Applications to Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03445v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03445v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeeva L. Karandikar, M. Vidyasagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We begin by briefly surveying some results on the convergence of the
Stochastic Gradient Descent (SGD) Method, proved in a companion paper by the
present authors. These results are based on viewing SGD as a version of
Stochastic Approximation (SA). Ever since its introduction in the classic paper
of Robbins and Monro in 1951, SA has become a standard tool for finding a
solution of an equation of the form $f(\theta) = 0$, when only noisy
measurements of $f(\cdot)$ are available. In most situations, \textit{every
component} of the putative solution $\theta_t$ is updated at each step $t$. In
some applications in Reinforcement Learning (RL), \textit{only one component}
of $\theta_t$ is updated at each $t$. This is known as \textbf{asynchronous}
SA. In this paper, we study \textbf{Block Asynchronous SA (BASA)}, in which, at
each step $t$, \textit{some but not necessarily all} components of $\theta_t$
are updated. The theory presented here embraces both conventional (synchronous)
SA as well as asynchronous SA, and all in-between possibilities. We provide
sufficient conditions for the convergence of BASA, and also prove bounds on the
\textit{rate} of convergence of $\theta_t$ to the solution. For the case of
conventional SGD, these results reduce to those proved in our companion paper.
Then we apply these results to the problem of finding a fixed point of a map
with only noisy measurements. This problem arises frequently in RL. We prove
sufficient conditions for convergence as well as estimates for the rate of
convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ State Estimation with Protecting Exogenous Inputs via Cramér-Rao Lower
  Bound Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Guo, Jimin Wang, Yanlong Zhao, Ji-Feng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the real-time state estimation problem for dynamic
systems while protecting exogenous inputs against adversaries, who may be
honest-but-curious third parties or external eavesdroppers. The Cram\'er-Rao
lower bound (CRLB) is employed to constrain the mean square error (MSE) of the
adversary's estimate for the exogenous inputs above a specified threshold. By
minimizing the MSE of the state estimate while ensuring a certain privacy level
measured by CRLB, the problem is formulated as a constrained optimization. To
solve the optimization problem, an explicit expression for CRLB is first
provided. As the computational complexity of the CRLB increases with the time
step, a low-complexity approach is proposed to make the complexity independent
of time. Then, a relaxation approach is proposed to efficiently solve the
optimization problem. Finally, a privacy-preserving state estimation algorithm
with low complexity is developed, which also ensures $(\epsilon,
\delta)$-differential privacy. Two illustrative examples, including a practical
scenario for protecting building occupancy, demonstrate the effectiveness of
the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRIME: Fast Primal-Dual Feedback Optimization for Markets with
  Application to Optimal Power Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.16048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.16048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Julian Behr, Mattia Bianchi, Keith Moffat, Saverio Bolognani, Florian Dörfler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Feedback Optimization (OFO) controllers iteratively drive a plant to
an optimal operating point that satisfies input and output constraints, relying
solely on the input-output sensitivity as model information. This paper
introduces PRIME (PRoximal Iterative MarkEts), a novel OFO approach based on
proximal-point iterations. Unlike existing OFO solutions, PRIME admits a
market-based implementation, where self-interested actors are incentivized to
make choices that result in safe and efficient operation, without communicating
private costs or constraints. Furthermore, PRIME can handle non-smooth
objective functions, achieve fast convergence rates and rapid constraint
satisfaction, and effectively reject measurement noise. We demonstrate PRIME on
an AC optimal power flow problem, obtaining an efficient real-time nonlinear
local marginal pricing scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code available at https://github.com/NicholasBehr/prime</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Enhanced Intelligent NIDS Framework: Leveraging Metaheuristic
  Optimization for Robust Attack Detection and Prevention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00896v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00896v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Mahdi Alhusseini, Mohammad Reza Feizi Derakhshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In todays rapidly evolving digital landscape, safeguarding network
infrastructures against cyberattacks has become a critical priority. This
research presents an innovative AI-driven real-time intrusion detection
framework designed to enhance network security, particularly in Wireless Sensor
Networks (WSNs), Cloud Computing (CC), and Internet of Things (IoT)
environments. The system employs classical machine learning models, Logistic
Regression, decision trees, and K-Nearest Neighbors, optimized through the
novel Energy Valley Optimization (EVO) method using the NSL-KDD dataset.
Feature selection significantly reduced the number of input features from 42 to
18, while maintaining strong detection capabilities. The proposed system
achieved 98.95 percent. accuracy with Decision Tree, 98.47 percent with
K-Nearest Neighbors, and 88.84 percent with Logistic Regression. Moreover, high
precision, recall, and F1-scores were attained across all classifiers while
substantially reducing training and testing times, making the framework highly
suitable for real-time applications. To ensure fair detection across diverse
attack types, dataset balancing via Downsampling was applied to address class
imbalance challenges. This investigation focuses on the significance of
advancing IDSs. in cloud computing and WSNs. Overall, this work advances secure
communications by delivering a scalable, low-latency, and high-accuracy
intrusion detection solution aligned with the latest trends in artificial
intelligence, cybersecurity, and real-time digital networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures, Second version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Gradient Waveform Design for Arbitrary $k$-Space Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Luo, Hongzhang Huang, Qinfang Miao, Jian Xu, Peng Hu, Haikun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \textbf{Objective: }To develop a real-time method for designing gradient
waveforms for arbitrary $k$-space trajectories that are time-optimal and
hardware-compliant. \textbf{Methods: }The gradient waveform is solved
recursively under both the slew-rate and the trajectory constraints. The
gradient constraint is enforced by thresholding the $\ell_2$-norm of the next
gradient vector. The constraints form a quadratic equation. To ensure the
existence of the solution, a novel Discrete-Time Forward and Backward Sweep
(DTFBS) strategy is proposed. To ensure the existence of the trajectory
derivatives, the trajectory function is reparameterized as a piecewise cubic
polynomial function with $C^2$ continuity. To ensure trajectory fidelity, the
output gradient waveform is reparameterized by the finite difference of the
trajectory samples. Simulation experiments across seven commonly adopted
non-Cartesian trajectories were conducted to validate generality,
time-optimality, real-time capability, slew-rate accuracy, and improvements
over prior work. Imaging feasibility of the designed time-optimal gradient
waveform was validated in phantom and in vivo experiments. \textbf{Results:
}The proposed method achieves a $>89\%$ reduction in computation time and
simultaneously reduces slew-rate overshoot by $>98\%$ compared to the prior
method across all involved trajectories. The computation time of the proposed
method is shorter than the gradient duration for all tested cases, validating
the real-time capability of the proposed method. \textbf{Conclusions: }The
proposed method enables real-time and hardware-compliant gradient waveform
design, achieving significant reductions in computation time and slew-rate
overshoot compared to the previous method. \textbf{Significance: }This is the
first method achieving real-time gradient waveform design for arbitrary
$k$-space trajectories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Bandwidth and Bode Diagrams based on Scaled Relative Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius P. J. Krebbekx, Roland Tóth, Amritam Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain
method for the analysis of Nonlinear (NL) systems. In this paper, we restrict
the SRG to particular input spaces to compute frequency-dependent incremental
gain bounds for nonlinear systems. This leads to a NL generalization of the
Bode diagram, where the sinusoidal, harmonic, and subharmonic inputs are
considered separately. When applied to the analysis of the NL loop transfer and
sensitivity, we define a notion of bandwidth for both the open-loop and
closed-loop, compatible with the Linear Time-Invariant (LTI) definitions. We
illustrate the power of our method on the analysis of a DC motor with a
parasitic nonlinearity and verify our results in simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, accepted for CDC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearly Controlled Language Generation with Performative Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Cheng, Carmen Amo Alonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of Large Language Models (LMs) in critical
applications highlights the need for controlled language generation strategies
that are not only computationally efficient but that also enjoy performance
guarantees. To achieve this, we use a common model of concept semantics as
linearly represented in an LM's latent space. In particular, we take the view
that natural language generation traces a trajectory in this continuous
semantic space, realized by the language model's hidden activations. This view
permits a control-theoretic treatment of text generation in latent space, in
which we propose a lightweight, gradient-free intervention that dynamically
steers trajectories away from regions corresponding to undesired meanings. In
particular, we propose to directly intervene the activations of the token that
is being generated in embedding space in an online fashion. Crucially, we do
not simply steer activations towards a desirable region. Instead, our method
relies on classical techniques from control theory to precisely control
activations in a context-dependent way, and guarantees that they are brought
into a specific pre-defined region of embedding space that corresponds to
allowed semantics. Our intervention is computed in closed-form according to an
optimal controller formulation, minimally impacting generation time. This
control of the activations in embedding space allows for fine-grained steering
of attributes of the generated sequence. We demonstrate the effectiveness of
our approach on different objectives -- toxicity avoidance and sentiment
control -- while maintaining text quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety Controller Synthesis for Stochastic Networked Systems under
  Communication Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omid Akbarzadeh, Mohammad H. Mamduhi, Abolfazl Lavaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a framework for synthesizing safety controllers for
discrete-time stochastic linear control systems (dt-SLS) operating under
communication imperfections. The control unit is remote and communicates with
the sensor and actuator through an imperfect wireless network. We consider a
constant delay in the sensor-to-controller channel (uplink), and data loss in
both sensor-to-controller and controller-to-actuator (downlink) channels. In
our proposed scheme, data loss in each channel is modeled as an independent
Bernoulli-distributed random process. To systematically handle the uplink
delay, we first introduce an augmented discrete-time stochastic linear system
(dt-ASLS) by concatenating all states and control inputs that sufficiently
represent the state-input evolution of the original dt-SLS under the delay and
packet loss constraints. We then leverage control barrier certificates for
dt-ASLS to synthesize a controller that ensures the stochastic safety of
dt-SLS, guaranteeing that all trajectories remain outside unsafe regions with a
quantified probabilistic bound. Our approach translates safety constraints into
matrix inequalities, leading to an optimization problem that eventually
quantifies the probability of satisfying the safety specification in the
presence of communication imperfections. We validate our results on an RLC
circuit subject to both constant delay and probabilistic data loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid impedance estimation based Kalman Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17325v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17325v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuoc Sang Nguyen, Ghavameddin Nourbakhsh, Gerard Ledwich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern power systems face new operational hurdles due to the increasing
adoption of inverter-coupled distributed energy resources, which impact system
stability and control. Central to these challenges is the dynamic nature of
grid impedance. To address this, a novel real-time estimation algorithm based
on the Discrete Fourier Transform is proposed. This algorithm is embedded
within an Advanced Angle Estimation Kalman Filter framework that employs a
Linear Quadratic Regulator for current control (AAEKF-LQR). The impedance data
directly informs and refines the controller's phase angle estimation.
Simulation analyses demonstrate robust collaboration between the estimator and
controller, sustaining system stability under weak grid conditions. The
technique proves capable of delivering swift and accurate impedance updates
during grid variations, which is crucial for maintaining stable inverter
operation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been withdrawn by the author because it does not
  include grid voltage estimation, which is essential for accurate grid
  impedance estimation. Additional validation and the application of
  appropriate methods for grid voltage estimation are required before the work
  can be finalised</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecture <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HYLU: Hybrid Parallel Sparse LU Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces HYLU, a hybrid parallel LU factorization-based
general-purpose solver designed for efficiently solving sparse linear systems
(Ax=b) on multi-core shared-memory architectures. The key technical feature of
HYLU is the integration of hybrid numerical kernels so that it can adapt to
various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices
from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL
PARDISO in the numerical factorization phase by geometric means of 1.74X (for
one-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from
https://github.com/chenxm1986/hylu.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Task Scheduling in Fog Computing with Deadline Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sadegh Sirjani, Somayeh Sobati-Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Internet of Things (IoT) devices has led to the development of
numerous applications that require quick responses and low latency. Fog
computing has emerged as a solution for processing these IoT applications, but
it faces challenges such as resource allocation and job scheduling. Therefore,
it is crucial to determine how to assign and schedule tasks on Fog nodes. A
well-designed job scheduling algorithm can help decrease energy usage and
improve response times for application requests. This work aims to schedule
tasks in IoT while minimizing the total energy consumption of nodes and
enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into
account task deadlines. Initially, this paper classifies the Fog nodes into two
categories based on their traffic level: low and high. It schedules
low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle
Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization
Algorithm that utilizes genetic operators for discretization. High-deadline
tasks are processed on high-traffic nodes using reinforcement learning (RL).
This combined approach is called the Reinforcement Improved Golden Eagle
Optimization (RIGEO) algorithm. Experimental results demonstrate that the
proposed algorithms optimize system response time, total deadline violation
time, and resource and system energy consumption compared to other
state-of-the-art algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifetime-Aware Design of Item-Level Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shvetank Prakash, Andrew Cheng, Olof Kindgren, Ashiq Ahamed, Graham Knight, Jed Kufel, Francisco Rodriguez, Arya Tschand, David Kong, Mariam Elgamal, Jerry Huang, Emma Chen, Gage Hills, Richard Price, Emre Ozer, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FlexiFlow, a lifetime-aware design framework for item-level
intelligence (ILI) where computation is integrated directly into disposable
products like food packaging and medical patches. Our framework leverages
natively flexible electronics which offer significantly lower costs than
silicon but are limited to kHz speeds and several thousands of gates. Our
insight is that unlike traditional computing with more uniform deployment
patterns, ILI applications exhibit 1000X variation in operational lifetime,
fundamentally changing optimal architectural design decisions when considering
trillion-item deployment scales. To enable holistic design and optimization, we
model the trade-offs between embodied carbon footprint and operational carbon
footprint based on application-specific lifetimes. The framework includes: (1)
FlexiBench, a workload suite targeting sustainability applications from
spoilage detection to health monitoring; (2) FlexiBits, area-optimized RISC-V
cores with 1/4/8-bit datapaths achieving 2.65X to 3.50X better energy
efficiency per workload execution; and (3) a carbon-aware model that selects
optimal architectures based on deployment characteristics. We show that
lifetime-aware microarchitectural design can reduce carbon footprint by 1.62X,
while algorithmic decisions can reduce carbon footprint by 14.5X. We validate
our approach through the first tape-out using a PDK for flexible electronics
with fully open-source tools, achieving 30.9kHz operation. FlexiFlow enables
exploration of computing at the Extreme Edge where conventional design
methodologies must be reevaluated to account for new constraints and
considerations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the capabilities of HLS and RTL tools in the design of an FPGA
  Montgomery Multiplier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rares Ifrim, Decebal Popescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the analysis of various FPGA design implementations of a
Montgomery Modular Multiplier, compatible with the BLS12-381 elliptic curve,
using the Coarsely Integrated Operand Scanning approach of working with
complete partial products on different digit sizes. The scope of the
implemented designs is to achieve a high-frequency, high-throughput solution
capable of computing millions of operations per second, which can provide a
strong foundation for different Elliptic Curve Cryptography operations such as
point addition and point multiplication. One important constraint for our
designs was to only use FPGA DSP primitives for the arithmetic operations
between digits employed in the CIOS algorithm as these primitives, when
pipelined properly, can operate at a high frequency while also relaxing the
resource consumption of FPGA LUTs and FFs. The target of the analysis is to see
how different design choices and tool configurations influence the frequency,
latency and resource consumption when working with the latest AMD-Xilinx tools
and Alveo FPGA boards in an RTL-HLS hybrid approach. We compare three
categories of designs: a Verilog naive approach where we rely on the Vivado
synthesizer to automatically choose when and where to use DSPs, a Verilog
optimized approach by manually instantiating the DSP primitives ourselves and a
complete High-Level Synthesis approach. We also compare the FPGA
implementations with an optimized software implementation of the same
Montgomery multiplier written in Rust.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGe: A Lightweight Algorithm-Architecture Co-Design for Mitigating the
  Data Preparation Bottleneck in Large-Scale Genome Sequence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03732v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03732v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nika Mansouri Ghiasi, Talu Güloglu, Harun Mustafa, Can Firtina, Konstantina Koliogeorgi, Konstantinos Kanellopoulos, Haiyu Mao, Rakesh Nadig, Mohammad Sadrosadati, Jisung Park, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Genome sequence analysis, which analyzes the DNA sequences of organisms,
drives advances in many critical medical and biotechnological fields. Given its
importance and the exponentially growing volumes of genomic sequence data,
there are extensive efforts to accelerate genome sequence analysis. In this
work, we demonstrate a major bottleneck that greatly limits and diminishes the
benefits of state-of-the-art genome sequence analysis accelerators: the data
preparation bottleneck, where genomic sequence data is stored in compressed
form and needs to be decompressed and formatted first before an accelerator can
operate on it. To mitigate this bottleneck, we propose SAGe, an
algorithm-architecture co-design for highly-compressed storage and
high-performance access of large-scale genomic sequence data. The key challenge
is to improve data preparation performance while maintaining high compression
ratios (comparable to genomic-specific compression algorithms) at low hardware
cost. We address this challenge by leveraging key properties of genomic
datasets to co-design (i) a new (de)compression algorithm, (ii) hardware that
decompresses data with lightweight operations and efficient streaming accesses,
(iii) storage data layout, and (iv) interface commands to access data. SAGe is
highly versatile as it supports datasets from different sequencing technologies
and species. Thanks to its lightweight design, SAGe can be seamlessly
integrated with a broad range of genome sequence analysis hardware accelerators
to mitigate their data preparation bottlenecks. Our results demonstrate that
SAGe improves the average end-to-end performance and energy efficiency of two
state-of-the-art genome sequence analysis accelerators by 3.0x-32.1x and
13.0x-34.0x, respectively, compared to when the accelerators rely on
state-of-the-art decompression tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUADOL: A Quality-Driven Approximate Logic Synthesis Method Exploiting
  Dual-Output LUTs for Modern FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Shi, Xuan Wang, Chang Meng, Weikang Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate computing is a new computing paradigm. One important area of it
is designing approximate circuits for FPGA. Modern FPGAs support dual-output
LUT, which can significantly reduce the area of FPGA designs. Several existing
works explored the use of dual-output in approximate computing. However, they
are limited to small-scale arithmetic circuits. To address the problem, this
work proposes QUADOL, a quality-driven ALS method by exploiting dual-output
LUTs for modern FPGAs. We propose a technique to approximately merge two
single-output LUTs (i.e., a LUT pair) into a dual-output LUT. In addition, we
transform the problem of selecting multiple LUT pairs for simultaneous
approximate merging into a maximum matching problem to maximize the area
reduction. Since QUADOL exploits a new dimension, i.e., approximately merging a
LUT pair into a dual-output LUT, it can be integrated with any existing ALS
methods to strengthen them. Therefore, we also introduce QUADOL+, which is a
generic framework to integrate QUADOL into existing ALS methods. The
experimental results showed that QUADOL+ can reduce the LUT count by up to 18%
compared to the state-of-the-art ALS methods for FPGA. Moreover, the
approximate multipliers optimized by QUADOL+ dominate most prior FPGA-based
approximate multipliers in the area-error plane.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work will be incorporated into a larger project and will undergo
  significant revisions. As such, it will be resubmitted as part of a new
  paper. Thank you for your understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure
  HBM Accelerators <span class="chip">CCS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, Chenghong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Trusted Execution Environments provide a strong foundation for secure
cloud computing, they remain vulnerable to access pattern leakages. Oblivious
Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high
overhead due to randomized remapping and worst-case padding. We argue these
costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory
(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that
eavesdropping on HBM is difficult -- even for physical attackers -- as its
memory channels are sealed together with processor cores inside the same
physical package. Later, Hunt et al. [NSDI'20] show that, with proper
isolation, HBM can be turned into an unobservable region where both data and
memory traces are hidden. This motivates a rethink of OMAP design with
HBM-backed solutions to finally overcome their traditional performance limits.
Building on these insights, we present BOLT, a Bandwidth Optimized,
Lightning-fast OMAP accelerator that, for the first time, achieves O(1) +
O(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:
(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache
to accelerate oblivious access to large host memory; (ii) a self-hosted
architecture that offloads execution and memory control from the host to
mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs
that maximize resource efficiency. We implement a prototype BOLT on a Xilinx
U55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in
initialization and query time, respectively, over state-of-the-art OMAPs,
including an industry implementation from Facebook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CCS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KLLM: Fast LLM Inference with K-Means Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueying Wu, Baijun Zhou, Zhihui Gao, Yuzhe Fu, Qilin Zheng, Yintao He, Hai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. Traditional WAQ designs rely on uniform integer
quantization for hardware efficiency, but often suffer from significant model
performance degradation at low precision. In contrast, K-Means quantization, a
non-uniform technique, achieves higher accuracy by aligning with the
Gaussian-like distributions of weights and activations in LLMs. However, two
key challenges prevent the efficient deployment of K-Means-based WAQ designs
for LLM inference: (1) The non-uniform structure of K-Means-quantized data
precludes direct execution on low-precision compute units, necessitating
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers hinder effective low-precision quantization.
Offline thresholding methods for outlier detection degrade model performance
substantially, while existing online detection techniques introduce significant
runtime overhead. To address the aforementioned challenges and fully unleash
the potential of K-Means-based WAQ for LLM inference, in this paper, we propose
KLLM, an LLM inference accelerator for efficient execution with
K-Means-quantized weights and activations. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a lightweight outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring
  Motivations in Open-Source Projects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikel Robredo, Matteo Esposito, Fabio Palomba, Rafael Peñaloza, Valentina Lenarduzzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context. Code refactoring improves software quality without changing external
behavior. Despite its advantages, its benefits are hindered by the considerable
cost of time, resources, and continuous effort it demands. Aim. Understanding
why developers refactor, and which metrics capture these motivations, may
support wider and more effective use of refactoring in practice. Method. We
performed a large-scale empirical study to analyze developers refactoring
activity, leveraging Large Language Models (LLMs) to identify underlying
motivations from version control data, comparing our findings with previous
motivations reported in the literature. Results. LLMs matched human judgment in
80% of cases, but aligned with literature-based motivations in only 47%. They
enriched 22% of motivations with more detailed rationale, often highlighting
readability, clarity, and structural improvements. Most motivations were
pragmatic, focused on simplification and maintainability. While metrics related
to developer experience and code readability ranked highest, their correlation
with motivation categories was weak. Conclusions. We conclude that LLMs
effectively capture surface-level motivations but struggle with architectural
reasoning. Their value lies in providing localized explanations, which, when
combined with software metrics, can form hybrid approaches. Such integration
offers a promising path toward prioritizing refactoring more systematically and
balancing short-term improvements with long-term architectural goals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What's in the Box: Ergonomic and Expressive Capture Tracking over
  Generic Data Structures (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Xu, Oliver Bračevac, Cao Nguyen Pham, Martin Odersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing types in Scala unify static effect and resource tracking with
object capabilities, enabling lightweight effect polymorphism with minimal
notational overhead. However, their expressiveness has been insufficient for
tracking capabilities embedded in generic data structures, preventing them from
scaling to the standard collections library -- an essential prerequisite for
broader adoption. This limitation stems from the inability to name capabilities
within the system's notion of box types.
  This paper develops System Capless, a new foundation for capturing types that
provides the theoretical basis for reach capabilities (rcaps), a novel
mechanism for naming "what's in the box." The calculus refines the universal
capability notion into a new scheme with existential and universal capture set
quantification. Intuitively, rcaps witness existentially quantified capture
sets inside the boxes of generic types in a way that does not require exposing
existential capture types in the surface language. We have fully mechanized the
formal metatheory of System Capless in Lean, including proofs of type soundness
and scope safety. System Capless supports the same lightweight notation of
capturing types plus rcaps, as certified by a type-preserving translation, and
also enables fully optional explicit capture-set quantification to increase
expressiveness.
  Finally, we present a full reimplementation of capture checking in Scala 3
based on System Capless and migrate the entire Scala collections library and an
asynchronous programming library to evaluate its practicality and ergonomics.
Our results demonstrate that reach capabilities enable the adoption of capture
checking in production code with minimal changes and minimal-to-zero notational
overhead in a vast majority of cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Extensible Hybrid Embeddings with Micros 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Bocirnea, William J. Bowman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Macro embedding is a popular approach to defining extensible shallow
embeddings of object languages in Scheme like host languages. While macro
embedding has even been shown to enable implementing extensible typed languages
in systems like Racket, it comes at a cost: compile-time performance. In this
paper, we revisit micros - syntax to intermediate representation (IR)
transformers, rather than source syntax to source syntax transformers (macros).
Micro embedding enables stopping at an IR, producing a deep embedding and
enabling high performance compile-time functions over an efficient IR, before
shallowly embedding the IR back into source syntax. Combining micros with
several design patterns to enable the IR and functions over it to be
extensible, we achieve extensible hybrid embedding of statically typed
languages with significantly improved compile-time compared to macro-embedding
approaches. We describe our design patterns and propose new abstractions
packaging these patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XML <span class="highlight-title">Prompt</span>ing as Grammar-Constrained Interaction: Fixed-Point Semantics,
  Convergence Guarantees, and Human-AI Protocols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faruk Alpay, Taylan Alpay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, multiple XML prompts</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Data Structures and Algorithms <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressibility Measures and Succinct Data Structures for Piecewise
  Linear Approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Ferragina, Filippo Lari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of deriving compressibility measures for \emph{Piecewise
Linear Approximations} (PLAs), i.e., error-bounded approximations of a set of
two-dimensional {\em increasing} data points using a sequence of segments. Such
approximations are widely used tools in implementing many \emph{learned data
structures}, which mix learning models with traditional algorithmic design
blocks to exploit regularities in the underlying data distribution, providing
novel and effective space-time trade-offs.
  We introduce the first lower bounds to the cost of storing PLAs in two
settings, namely {\em compression} and {\em indexing}. We then compare these
compressibility measures to known data structures, and show that they are
asymptotically optimal up to a constant factor from the space lower bounds.
Finally, we design the first data structures for the aforementioned settings
that achieve the space lower bounds plus small additive terms, which turn out
to be {\em succinct} in most practical cases. Our data structures support the
efficient retrieval and evaluation of a segment in the (compressed) PLA for a
given $x$-value, which is a core operation in any learned data structure
relying on PLAs.
  As a result, our paper offers the first theoretical analysis of the maximum
compressibility achievable by PLA-based learned data structures, and provides
novel storage schemes for PLAs offering strong theoretical guarantees while
also suggesting simple and efficient practical implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proximity Graphs for Similarity Search: Fast Construction, Lower Bounds,
  and Euclidean Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangqi Lu, Yufei Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proximity graph-based methods have emerged as a leading paradigm for
approximate nearest neighbor (ANN) search in the system community. This paper
presents fresh insights into the theoretical foundation of these methods. We
describe an algorithm to build a proximity graph for $(1+\epsilon)$-ANN search
that has $O((1/\epsilon)^\lambda \cdot n \log \Delta)$ edges and guarantees
$(1/\epsilon)^\lambda \cdot \text{polylog }\Delta$ query time. Here, $n$ and
$\Delta$ are the size and aspect ratio of the data input, respectively, and
$\lambda = O(1)$ is the doubling dimension of the underlying metric space. Our
construction time is near-linear to $n$, improving the $\Omega(n^2)$ bounds of
all previous constructions. We complement our algorithm with lower bounds
revealing an inherent limitation of proximity graphs: the number of edges needs
to be at least $\Omega((1/\epsilon)^\lambda \cdot n + n \log \Delta)$ in the
worst case, up to a subpolynomial factor. The hard inputs used in our
lower-bound arguments are non-geometric, thus prompting the question of whether
improvement is possible in the Euclidean space (a key subclass of metric
spaces). We provide an affirmative answer by using geometry to reduce the graph
size to $O((1/\epsilon)^\lambda \cdot n)$ while preserving nearly the same
query and construction time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Bounds for Low-Error Frequency Moment Estimation and the Power of
  Multiple Passes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naomi Green-Maimon, Or Zamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the second frequency moment $F_2$ of a data stream up to a $(1 \pm
\varepsilon)$ factor is a central problem in the streaming literature. For
errors $\varepsilon > \Omega(1/\sqrt{n})$, the tight bound
$\Theta\left(\log(\varepsilon^2 n)/\varepsilon^2\right)$ was recently
established by Braverman and Zamir. In this work, we complete the picture by
resolving the remaining regime of small error, $\varepsilon < 1/\sqrt{n}$,
showing that the optimal space complexity is $\Theta\left( \min\left(n,
\frac{1}{\varepsilon^2} \right) \cdot \left(1 + \left| \log(\varepsilon^2 n)
\right| \right) \right)$ bits for all $\varepsilon \geq 1/n^2$, assuming a
sufficiently large universe. This closes the gap between the best known
$\Omega(n)$ lower bound and the straightforward $O(n \log n)$ upper bound in
that range, and shows that essentially storing the entire stream is necessary
for high-precision estimation.
  To derive this bound, we fully characterize the two-party communication
complexity of estimating the size of a set intersection up to an arbitrary
additive error $\varepsilon n$. In particular, we prove a tight $\Omega(n \log
n)$ lower bound for one-way communication protocols when $\varepsilon <
n^{-1/2-\Omega(1)}$, in contrast to classical $O(n)$-bit protocols that use
two-way communication. Motivated by this separation, we present a two-pass
streaming algorithm that computes the exact histogram of a stream with high
probability using only $O(n \log \log n)$ bits of space, in contrast to the
$\Theta(n \log n)$ bits required in one pass even to approximate $F_2$ with
small error. This yields the first asymptotic separation between one-pass and
$O(1)$-passes space complexity for small frequency moment estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The General Expiration Streaming Model: Diameter, $k$-Center, Counting,
  Sampling, and Friends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lotte Blank, Sergio Cabello, MohammadTaghi Hajiaghayi, Robert Krauthgamer, Sepideh Mahabadi, André Nusser, Jeff M. Phillips, Jonas Sauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important thread in the study of data-stream algorithms focuses on
settings where stream items are active only for a limited time. We introduce a
new expiration model, where each item arrives with its own expiration time. The
special case where items expire in the order that they arrive, which we call
consistent expirations, contains the classical sliding-window model of Datar,
Gionis, Indyk, and Motwani [SICOMP 2002] and its timestamp-based variant of
Braverman and Ostrovsky [FOCS 2007].
  Our first set of results presents algorithms (in the expiration streaming
model) for several fundamental problems, including approximate counting,
uniform sampling, and weighted sampling by efficiently tracking active items
without explicitly storing them all. Naturally, these algorithms have many
immediate applications to other problems.
  Our second and main set of results designs algorithms (in the expiration
streaming model) for the diameter and $k$-center problems, where items are
points in a metric space. Our results significantly extend those known for the
special case of sliding-window streams by Cohen-Addad, Schwiegelshohn, and
Sohler [ICALP 2016], including also a strictly better approximation factor for
the diameter in the important special case of high-dimensional Euclidean space.
We develop new decomposition and coordination techniques along with a geometric
dominance framework, to filter out redundant points based on both temporal and
spatial proximity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimension Reduction for Clustering: The Curious Case of Discrete Centers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaofeng H. -C. Jiang, Robert Krauthgamer, Shay Sapir, Sandeep Silwal, Di Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Johnson-Lindenstrauss transform is a fundamental method for dimension
reduction in Euclidean spaces, that can map any dataset of $n$ points into
dimension $O(\log n)$ with low distortion of their distances. This dimension
bound is tight in general, but one can bypass it for specific problems. Indeed,
tremendous progress has been made for clustering problems, especially in the
\emph{continuous} setting where centers can be picked from the ambient space
$\mathbb{R}^d$. Most notably, for $k$-median and $k$-means, the dimension bound
was improved to $O(\log k)$ [Makarychev, Makarychev and Razenshteyn, STOC
2019].
  We explore dimension reduction for clustering in the \emph{discrete} setting,
where centers can only be picked from the dataset, and present two results that
are both parameterized by the doubling dimension of the dataset, denoted as
$\operatorname{ddim}$. The first result shows that dimension
$O_{\epsilon}(\operatorname{ddim} + \log k + \log\log n)$ suffices, and is
moreover tight, to guarantee that the cost is preserved within factor
$1\pm\epsilon$ for every set of centers. Our second result eliminates the
$\log\log n$ term in the dimension through a relaxation of the guarantee
(namely, preserving the cost only for all approximately-optimal sets of
centers), which maintains its usefulness for downstream applications.
  Overall, we achieve strong dimension reduction in the discrete setting, and
find that it differs from the continuous setting not only in the dimension
bound, which depends on the doubling dimension, but also in the guarantees
beyond preserving the optimal value, such as which clusterings are preserved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamic, Self-balancing k-d Tree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Russell A. Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The original description of the k-d tree recognized that rebalancing
techniques, such as used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree, because these techniques involve cyclic exchange (aka
rotation) of tree nodes, which destroys the sorted order of the k-d tree. For
this reason, a static k-d tree is often built from all of the k-dimensional
data en masse. However, it is possible to build a dynamic k-d tree that
self-balances when necessary after insertion or deletion of each individual
k-dimensional datum. This article describes insertion and deletion algorithms
for a dynamic k-d tree, and measures their performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupling via Affine Spectral-Independence: Beck-Fiala and Komlós
  Bounds Beyond Banaszczyk 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.03961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.03961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Bansal, Haotian Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Beck-Fiala Conjecture [Discrete Appl. Math, 1981] asserts that any set
system of $n$ elements with degree $k$ has combinatorial discrepancy
$O(\sqrt{k})$. A substantial generalization is the Koml\'os Conjecture, which
states that any $m \times n$ matrix with unit length columns has discrepancy
$O(1)$.
  In this work, we resolve the Beck-Fiala Conjecture for $k \geq \log^2 n$. We
also give an $\widetilde{O}(\sqrt{k} + \sqrt{\log n})$ bound for $k \leq \log^2
n$, where $\widetilde{O}(\cdot)$ hides $\mathsf{poly}(\log \log n)$ factors.
These bounds improve upon the $O(\sqrt{k \log n})$ bound due to Banaszczyk
[Random Struct. Algor., 1998].
  For the Komlos problem, we give an $\widetilde{O}(\log^{1/4} n)$ bound,
improving upon the previous $O(\sqrt{\log n})$ bound [Random Struct. Algor.,
1998]. All of our results also admit efficient polynomial-time algorithms.
  To obtain these results, we exploit a new technique of ``decoupling via
affine spectral-independence'' in designing rounding algorithms. In particular,
our algorithms obtain the desired colorings via a discrete Brownian motion,
guided by a semidefinite program (SDP). Besides standard constraints used in
prior works, we add some extra affine spectral-independence constraints, which
effectively decouple the evolution of discrepancies across different rows, and
allow us to better control how many rows accumulate large discrepancies at any
point during the process. This new technique is quite general and may be of
independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Effort Distribution via Regret-enabled Greedy Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11107v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11107v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Cao, Taikun Zhu, Kai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses resource allocation problem with a separable objective
function under a single linear constraint, formulated as maximizing
$\sum_{j=1}^{n}R_j(x_j)$ subject to $\sum_{j=1}^{n}x_j=k$ and
$x_j\in\{0,\dots,m\}$. While classical dynamic programming approach solves this
problem in $O(n^2m^2)$ time, we propose a regret-enabled greedy algorithm that
achieves $O(n\log n)$ time when $m=O(1)$. The algorithm significantly
outperforms traditional dynamic programming for small $m$. Our algorithm
actually solves the problem for all $k~(0\leq k\leq nm)$ in the mentioned time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smaller and More Flexible Cuckoo Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Elena Schmitz, Jens Zentgraf, Sven Rahmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cuckoo filters are space-efficient approximate set membership data structures
with a controllable false positive rate (FPR) and zero false negatives, similar
to Bloom filters. In contrast to Bloom filters, Cuckoo filters store multi-bit
fingerprints of keys in a hash table using variants of Cuckoo hashing, allowing
each fingerprint to be stored at a small number of possible locations. Existing
Cuckoo filters use fingerprints of $(k+3)$ bits per key and an additional space
overhead factor of at least $1.05$ to achieve an FPR of $2^{-k}$. For $k=10$,
this amounts to $1.365\, kn$ bits to store $n$ keys, which is better than
$1.443\, kn$ bits for Bloom filters. The $+3$ for the fingerprint size is
required to balance out the multiplied FPR caused by looking for the
fingerprint at several locations. In the original Cuckoo filter, the number of
hash table buckets is restricted to a power of 2, which may lead to much larger
space overheads, up to $2.1\, (1+3/k)\, kn$ bits.
  We present two improvements of Cuckoo filters. First, we remove the
restriction that the number of buckets must be a power of 2 by using a
different placement strategy. Second, we reduce the space overhead factor of
Cuckoo filters to $1.06 \, (1+2/k)$ by using overlapping windows instead of
disjoint buckets to maintain the load threshold of the hash table, while
reducing the number of alternative slots where any fingerprint may be found.
  A detailed evaluation demonstrates that the alternative memory layout based
on overlapping windows decreases the size of Cuckoo filters not only in theory,
but also in practice. A comparison with other state-of-the art filter types,
Prefix filters and Vector Quotient filters (VQFs), shows that the reduced space
overhead makes windowed Cuckoo filters the smallest filters supporting online
insertions, with similarly fast queries, but longer insertion times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computing in a Faulty Congested Clique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keren Censor-Hillel, Pedro Soto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a Faulty Congested Clique model, in which an adversary may fail
nodes in the network throughout the computation. We show that any task of
$O(n\log{n})$-bit input per node can be solved in roughly $n$ rounds, where $n$
is the size of the network. This nearly matches the linear upper bound on the
complexity of the non-faulty Congested Clique model for such problems, by
learning the entire input, and it holds in the faulty model even with a linear
number of faults.
  Our main contribution is that we establish that one can do much better by
looking more closely at the computation. Given a deterministic algorithm
$\mathcal{A}$ for the non-faulty Congested Clique model, we show how to
transform it into an algorithm $\mathcal{A}'$ for the faulty model, with an
overhead that could be as small as some logarithmic-in-$n$ factor, by
considering refined complexity measures of $\mathcal{A}$.
  As an exemplifying application of our approach, we show that the
$O(n^{1/3})$-round complexity of semi-ring matrix multiplication
[Censor-Hillel, Kaski, Korhonen, Lenzen, Paz, Suomela, PODC 2015] remains the
same up to polylog factors in the faulty model, even if the adversary can fail
$99\%$ of the nodes (or any other constant fraction).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Max-Min and 1-Bounded Space Algorithms for the Bin Packing Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.18718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.18718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroshi Fujiwara, Rina Atsumi, Hiroaki Yamamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the (1-dimensional) bin packing problem, we are asked to pack all the
given items into bins, each of capacity one, so that the number of non-empty
bins is minimized. Zhu~[Chaos, Solitons \& Fractals 2016] proposed an
approximation algorithm $MM$ that sorts the item sequence in a non-increasing
order by size at the beginning, and then repeatedly packs, into the current
single open bin, first as many of the largest items in the remaining sequence
as possible and then as many of the smallest items in the remaining sequence as
possible. In this paper we prove that the asymptotic approximation ratio of
$MM$ is at most 1.5. Next, focusing on the fact that $MM$ is at the
intersection of two algorithm classes, max-min algorithms and 1-bounded space
algorithms, we comprehensively analyze the theoretical performance bounds of
each subclass derived from the two classes. Our results include a lower bound
of 1.25 for the intersection of the two classes. Furthermore, we extend the
theoretical analysis over algorithm classes to the cardinality constrained bin
packing problem.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ dciWebMapper2: Enhancing the dciWebMapper framework toward integrated,
  interactive visualization of linked multi-type maps, charts, and spatial
  statistics and analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarigai Sarigai, Liping Yang, Katie Slack, Carolyn Fish, Michaela Buenemann, Qiusheng Wu, Yan Lin, Joseph A. Cook, David Jacobs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As interactive web-based geovisualization becomes increasingly vital across
disciplines, there is a growing need for open-source frameworks that support
dynamic, multi-attribute spatial analysis and accessible design. This paper
introduces dciWebMapper2, a significant expansion of the original dciWebMapper
framework, designed to enable exploratory analysis across domains such as
climate justice, food access, and social vulnerability. The enhanced framework
integrates multiple map types, including choropleth, proportional symbol, small
multiples, and heatmaps, with linked statistical charts (e.g., scatter plots,
boxplots) and time sliders, all within a coordinated-view environment.
Dropdown-based controls allow flexible, high-dimensional comparisons while
maintaining visual clarity. Grounded in cartographic and information
visualization principles, dciWebMapper2 is fully open-source, self-contained,
and server-free, supporting modularity, reproducibility, and long-term
sustainability. Three applied use cases demonstrate its adaptability and
potential to democratize interactive web cartography. This work offers a
versatile foundation for inclusive spatial storytelling and transparent
geospatial analysis in research, education, and civic engagement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 figures, 2 tables, and three advanced interactive web map apps
  that are openly available to the public</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topology-Aware Optimization of Gaussian Primitives for Human-Centric
  Volumetric Videos <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Jiang, Chengcheng Guo, Yize Wu, Yu Hong, Shengkun Zhu, Zhehao Shen, Yingliang Zhang, Shaohui Jiao, Zhuo Su, Lan Xu, Marc Habermann, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video is emerging as a key medium for digitizing the dynamic
physical world, creating the virtual environments with six degrees of freedom
to deliver immersive user experiences. However, robustly modeling general
dynamic scenes, especially those involving topological changes while
maintaining long-term tracking remains a fundamental challenge. In this paper,
we present TaoGS, a novel topology-aware dynamic Gaussian representation that
disentangles motion and appearance to support, both, long-range tracking and
topological adaptation. We represent scene motion with a sparse set of motion
Gaussians, which are continuously updated by a spatio-temporal tracker and
photometric cues that detect structural variations across frames. To capture
fine-grained texture, each motion Gaussian anchors and dynamically activates a
set of local appearance Gaussians, which are non-rigidly warped to the current
frame to provide strong initialization and significantly reduce training time.
This activation mechanism enables efficient modeling of detailed textures and
maintains temporal coherence, allowing high-fidelity rendering even under
challenging scenarios such as changing clothes. To enable seamless integration
into codec-based volumetric formats, we introduce a global Gaussian Lookup
Table that records the lifespan of each Gaussian and organizes attributes into
a lifespan-aware 2D layout. This structure aligns naturally with standard video
codecs and supports up to 40 compression. TaoGS provides a unified, adaptive
solution for scalable volumetric video under topological variation, capturing
moments where "elegance in motion" and "Power in Stillness", delivering
immersive experiences that harmonize with the physical world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGGRAPH Asia 2025. Project page:
  https://guochch.github.io/TaoGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReShape: a Collaborative Art Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Parlier, Bruno Teheux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article describes a project called ReShape in which we created and
designed a crowdsourced art initiative, inspired and powered by mathematics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Cone Radiosity for Interactive Global Illumination with Glossy
  Materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Ren, Haojie Jin, Bo Pang, Yisong Chen, Guoping Wang, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling of high-frequency outgoing radiance distributions has long been a
key challenge in rendering, particularly for glossy material. Such
distributions concentrate radiative energy within a narrow lobe and are highly
sensitive to changes in view direction. However, existing neural radiosity
methods, which primarily rely on positional feature encoding, exhibit notable
limitations in capturing these high-frequency, strongly view-dependent radiance
distributions. To address this, we propose a highly-efficient approach by
reflectance-aware ray cone encoding based on the neural radiosity framework,
named neural cone radiosity. The core idea is to employ a pre-filtered
multi-resolution hash grid to accurately approximate the glossy BSDF lobe,
embedding view-dependent reflectance characteristics directly into the encoding
process through continuous spatial aggregation. Our design not only
significantly improves the network's ability to model high-frequency reflection
distributions but also effectively handles surfaces with a wide range of
glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our
method reduces the network's burden in fitting complex radiance distributions,
allowing the overall architecture to remain compact and efficient.
Comprehensive experimental results demonstrate that our method consistently
produces high-quality, noise-free renderings in real time under various
glossiness conditions, and delivers superior fidelity and realism compared to
baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Morphology-Preserving Remeshing Approach to Particulate Microstructures
  via Harmonic Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.08855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.08855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Shaqfa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harmonic decomposition of surfaces, such as spherical and spheroidal
harmonics, is used to analyze morphology, reconstruct, and generate surface
inclusions of particulate microstructures. However, obtaining high-quality
meshes of engineering microstructures using these approaches remains an open
question. In harmonic approaches, we usually reconstruct surfaces by evaluating
the harmonic bases on equidistantly sampled simplicial complexes of the base
domains (e.g., triangular spheroids and disks). However, this traditional
sampling does not account for local changes in the Jacobian of the basis
functions, resulting in nonuniform discretization after reconstruction or
generation. As it impacts the accuracy and time step, high-quality
discretization of microstructures is crucial for efficient numerical
simulations (e.g., finite element and discrete element methods). To circumvent
this issue, we propose an efficient hierarchical diffusion-based approach for
resampling the surface-i.e., performing a reparameterization-to yield an
equalized mesh triangulation. Analogous to heat problems, we use nonlinear
diffusion to resample the curvilinear coordinates of the analysis domain,
thereby enlarging small triangles at the expense of large triangles on
surfaces. We tested isotropic and anisotropic diffusion schemes on the recent
spheroidal and hemispheroidal harmonics methods. The results show a substantial
improvement in the quality metrics for surface triangulation. Unlike
traditional surface reconstruction and meshing techniques, this approach
preserves surface morphology, along with the areas and volumes of surfaces. We
discuss the results and the associated computational costs for large 2D and 3D
microstructures, such as digital twins of concrete and stone masonry, and their
future applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEAM: Bridging Physically-based Rendering and Gaussian Modeling for
  Relightable Volumetric Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Hong, Yize Wu, Zhehao Shen, Chengcheng Guo, Yuheng Jiang, Yingliang Zhang, Jingyi Yu, Lan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video enables immersive experiences by capturing dynamic 3D
scenes, enabling diverse applications for virtual reality, education, and
telepresence. However, traditional methods struggle with fixed lighting
conditions, while neural approaches face trade-offs in efficiency, quality, or
adaptability for relightable scenarios. To address these limitations, we
present BEAM, a novel pipeline that bridges 4D Gaussian representations with
physically-based rendering (PBR) to produce high-quality, relightable
volumetric videos from multi-view RGB footage. BEAM recovers detailed geometry
and PBR properties via a series of available Gaussian-based techniques. It
first combines Gaussian-based human performance tracking with geometry-aware
rasterization in a coarse-to-fine optimization framework to recover spatially
and temporally consistent geometries. We further enhance Gaussian attributes by
incorporating PBR properties step by step. We generate roughness via a
multi-view-conditioned diffusion model, and then derive AO and base color using
a 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for
efficient visibility computation. Once recovered, these dynamic, relightable
assets integrate seamlessly into traditional CG pipelines, supporting real-time
rendering with deferred shading and offline rendering with ray tracing. By
offering realistic, lifelike visualizations under diverse lighting conditions,
BEAM opens new possibilities for interactive entertainment, storytelling, and
creative visualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IntuiTF: MLLM-Guided Transfer Function Optimization for Direct Volume
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyao Wang, Bo Pan, Ke Wang, Han Liu, Jinyuan Mao, Yuxin Liu, Minfeng Zhu, Xiuqi Huang, Weifeng Chen, Bo Zhang, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct volume rendering (DVR) is a fundamental technique for visualizing
volumetric data, where transfer functions (TFs) play a crucial role in
extracting meaningful structures. However, designing effective TFs remains
unintuitive due to the semantic gap between user intent and TF parameter space.
Although numerous TF optimization methods have been proposed to mitigate this
issue, existing approaches still face two major challenges: the vast
exploration space and limited generalizability. To address these issues, we
propose IntuiTF, a novel framework that leverages Multimodal Large Language
Models (MLLMs) to guide TF optimization in alignment with user intent.
Specifically, our method consists of two key components: (1) an
evolution-driven explorer for effective exploration of the TF space, and (2) an
MLLM-guided human-aligned evaluator that provides generalizable visual feedback
on rendering quality. The explorer and the evaluator together establish an
efficient Trial-Insight-Replanning paradigm for TF space exploration. We
further extend our framework with an interactive TF design system. We
demonstrate the broad applicability of our framework through three case studies
and validate the effectiveness of each component through extensive experiments.
We strongly recommend readers check our cases, demo video, and source code at:
https://github.com/wyysteelhead/IntuiTF
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based
  Implicit Neural Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Pan, Xingguang Zhong, Liren Jin, Louis Wiesmann, Marija Popović, Jens Behley, Cyrill Stachniss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots benefit from high-fidelity reconstructions of their environment, which
should be geometrically accurate and photorealistic to support downstream
tasks. While this can be achieved by building distance fields from range
sensors and radiance fields from cameras, realising scalable incremental
mapping of both fields consistently and at the same time with high quality is
challenging. In this paper, we propose a novel map representation that unifies
a continuous signed distance field and a Gaussian splatting radiance field
within an elastic and compact point-based implicit neural map. By enforcing
geometric consistency between these fields, we achieve mutual improvements by
exploiting both modalities. We present a novel LiDAR-visual SLAM system called
PINGS using the proposed map representation and evaluate it on several
challenging large-scale datasets. Experimental results demonstrate that PINGS
can incrementally build globally consistent distance and radiance fields
encoded with a compact set of neural points. Compared to state-of-the-art
methods, PINGS achieves superior photometric and geometric rendering at novel
views by constraining the radiance field with the distance field. Furthermore,
by utilizing dense photometric cues and multi-view consistency from the
radiance field, PINGS produces more accurate distance fields, leading to
improved odometry estimation and mesh reconstruction. We also provide an
open-source implementation of PING at: https://github.com/PRBonn/PINGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, presented at RSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HodgeFormer: <span class="highlight-title">Transformer</span>s for Learnable Operators on Triangular Meshes
  through Data-Driven Hodge Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akis Nousias, Stavros Nousias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, prominent Transformer architectures applied on graphs and meshes
for shape analysis tasks employ traditional attention layers that heavily
utilize spectral features requiring costly eigenvalue decomposition-based
methods. To encode the mesh structure, these methods derive positional
embeddings, that heavily rely on eigenvalue decomposition based operations,
e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then
concatenated to the input features. This paper proposes a novel approach
inspired by the explicit construction of the Hodge Laplacian operator in
Discrete Exterior Calculus as a product of discrete Hodge operators and
exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust
the Transformer architecture in a novel deep learning layer that utilizes the
multi-head attention mechanism to approximate Hodge matrices $\star_0$,
$\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act
on mesh vertices, edges and faces. Our approach results in a
computationally-efficient architecture that achieves comparable performance in
mesh segmentation and classification tasks, through a direct learning
framework, while eliminating the need for costly eigenvalue decomposition
operations or complex preprocessing operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-08T00:00:00Z">2025-09-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Deterministic Sub-0.5 us Response on Linux through Interrupt
  Isolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyi Zhou, Zhili Liu, Shancong Zhang, Jiemin Li, Dengke Du, Mengke Sun, Zhiqiang Wang, Hongyan Liu, Guokai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time responsiveness in Linux is often constrained by interrupt
contention and timer handling overhead, making it challenging to achieve
sub-microsecond latency. This work introduces an interrupt isolation approach
that centralizes and minimizes timer interrupt interference across CPU cores.
By enabling a dedicated API to selectively invoke timer handling routines and
suppress non-critical inter-processor interrupts, our design significantly
reduces jitter and response latency. Experiments conducted on an ARM-based
multicore platform demonstrate that the proposed mechanism consistently
achieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT
configurations. These results highlight the potential of interrupt isolation as
a lightweight and effective strategy for deterministic real-time workloads in
general-purpose operating systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control <span class="chip" style="font-size: 60%">39</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for
  Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating collision-free motion in dynamic, partially observable
environments is a fundamental challenge for robotic manipulators. Classical
motion planners can compute globally optimal trajectories but require full
environment knowledge and are typically too slow for dynamic scenes. Neural
motion policies offer a promising alternative by operating in closed-loop
directly on raw sensory inputs but often struggle to generalize in complex or
dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input. At its core is
IMPACT, a transformer-based neural motion policy pretrained on 10 million
generated expert trajectories across diverse simulation scenarios. We further
improve IMPACT's static obstacle avoidance through iterative student-teacher
finetuning. We additionally enhance the policy's dynamic obstacle avoidance at
inference time using DCP-RMP, a locally reactive goal-proposal module. We
evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving
obstacles, and goal obstructions. DRP achieves strong generalization,
outperforming prior classical and neural methods in success rate across both
simulated and real-world settings. Video results and code available at
https://deep-reactive-policy.com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at \url{deep-reactive-policy.com}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement learning meets bioprocess control through behaviour
  cloning: Real-world deployment in an industrial photobioreactor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan D. Gil, Ehecatl Antonio Del Rio Chanona, José L. Guzmán, Manuel Berenguel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent complexity of living cells as production units creates major
challenges for maintaining stable and optimal bioprocess conditions, especially
in open Photobioreactors (PBRs) exposed to fluctuating environments. To address
this, we propose a Reinforcement Learning (RL) control approach, combined with
Behavior Cloning (BC), for pH regulation in open PBR systems. This represents,
to the best of our knowledge, the first application of an RL-based control
strategy to such a nonlinear and disturbance-prone bioprocess. Our method
begins with an offline training stage in which the RL agent learns from
trajectories generated by a nominal Proportional-Integral-Derivative (PID)
controller, without direct interaction with the real system. This is followed
by a daily online fine-tuning phase, enabling adaptation to evolving process
dynamics and stronger rejection of fast, transient disturbances. This hybrid
offline-online strategy allows deployment of an adaptive control policy capable
of handling the inherent nonlinearities and external perturbations in open
PBRs. Simulation studies highlight the advantages of our method: the Integral
of Absolute Error (IAE) was reduced by 8% compared to PID control and by 5%
relative to standard off-policy RL. Moreover, control effort decreased
substantially-by 54% compared to PID and 7% compared to standard RL-an
important factor for minimizing operational costs. Finally, an 8-day
experimental validation under varying environmental conditions confirmed the
robustness and reliability of the proposed approach. Overall, this work
demonstrates the potential of RL-based methods for bioprocess control and paves
the way for their broader application to other nonlinear, disturbance-prone
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic DDQN-Based Scheduling for Licensed and Unlicensed Band
  Allocation in Sidelink Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Heng Chou, Pin-Qi Fu, Walid Saad, Li-Chun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an agentic artificial intelligence (AI)-driven double
deep Q-network (DDQN) scheduling framework for licensed and unlicensed band
allocation in New Radio (NR) sidelink (SL) networks. SL must share licensed
spectrum with cellular communications (CC) and unlicensed bands with Wi-Fi,
posing significant challenges for coexistence. Unlike prior rule-based or
threshold-based methods, the proposed agentic scheduler autonomously perceives
queueing dynamics, channel conditions, and coexistence states, and adapts its
policy to maintain quality-of-service (QoS). Simulation results show that our
framework reduces the blocking rate by up to 87.5% compared to threshold-based
scheduling under limited licensed bandwidth. These findings demonstrate the
potential of Agentic AI to enable stable, QoS-aware, and adaptive scheduling
for future NR SL systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted by 2025 IEEE Globecom Workshops</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering Opinion through Dynamic Stackelberg Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Rastgoftar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper employs the Friedkin-Johnsen (FJ) model to describe the dynamics
of opinion evolution within a social network. Under the FJ framework, the
society is divided into two subgroups that include stubborn agents and regular
agents. The opinions of stubborn agents are not influenced by regular agents,
whereas the opinions of regular agents evolve based on the opinions of their
neighboring agents. By defining the origin as the desired collective opinion of
the society, the objective of the paper is to minimize deviations from this
desired opinion. To achieve this, a Stackelberg game is established between the
stubborn and regular subgroups, where the opinion adjustments of the stubborn
agents and the openness variables of regular agents serve as the decision
variables. The proposed solution approach integrates quadratic programming and
dynamic programming to optimize these decision variables at each discrete time
step using forward and backward propagation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge Server Monitoring for Job Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Chamoun, Sirin Chakraborty, Eric Graves, Kevin Chan, Yin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study a goal-oriented communication problem for edge server
monitoring, where compute jobs arrive intermittently at dispatchers and must be
immediately assigned to distributed edge servers. Due to competing workloads
and the dynamic nature of the edge environment, server availability fluctuates
over time. To maintain accurate estimates of server availability states, each
dispatcher updates its belief using two mechanisms: (i) active queries over
shared communication channels and (ii) feedback from past job executions. We
formulate a query scheduling problem that maximizes the job success rate under
limited communication resources for queries. This problem is modeled as a
Restless Multi-Armed Bandit (RMAB) with multiple actions and addressed using a
Net-Gain Maximization (NGM) scheduling algorithm, which selects servers to
query based on their expected improvement in execution performance. Simulation
results show that the proposed NGM Policy significantly outperforms baseline
strategies, achieving up to a 30% gain over the Round-Robin Policy and up to a
107% gain over the Never-Query Policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE MILCOM 2025 (Networking Protocols and Performance
  Track), 6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Robust Predictive Control-based Motion Planning of Automated
  Surface Vessels in Inland Waterways 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Ahmadi, Hossein Nejatbakhsh Esfahani, Javad Mohammadpour Velni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying self-navigating surface vessels in inland waterways offers a
sustainable alternative to reduce road traffic congestion and emissions.
However, navigating confined waterways presents unique challenges, including
narrow channels, higher traffic density, and hydrodynamic disturbances.
Existing methods for autonomous vessel navigation often lack the robustness or
precision required for such environments. This paper presents a new motion
planning approach for Automated Surface Vessels (ASVs) using Robust Model
Predictive Control (RMPC) combined with Control Barrier Functions (CBFs). By
incorporating channel borders and obstacles as safety constraints within the
control design framework, the proposed method ensures both collision avoidance
and robust navigation on complex waterways. Simulation results demonstrate the
efficacy of the proposed method in safely guiding ASVs under realistic
conditions, highlighting its improved safety and adaptability compared to the
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Adaptive Coverage Control Approach for Multiple Autonomous Off-road
  Vehicles in Dynamic Agricultural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Ahmadi, Mohammadreza Davoodi, Javad Mohammadpour Velni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an adaptive coverage control method for a fleet of
off-road and Unmanned Ground Vehicles (UGVs) operating in dynamic
(time-varying) agricultural environments. Traditional coverage control
approaches often assume static conditions, making them unsuitable for
real-world farming scenarios where obstacles, such as moving machinery and
uneven terrains, create continuous challenges. To address this, we propose a
real-time path planning framework that integrates Unmanned Aerial Vehicles
(UAVs) for obstacle detection and terrain assessment, allowing UGVs to
dynamically adjust their coverage paths. The environment is modeled as a
weighted directed graph, where the edge weights are continuously updated based
on the UAV observations to reflect obstacle motion and terrain variations. The
proposed approach incorporates Voronoi-based partitioning, adaptive edge weight
assignment, and cost-based path optimization to enhance navigation efficiency.
Simulation results demonstrate the effectiveness of the proposed method in
improving path planning, reducing traversal costs, and maintaining robust
coverage in the presence of dynamic obstacles and muddy terrains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Hardware-in-the-Loop simulations for systemic resilience
  assessment in cyber-socio-technical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Simone, Marco Bortolini, Giovanni Mazzuto, Giulio di Gravio, Riccardo Patriarca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern industrial systems require updated approaches to safety management, as
the tight interplay between cyber-physical, human, and organizational factors
has driven their processes toward increasing complexity. In addition to dealing
with known risks, managing system resilience acquires great value to address
complex behaviors pragmatically. This manuscript starts from the
System-Theoretic Accident Model and Processes (STAMP) as a modelling initiative
for such complexity. The STAMP can be natively integrated with simulation-based
approaches, which however fail to realistically represent human behaviors and
their influence on the system performance. To overcome this limitation, this
paper proposes a Human-Hardware-in-the-Loop (HHIL) modeling and simulation
framework aimed at supporting a more realistic and comprehensive assessments of
systemic resilience. The approach is tested on an experimental oil and gas
plant experiencing cyber-attacks, where two personas of operators (experts and
novices) work. This research provides a mean to quantitatively assess how
variations in operator behavior impact the overall system performance, offering
insights into how resilience should be understood and implemented in complex
socio-technical systems at large.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Bounds and Task-Centric Learning Complexity for
  Real-World Dynamic Nonlinear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanpää
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic nonlinear systems exhibit distortions arising from coupled static and
dynamic effects. Their intertwined nature poses major challenges for
data-driven modeling. This paper presents a theoretical framework grounded in
structured decomposition, variance analysis, and task-centric complexity
bounds.
  The framework employs a directional lower bound on interactions between
measurable system components, extending orthogonality in inner product spaces
to structurally asymmetric settings. This bound supports variance inequalities
for decomposed systems. Key behavioral indicators are introduced along with a
memory finiteness index. A rigorous power-based condition establishes a
measurable link between finite memory in realizable systems and the First Law
of Thermodynamics. This offers a more foundational perspective than classical
bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty
Principle,' demonstrating that static and dynamic distortions cannot be
minimized simultaneously. We identify that real-world systems seem to resist
complete deterministic decomposition due to entangled static and dynamic
effects. We also present two general-purpose theorems linking function variance
to mean-squared Lipschitz continuity and learning complexity. This yields a
model-agnostic, task-aware complexity metric, showing that lower-variance
components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual
learning, including improved generalization, reduced parameter count, and lower
training cost, as previously observed in power amplifier linearization
experiments. The framework is broadly applicable and offers a scalable,
theoretically grounded approach to modeling complex dynamic nonlinear systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure, 2 photographs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Automatic Generation Control subject to Ramp-Rate-Limits:
  Anytime Feasibility and Uniform Network-Connectivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Doostmohammadian, Hamid R. Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers automatic generation control over an information-sharing
network of communicating generators as a multi-agent system. The optimization
solution is distributed among the agents based on information consensus
algorithms, while addressing the generators' ramp-rate-limits (RRL). This is
typically ignored in the existing linear/nonlinear optimization solutions but
they exist in real-time power generation scenarios. Without addressing the RRL,
the generators cannot follow the assigned rate of generating power by the
optimization algorithm; therefore, the existing solutions may not necessarily
converge to the exact optimal cost or may lose feasibility in practice. The
proposed solution in this work addresses the ramp-rate-limit constraint along
with the box constraint (limits on the generated powers) and the
coupling-constraint (generation-demand balance) at all iteration times of the
algorithm. The latter is referred to as the anytime feasibility and implies
that at every termination point of the algorithm, the balance between the
demand and generated power holds. To improve the convergence rate of the
algorithm we further consider internal signum-based nonlinearity. We also show
that our solution can tolerate communication link removal. This follows from
the uniform-connectivity assumption on the communication network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Digital Signal Processing journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wireless Low-Latency Synchronization for Body-Worn Multi-Node Systems in
  Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nico Krull, Lukas Schulthess, Michele Magno, Luca Benini, Christoph Leitner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomechanical data acquisition in sports demands sub-millisecond
synchronization across distributed body-worn sensor nodes. This study evaluates
and characterizes the Enhanced ShockBurst (ESB) protocol from Nordic
Semiconductor under controlled laboratory conditions for wireless, low-latency
command broadcasting, enabling fast event updates in multi-node systems.
Through systematic profiling of protocol parameters, including
cyclic-redundancy-check modes, bitrate, transmission modes, and payload
handling, we achieve a mean Device-to-Device (D2D) latency of 504.99 +- 96.89
us and a network-to-network core latency of 311.78 +- 96.90 us using a one-byte
payload with retransmission optimization. This performance significantly
outperforms Bluetooth Low Energy (BLE), which is constrained by a 7.5 ms
connection interval, by providing deterministic, sub-millisecond
synchronization suitable for high-frequency (500 Hz to 1000 Hz) biosignals.
These results position ESB as a viable solution for time-critical, multi-node
wearable systems in sports, enabling precise event alignment and reliable
high-speed data fusion for advanced athlete monitoring and feedback
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter Robustness in Data-Driven Estimation of Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the robustness of system estimation to parametric perturbations in
system dynamics and initial conditions. We define the problem of
sensitivity-based parametric uncertainty quantification in dynamical system
estimation. The main contribution of this paper is the development of a novel
robustness metric for estimation of parametrized linear dynamical systems with
and without control actions. For the computation of this metric, we delineate
the uncertainty contributions arising from control actions, system dynamics,
and initial conditions. Furthermore, to validate our theoretical findings, we
establish connections between these new results and the existing literature on
the robustness of model reduction. This work provides guidance for selecting
estimation methods based on tolerable levels of parametric uncertainty and
paves the way for new cost functions in data-driven estimation that reward
sensitivity to a desired subset of parameters while penalizing others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication in the IEEE Conference on Decision and
  Control (CDC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Graph-Theoretic Modeling of Multi-Energy Flows in Distribution
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwan Mostafa, Daniel Wenser, Payam Teimourzadeh Baboli, Christian Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of energy systems due to sector coupling and
decarbonization calls for unified modeling frameworks that capture the physical
and structural interactions between electricity, gas, and heat networks. This
paper presents a graph-based modeling approach for multi-energy systems, where
each domain is represented as a layer in a multi-layer graph, and coupling
technologies are modeled as inter-layer edges via a dedicated coupling layer. A
steady-state solver based on a block-structured Newton-Raphson method is
developed to jointly compute flows and state variables across all carriers. The
proposed model is tested and validated on a realistic case study based on data
from a German distribution network. The results demonstrate convergence,
numerical accuracy, and consistent domain interaction, and demonstrate the
method's applicability for system-wide analysis and its potential as a
foundation for future optimizations in integrated energy systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First-Principle Modeling Framework of Boost Converter Dynamics for
  Precise Energy Conversions in Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Wenhua Li, Zhenlong Wang, Xinrui Zhang, Jianfeng Sun, Qianfu Xia, Zhongtao Gou, Jiangang Rong, Tao Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Boost converters are essential for modern electrification and intelligent
technologies. However, conventional Boost converter models relying on
steady-state assumptions fail to accurately predict transient behaviors during
input voltage and load fluctuations, which cause significant output voltage
overshoots and instability, resulting in failures of electrical systems,
thereby restricting their use in space. This study introduces a first-principle
modeling framework that derives precise dynamic equations for Boost converters
by incorporating non-ideal component coupling. As compared to the most accurate
existing Boost converter model, the proposed models reduce steady-state and
dynamic-state errors between experimental and simulated output voltages by
factors of 11.0 (from 20.9% to 1.9%) and 15.4 (from 77.1% to 5.0%) under input
voltage variations, and by factors of 10.2 (from 15.3% to 1.5%) and 35.1 (from
42.1% to 1.2%) under load changes, respectively. Consequently, a reliable Boost
converter is accordingly designed and on-orbit deployed for precise energy
conversions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 30 pages supplementary material, 5 figures, 14
  supplementary figures, 6 supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyu Lei, Tianhao Liang, Yuqi Ping, Xinglin Chen, Longyu Zhou, Junwei Wu, Xiyuan Zhang, Huahao Ding, Xingjian Zhang, Weijie Yuan, Tingting Zhang, Qinyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of the low-altitude economy emphasizes the critical
need for effective perception and intent recognition of non-cooperative
unmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities
of multimodal large language models (MLLMs) present a promising approach in
such tasks. In this paper, we focus on the combination of UAV intent
recognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV
intent recognition architecture, where the multimodal perception system is
utilized to obtain real-time payload and motion information of UAVs, generating
structured input information, and MLLM outputs intent recognition results by
incorporating environmental information, prior knowledge, and tactical
preferences. Subsequently, we review the related work and demonstrate their
progress within the proposed architecture. Then, a use case for low-altitude
confrontation is conducted to demonstrate the feasibility of our architecture
and offer valuable insights for practical system design. Finally, the future
challenges are discussed, followed by corresponding strategic recommendations
for further applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to IEEE Internet of Things Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNN-based Digital Twin Framework of a DC-DC Buck Converter using Spider
  Monkey Optimization Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahmin Mahmud, Euzeli Cipriano Dos Santos Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Component ageing is a critical concern in power electronic converter systems
(PECSs). It directly impacts the reliability, performance, and operational
lifespan of converters used across diverse applications, including electric
vehicles (EVs), renewable energy systems (RESs) and industrial automation.
Therefore, understanding and monitoring component ageing is crucial for
developing robust converters and achieving long-term system reliability. This
paper proposes a data-driven digital twin (DT) framework for DC-DC buck
converters, integrating deep neural network (DNN) with the spider monkey
optimization (SMO) algorithm to monitor and predict component degradation.
Utilizing a low-power prototype testbed along with empirical and synthetic
datasets, the SMO+DNN approach achieves the global optimum in 95% of trials,
requires 33% fewer iterations, and results in 80% fewer parameter constraint
violations compared to traditional methods. The DNN model achieves $R^2$ scores
above 0.998 for all key degradation parameters and accurately forecasts time to
failure ($t_{failure}$). In addition, SMO-tuned degradation profile improves
the converter's performance by reducing voltage ripple by 20-25% and inductor
current ripple by 15-20%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 13 figures, 2 tables. Accepted for a lecture presentation at
  the 2025 IEEE Energy Conversion Conference and Expo (ECCE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Body Weight Estimation Through Music-Induced Bed Vibrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyan Wu, Jiale Zhang, Moon Lee, Cherrelle Smith, Xinyi Li, Ankur Senapati, Pei Zhang, Hae Young Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid and accurate body weight estimation is critical in emergency medical
care, as it directly influences treatment decisions, such as drug dosing,
defibrillation energy selection, and fluid resuscitation. Traditional methods
such as stand-on scales, length-based tapes, or transfer-based weighing scales
are often impractical for immobilized patients, inaccurate, or labor-intensive
and time-consuming. This paper introduces MelodyBedScale, a non-intrusive and
rapid on-bed weight estimation system that leverages bed vibration induced by
music. The core insight is that body weight affects the vibration transfer
function of the bed-body system, which is captured using vibration sensors
placed on opposite sides of the bed. First, we identify weight-sensitive
frequency bands and compose clinically acceptable soft, natural music with high
signal energy in these frequency bands. This music is then played through a
speaker mounted on the bed to induce bed vibrations. Additionally, to
efficiently capture the complex weight-vibration relationship with limited data
and enhance generalizability to unseen individuals and weights, we
theoretically analyze the weight-vibration relationship and integrate the
results into the activation functions of the neural network for
physics-informed weight regression. We evaluated MelodyBedScale on both wooden
and steel beds across 11 participants, achieving a mean absolute error of up to
1.55 kg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Mobicom 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Neural Koopman Operators with Dissipativity Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuezhu Xu, S. Sivaranjani, Vijay Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of learning a neural Koopman operator model that
provides dissipativity guarantees for an unknown nonlinear dynamical system
that is known to be dissipative. We propose a two-stage approach. First, we
learn an unconstrained neural Koopman model that closely approximates the
system dynamics. Then, we minimally perturb the parameters to enforce strict
dissipativity. Crucially, we establish theoretical guarantees that extend the
dissipativity properties of the learned model back to the original nonlinear
system. We realize this by deriving an exact relationship between the
dissipativity of the learned model and the true system through careful
characterization of the identification errors from the noisy data, Koopman
operator truncation, and generalization to unseen data. We demonstrate our
approach through simulation on a Duffing oscillator model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electricity Demand and Grid Impacts of AI Data Centers: Challenges and
  Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chen, Xiaoyang Wang, Ana Colacelli, Matt Lee, Le Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of artificial intelligence (AI) is driving an unprecedented
increase in the electricity demand of AI data centers, raising emerging
challenges for electric power grids. Understanding the characteristics of AI
data center loads and their interactions with the grid is therefore critical
for ensuring both reliable power system operation and sustainable AI
development. This paper provides a comprehensive review and vision of this
evolving landscape. Specifically, this paper (i) presents an overview of AI
data center infrastructure and its key components, (ii) examines the key
characteristics and patterns of electricity demand across the stages of model
preparation, training, fine-tuning, and inference, (iii) analyzes the critical
challenges that AI data center loads pose to power systems across three
interrelated timescales, including long-term planning and interconnection,
short-term operation and electricity markets, and real-time dynamics and
stability, and (iv) discusses potential solutions from the perspectives of the
grid, AI data centers, and AI end-users to address these challenges. By
synthesizing current knowledge and outlining future directions, this review
aims to guide research and development in support of the joint advancement of
AI data centers and power systems toward reliable, efficient, and sustainable
operation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Version: Market-Driven Equilibria for Distributed Solar Panel
  Investment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Davoudi, Junjie Qin, Xiaojun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates market-driven long-term investment decisions in
distributed solar panels by individual investors. We consider a setting where
investment decisions are driven by expected revenue from participating in
short-term electricity markets over the panel's lifespan. These revenues depend
on short-term markets equilibria, i.e., prices and allocations, which are
influenced by aggregate invested panel capacity participating in the markets.
We model the interactions among investors by a non-atomic game and develop a
framework that links short-term markets equilibria to the resulting long-term
investment equilibrium. Then, within this framework, we analyze three market
mechanisms: (a) a single-product real-time energy market, (b) a
product-differentiated real-time energy market that treats solar energy and
grid energy as different products, and (c) a contract-based panel market that
trades claims or rights to the production of certain panel capacity ex-ante,
rather than the realized solar production ex-post. For each, we derive
expressions for short-term equilibria and the associated expected revenues, and
analytically characterize the corresponding long-term Nash equilibrium
aggregate capacity. We compare the solutions of these characterizing equations
under different conditions and theoretically establish that the
product-differentiated market always supports socially optimal investment,
while the single-product market consistently results in under-investment. We
also establish that the contract-based market leads to over-investment when the
extra valuations of users for solar energy are small. Finally, we validate our
theoretical findings through numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Longer version of a paper submitted to IEEE Transactions on Smart
  Grid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of Input-Output Observers for a Population of Systems with
  Bounded Frequency-Domain Variation using $DK$-iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Everett Adams, James Richard Forbes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a linear input-output observer design methodology for a
population of systems in which each observer uses knowledge of the linear
time-invariant dynamics of the particular device. Observers are typically
composed of a known model of the system and a correction mechanism to produce
an estimate of the state. The proposed design procedure characterizes the
variation within the population in the frequency domain and synthesizes a
single robust correction filter. The correction filter is compatible with all
system models that satisfy the variation characterization such that a given
level of estimation performance is guaranteed. This is accomplished by posing a
robust performance problem using the observer error dynamics and solving it
using $DK$-iteration. The design procedure is experimentally demonstrated on a
flexible joint robotic manipulator with varied joint stiffnesses. It is shown
that the proposed method that uses a single correction filter achieves
comparable estimation performance to a method that uses a correction gain
tailored toward each joint stiffness configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Effect of Sampling-Time Jitter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dieter Schwarzmann, Simon Käser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This brief, aimed at practitioners, offers an analysis of the effect of
sampling-time jitter, i. e., the error produced by execution-time inaccuracies.
We propose reinterpreting jitter-afflicted linear time-invariant systems
through equivalent jitter-free analogs. By constructing a perceived system that
absorbs the effects of timing perturbations into its dynamics, we find an
affine scaling of jitter. We examine both measurement and implementation
scenarios, demonstrating that the presence of jitter effectively scales the
system matrices. Moreover, we observe that, in the Laplace domain, jitter can
be interpreted as a frequency scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated Version of the one submitted. Submitted for review as letter
  in IEEE Journal for Transactions on Control Systems Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vanishing Stacked-Residual PINN for State Reconstruction of Hyperbolic
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14222v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14222v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katayoun Eshkofti, Matthieu Barreau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a more connected world, modeling multi-agent systems with hyperbolic
partial differential equations (PDEs) offers a compact, physics-consistent
description of collective dynamics. However, classical control tools need
adaptation for these complex systems. Physics-informed neural networks (PINNs)
provide a powerful framework to fix this issue by inferring solutions to PDEs
by embedding governing equations into the neural network. A major limitation of
original PINNs is their inability to capture steep gradients and
discontinuities in hyperbolic PDEs. To tackle this problem, we propose a
stacked residual PINN method enhanced with a vanishing viscosity mechanism.
Initially, a basic PINN with a small viscosity coefficient provides a stable,
low-fidelity solution. Residual correction blocks with learnable scaling
parameters then iteratively refine this solution, progressively decreasing the
viscosity coefficient to transition from parabolic to hyperbolic PDEs. Applying
this method to traffic state reconstruction improved results by an order of
magnitude in relative $\mathcal{L}^2$ error, demonstrating its potential to
accurately estimate solutions where original PINNs struggle with instability
and low fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearly Controlled Language Generation with Performative Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Cheng, Carmen Amo Alonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of Large Language Models (LMs) in critical
applications highlights the need for controlled language generation strategies
that are not only computationally efficient but that also enjoy performance
guarantees. To achieve this, we use a common model of concept semantics as
linearly represented in an LM's latent space. In particular, we take the view
that natural language generation traces a trajectory in this continuous
semantic space, realized by the language model's hidden activations. This view
permits a control-theoretic treatment of text generation in latent space, in
which we propose a lightweight, gradient-free intervention that dynamically
steers trajectories away from regions corresponding to undesired meanings. In
particular, we propose to directly intervene the activations of the token that
is being generated in embedding space in an online fashion. Crucially, we do
not simply steer activations towards a desirable region. Instead, our method
relies on classical techniques from control theory to precisely control
activations in a context-dependent way, and guarantees that they are brought
into a specific pre-defined region of embedding space that corresponds to
allowed semantics. Our intervention is computed in closed-form according to an
optimal controller formulation, minimally impacting generation time. This
control of the activations in embedding space allows for fine-grained steering
of attributes of the generated sequence. We demonstrate the effectiveness of
our approach on different objectives-- toxicity avoidance and sentiment
control-- while maintaining text quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Damping for the 1D Wave Equation Using a Single Damper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petar Mlinarić, Serkan Gugercin, Zoran Tomljanović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vibrational structures are susceptible to catastrophic failures or structural
damages when external forces induce resonances or repeated unwanted
oscillations. One common mitigation strategy is to use dampers to suppress
these disturbances. This leads to the problem of finding optimal damper
viscosities and positions for a given vibrational structure. Although extensive
research exists for the case of finite-dimensional systems, optimizing damper
positions remains challenging due to its discrete nature. To overcome this, we
introduce a novel model for the damped wave equation (at the PDE level) with a
damper of viscosity $\mathfrak{g}$ at position $\mathfrak{p}$ and develop a
system-theoretic input/output-based analysis in the frequency domain. In this
system-theoretic formulation, while we consider average displacement as the
output, for input (forcing), we analyze two separate cases, namely, the uniform
and boundary forcing. For both cases, explicit formulas are derived for the
corresponding transfer functions, parametrized by $\mathfrak{p}$ and
$\mathfrak{g}$. This explicit parametrization by $\mathfrak{p}$ and
$\mathfrak{g}$ facilitates analyzing the optimal damping problem (at the PDE
level) using norms such as the $\mathcal{H}_2$ and $\mathcal{H}_\infty$ norms.
We also examine limiting cases, such as when the viscosity is very large or
when no external damping is present. To illustrate our approach, we present
numerical examples, compare different optimization criteria, and discuss the
impact of damping parameters on the damped wave equation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Ji, Xiping Wu, Zhihong Zeng, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a
promising paradigm of heterogeneous network (HetNet), attributed to the
complementary physical properties of optical spectra and radio frequency.
However, the current development of such HetNets is mostly bottlenecked by the
existing transmission control protocol (TCP), which restricts the user
equipment (UE) to connecting one access point (AP) at a time. While the ongoing
investigation on multipath TCP (MPTCP) can bring significant benefits, it
complicates the network topology of HetNets, making the existing load balancing
(LB) learning models less effective. Driven by this, we propose a graph neural
network (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets,
which results in a partial mesh topology. Such a topology can be modeled as a
graph, with the channel state information and data rate requirement embedded as
node features, while the LB solutions are deemed as edge labels. Compared to
the conventional deep neural network (DNN), the proposed GNN-based model
exhibits two key strengths: i) it can better interpret a complex network
topology; and ii) it can handle various numbers of APs and UEs with a single
trained model. Simulation results show that against the traditional
optimisation method, the proposed learning model can achieve near-optimal
throughput within a gap of 11.5%, while reducing the inference time by 4 orders
of magnitude. In contrast to the DNN model, the new method can improve the
network throughput by up to 21.7%, at a similar inference time level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We would like to withdraw this submission because it contains several
  errors that need substantial revision. We plan to prepare a corrected and
  improved version, which will be submitted as a new manuscript at a later
  stage</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification and Optimal Nonlinear Control of Turbojet Engine Using
  Koopman Eigenfunction Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Grasev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gas turbine engines are complex and highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging because it requires
performance characteristics that are not always available, often leading to
many simplifying assumptions. This paper discusses the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models, and addresses these issues by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics are estimated using the
sparse identification of nonlinear dynamics. Subsequently, the autonomous part
of the dynamics is mapped into an optimally constructed Koopman eigenfunction
space. This process involves eigenvalue optimization using metaheuristic
algorithms and temporal projection, followed by gradient-based eigenfunction
identification. The resulting Koopman model is validated against an in-house
reference component-level model. A globally optimal nonlinear feedback
controller and a Kalman estimator are then designed within the eigenfunction
space and compared to traditional and gain-scheduled proportional-integral
controllers, as well as a proposed internal model control approach. The
eigenmode structure enables targeting individual modes during optimization,
leading to improved performance tuning. Results demonstrate that the
Koopman-based controller surpasses other benchmark controllers in both
reference tracking and disturbance rejection under sea-level and varying flight
conditions, due to its global nature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 28 figures Under review at Springer Nonlinear Dynamics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifiability and Maximum Likelihood Estimation for System
  Identification of Networks of Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20628v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20628v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anders Hansson, João Victor Galvão da Mata, Martin S. Andersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we investigate identifiability and maximum likelihood
estimation for direct system identification of networks of dynamical systems.
We provide necessary and sufficient conditions for network identifiability in
terms of Gr\"obner bases. We show that the maximum likelihood approach is both
consistent and efficient, which is in contrast to existing prediction error
approaches. Moreover, our approach has wider applicability, i.e., it is
applicable whenever network identifiability holds. Finally, we show that we can
formulate the maximum likelihood problem without the use of a predictor, which
is the key to numerically being able to solve it efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Submitted to IEEE Transactions on Automatic Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bipedal Balance Control with Whole-body Musculoskeletal Standing and
  Falling Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengtian Ma, Yunyue Wei, Chenhui Zuo, Chen Zhang, Yanan Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial exponential decay of perturbations in optimal control of general
  evolution equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Göttlich, Benedikt Oppeneiger, Manuel Schaller, Karl Worthmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the robustness of optimally controlled evolution equations with
respect to spatially localized perturbations. We prove that if the involved
operators are domain-uniformly stabilizable and detectable, then these
localized perturbations only have a local effect on the optimal solution. We
characterize this domain-uniform stabilizability and detectability for the
transport equation with constant transport velocity, showing that even for
unitary semigroups, optimality implies exponential damping. We extend this
result to the case of a space-dependent transport velocity. Finally we leverage
the results for the transport equation to characterize domain-uniform
stabilizability of the wave equation. Numerical examples in one space dimension
complement the theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Distributionally Robust Control Based on Sinkhorn Ambiguity
  Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Cescon, Andrea Martin, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the complexity of modern control systems increases, it becomes challenging
to derive an accurate model of the uncertainty that affects their dynamics.
Wasserstein Distributionally Robust Optimization (DRO) provides a powerful
framework for decision-making under distributional uncertainty only using noise
samples. However, while the resulting policies inherit strong probabilistic
guarantees when the number of samples is sufficiently high, their performance
may significantly degrade when only a few data are available. Inspired by
recent results from the machine learning community, we introduce an entropic
regularization to penalize deviations from a given reference distribution and
study data-driven DR control over Sinkhorn ambiguity sets. We show that for
finite-horizon control problems, the optimal DR linear policy can be computed
via convex programming. By analyzing the relation between the ambiguity set
defined in terms of Wasserstein and Sinkhorn discrepancies, we reveal that, as
the regularization parameter increases, this optimal policy interpolates
between the solution of the Wasserstein DR problem and that of the stochastic
problem under the reference distribution. We validate our theoretical findings
and the effectiveness of our approach when only scarce data are available on a
numerical example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Multistage Optimization Structure in Proximal Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roland Schwan, Daniel Kuhn, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an efficient structure-exploiting algorithm for
multistage optimization problems. The proposed method extends existing
approaches by supporting full coupling between stages and global decision
variables in the cost, as well as equality and inequality constraints. The
algorithm is implemented as a new backend in the PIQP solver and leverages a
specialized block-tri-diagonal-arrow Cholesky factorization within a proximal
interior-point framework to handle the underlying problem structure
efficiently. The implementation features automatic structure detection and
seamless integration with existing interfaces. Numerical experiments
demonstrate significant performance improvements, achieving up to 13x speed-up
compared to a generic sparse backend and matching/exceeding the performance of
the state-of-the-art specialized solver HPIPM. The solver is particularly
effective for applications such as model predictive control, robust scenario
optimization, and periodic optimization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian behaviors: representations and data-driven control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.15838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.15838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        András Sasfi, Ivan Markovsky, Alberto Padoan, Florian Dörfler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a modeling framework for stochastic systems, termed Gaussian
behaviors, that describes finite-length trajectories of a system as a Gaussian
process. The proposed model naturally quantifies the uncertainty in the
trajectories, yet it is simple enough to allow for tractable formulations. We
relate the proposed model to existing descriptions of dynamical systems
including deterministic and stochastic behaviors, and linear time-invariant
(LTI) state-space models with Gaussian noise. Gaussian behaviors can be
estimated directly from observed data as the empirical sample covariance. The
distribution of future outputs conditioned on inputs and past outputs provides
a predictive model that can be incorporated in predictive control frameworks.
We show that subspace predictive control is a certainty-equivalence control
formulation with the estimated Gaussian behavior. Furthermore, the regularized
data-enabled predictive control (DeePC) method is shown to be a
distributionally optimistic formulation that optimistically accounts for
uncertainty in the Gaussian behavior. To mitigate the excessive optimism of
DeePC, we propose a novel distributionally robust control formulation, and
provide a convex reformulation allowing for efficient implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper accepted to the 64th IEEE Conference on
  Decision and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AARK: An Open Toolkit for Autonomous Racing Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Bockman, Matthew Howe, Adrian Orenstein, Feras Dayoub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous racing demands safe control of vehicles at their physical limits
for extended periods of time, providing insights into advanced vehicle safety
systems which increasingly rely on intervention provided by vehicle autonomy.
Participation in this field carries with it a high barrier to entry. Physical
platforms and their associated sensor suites require large capital outlays
before any demonstrable progress can be made. Simulators allow researches to
develop soft autonomous systems without purchasing a platform. However,
currently available simulators lack visual and dynamic fidelity, can still be
expensive to buy, lack customisation, and are difficult to use. AARK provides
three packages, ACI, ACDG, and ACMPC. These packages enable research into
autonomous control systems in the demanding environment of racing to bring more
people into the field and improve reproducibility: ACI provides researchers
with a computer vision-friendly interface to Assetto Corsa for convenient
comparison and evaluation of autonomous control solutions; ACDG enables
generation of depth, normal and semantic segmentation data for training
computer vision models to use in perception systems; and ACMPC gives newcomers
to the field a modular full-stack autonomous control solution, capable of
controlling vehicles to build from. AARK aims to unify and democratise research
into a field critical to providing safer roads and trusted autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Carbon Emission Flow Tracing: Fast Algorithm and California Grid Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Shen, Yuanyuan Shi, Daniel Kirschen, Yize Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power systems decarbonization are at the focal point of the clean energy
transition. While system operators and utility companies increasingly publicize
system-level carbon emission information, it remains unclear how emissions from
individual generators are transported through the grid and how they impact
electricity users at specific locations. This paper presents a novel and
computationally efficient approach for exact quantification of nodal average
and marginal carbon emission rates, applicable to both AC and DC optimal power
flow problems. The approach leverages graph-based topological sorting and
directed cycle removal techniques, applied to directed graphs formed by
generation dispatch and optimal power flow solutions. Our proposed algorithm
efficiently identifies each generator's contribution to each node, capturing
how emissions are spatially distributed under varying system conditions. To
validate its effectiveness and reveal locational and temporal emission patterns
in the real world, we simulate the 8,870-bus realistic California grid using
actual CAISO data and the CATS model. Based on year long hourly data on nodal
loads and renewable generation, obtained or estimated from CAISO public data,
our method accurately estimates power flow conditions, generation mixes, and
systemwide emissions, and delivers fine grained spatiotemporal emission
analysis for every California county. Both our algorithm and the California
study are open-sourced, providing a foundation for future research on grid
emissions, planning, operations, and energy policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Submission, 16 pages, 12 figures, code available at
  https://github.com/yuqing5/Carbon-Tracker-California</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid impedance estimation based Kalman Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17325v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17325v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuoc Sang Nguyen, Ghavameddin Nourbakhsh, Gerard Ledwich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern power systems face new operational hurdles due to the increasing
adoption of inverter-coupled distributed energy resources, which impact system
stability and control. Central to these challenges is the dynamic nature of
grid impedance. To address this, a novel real-time estimation algorithm based
on the Discrete Fourier Transform is proposed. This algorithm is embedded
within an Advanced Angle Estimation Kalman Filter framework that employs a
Linear Quadratic Regulator for current control (AAEKF-LQR). The impedance data
directly informs and refines the controller's phase angle estimation.
Simulation analyses demonstrate robust collaboration between the estimator and
controller, sustaining system stability under weak grid conditions. The
technique proves capable of delivering swift and accurate impedance updates
during grid variations, which is crucial for maintaining stable inverter
operation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.05702v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.05702v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhang, Ahmad Mohammad Saber, Amr Youssef, Deepa Kundur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern power grids face unprecedented complexity from Distributed Energy
Resources (DERs), Electric Vehicles (EVs), and extreme weather, while also
being increasingly exposed to cyberattacks that can trigger grid violations.
This paper introduces Grid-Agent, an autonomous AI-driven framework that
leverages Large Language Models (LLMs) within a multi-agent system to detect
and remediate violations. Grid-Agent integrates semantic reasoning with
numerical precision through modular agents: a planning agent generates
coordinated action sequences using power flow solvers, while a validation agent
ensures stability and safety through sandboxed execution with rollback
mechanisms. To enhance scalability, the framework employs an adaptive
multi-scale network representation that dynamically adjusts encoding schemes
based on system size and complexity. Violation resolution is achieved through
optimizing switch configurations, battery deployment, and load curtailment. Our
experiments on IEEE and CIGRE benchmark networks, including the IEEE 69-bus,
CIGRE MV, IEEE 30-bus test systems, demonstrate superior mitigation
performance, highlighting Grid-Agent's suitability for modern smart grids
requiring rapid, adaptive response.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Equivalence of Koopman Eigenfunctions and Commuting Symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Jiang, Yan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Koopman operator framework offers a way to represent a nonlinear system
as a linear one. The key to this simplification lies in the identification of
eigenfunctions. While various data-driven algorithms have been developed for
this problem, a theoretical characterization of Koopman eigenfunctions from
geometric properties of the flow is still missing. This paper provides such a
characterization by establishing an equivalence between a set of Koopman
eigenfunctions and a set of commuting symmetries -- both assumed to span the
tangent spaces at every point on a simply connected open set. Based on this
equivalence, we build an explicit and convergent formula for the principal
Koopman eigenfunctions defined on the region of attraction of a locally
asymptotically stable equilibrium point, thereby offering a constructive
formula to compute Koopman eigenfunctions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximally Resilient Controllers under Temporal Logic Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Ait Si, Ratnangshu Das, Negar Monir, Sadegh Soudjani, Pushpak Jagtap, Adnane Saoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the notion of resilience of a dynamical system,
defined by the maximum disturbance a controlled dynamical system can withstand
while satisfying given temporal logic specifications. Given a dynamical system
and a specification, the objective is to synthesize the controller such that
the closed-loop system satisfies this specification while maximizing its
resilience. The problem is formulated as a robust optimization program where
the objective is to compute the maximum resilience while simultaneously
synthesizing the corresponding controller parameters. For linear systems and
linear controllers, exact solutions are provided for the class of time-varying
polytopic specifications. For the case of nonlinear systems, nonlinear
controllers and more general specifications, we leverage tools from the
scenario optimization approach, offering a probabilistic guarantee of the
solution as well as computational feasibility. Different case studies are
presented to illustrate the theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecture <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dato: A Task-Based Programming Model for Dataflow Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihan Fang, Hongzheng Chen, Niansong Zhang, Jiajie Li, Han Meng, Adrian Liu, Zhiru Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal
  activity sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leidy Mabel Alvero-Gonzalez, Matias Miguez, Eric Gutierrez, Juan Sapriza, Susana Patón, David Atienza, José Miranda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous monitoring of electrodermal activity (EDA) through wearable
devices has attracted much attention in recent times. However, the persistent
challenge demands analog front-end (AFE) systems with high sensitivity, low
power consumption, and minimal calibration requirements to ensure practical
usability in wearable technologies. In response to this challenge, this
research introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog
Readout tailored for continuous EDA sensing. The results show that our system
achieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS
range and a negligible relative error of less than 0.0025% for
fixed-resistance. Furthermore, the proposed system consumes only an average of
2.3 uW based on post-layout validations and introduces a low noise
contribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.
This research aims to drive the evolution of wearable sensors characterized by
seamless adaptability to diverse users, minimal power consumption, and
outstanding noise resilience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ
  Bioprinting Monitoring <span class="chip">ICRA 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usman Haider, Lukasz Szemet, Daniel Kelly, Vasileios Sergis, Andrew C. Daly, Karl Mason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bioprinting is a rapidly advancing field that offers a transformative
approach to fabricating tissue and organ models through the precise deposition
of cell-laden bioinks. Ensuring the fidelity and consistency of printed
structures in real-time remains a core challenge, particularly under
constraints imposed by limited imaging data and resource-constrained embedded
hardware. Semantic segmentation of the extrusion process, differentiating
between nozzle, extruded bioink, and surrounding background, enables in situ
monitoring critical to maintaining print quality and biological viability. In
this work, we introduce a lightweight semantic segmentation framework tailored
for real-time bioprinting applications. We present a novel, manually annotated
dataset comprising 787 RGB images captured during the bioprinting process,
labeled across three classes: nozzle, bioink, and background. To achieve fast
and efficient inference suitable for integration with bioprinting systems, we
propose a BioLite U-Net architecture that leverages depthwise separable
convolutions to drastically reduce computational load without compromising
accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based
segmentation baselines using mean Intersection over Union (mIoU), Dice score,
and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess
real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%
and a Dice score of 96.17%, while being over 1300x smaller than
MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,
demonstrating near real-time capability. Compared to MobileNet baselines,
BioLite U-Net offers a superior tradeoff between segmentation accuracy,
efficiency, and deployability, making it highly suitable for intelligent,
closed-loop bioprinting systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, conference-style submission (ICRA 2026). Includes
  dataset description, BioLite U-Net architecture, benchmark results on edge
  device (Raspberry Pi 4B)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware Acceleration in Portable MRIs: State of the Art and Future
  Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Al Habsi, Safa Mohammed Sali, Anis Meribout, Mahmoud Meribout, Saif Almazrouei, Mohamed Seghier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in portable MRI (pMRI) systems for point-of-care
imaging, particularly in remote or resource-constrained environments. However,
the computational complexity of pMRI, especially in image reconstruction and
machine learning (ML) algorithms for enhanced imaging, presents significant
challenges. Such challenges can be potentially addressed by harnessing hardware
application solutions, though there is little focus in the current pMRI
literature on hardware acceleration. This paper bridges that gap by reviewing
recent developments in pMRI, emphasizing the role and impact of hardware
acceleration to speed up image acquisition and reconstruction. Key technologies
such as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays
(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent
performance in terms of reconstruction speed and power consumption. This review
also highlights the promise of AI-powered reconstruction, open low-field pMRI
datasets, and innovative edge-based hardware solutions for the future of pMRI
technology. Overall, hardware acceleration can enhance image quality, reduce
power consumption, and increase portability for next-generation pMRI
technology. To accelerate reproducible AI for portable MRI, we propose forming
a Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,
retrospective multi-center testing, prospective reader and non-inferiority
trials) to provide standardized datasets, benchmarks, and regulator-ready
testbeds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent
  Data Corruption inducing Circuit-Level Faults 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoqi Wei, Senling Wang, Hiroshi Kai, Yoshinobu Higami, Ruijun Ma, Tianming Ni, Xiaoqing Wen, Hiroshi Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures, plan to submit to ACM TODAES</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amplifying Effective CXL Memory Bandwidth for LLM Inference via
  Transparent Near-Data Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) inference is bottlenecked by the limited bandwidth
of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a
transparent near-data processing architecture that amplifies effective CXL
bandwidth without requiring changes to the CXL.mem interface or AI models.
CXL-NDP integrates a precision-scalable bit-plane layout for dynamic
quantization with transparent lossless compression of weights and KV caches
directly within the CXL device. In end-to-end serving, CXL-NDP improves
throughput by 43%, extends the maximum context length by 87%, and reduces the
KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms
its practicality with a modest silicon footprint, lowering the barrier for
adopting efficient, scalable CXL-based memory in generative AI infrastructure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELK: Exploring the Efficiency of Inter-core Connected AI Chips with Deep
  Learning Compiler Techniques <span class="chip">MICRO'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqi Liu, Yuqi Xue, Noelle Crawford, Jilong Xue, Jian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at the 58th IEEE/ACM International Symposium
  on Microarchitecture (MICRO'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spec2RTL-Agent: Automated Hardware Code Generation from Complex
  Specifications Using LLM Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongzhi Yu, Mingjie Liu, Michael Zimmer, Yingyan Celine Lin, Yong Liu, Haoxing Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in generating hardware RTL code with LLMs, existing
solutions still suffer from a substantial gap between practical application
scenarios and the requirements of real-world RTL code development. Prior
approaches either focus on overly simplified hardware descriptions or depend on
extensive human guidance to process complex specifications, limiting their
scalability and automation potential. In this paper, we address this gap by
proposing an LLM agent system, termed Spec2RTL-Agent, designed to directly
process complex specification documentation and generate corresponding RTL code
implementations, advancing LLM-based RTL code generation toward more realistic
application settings. To achieve this goal, Spec2RTL-Agent introduces a novel
multi-agent collaboration framework that integrates three key enablers: (1) a
reasoning and understanding module that translates specifications into
structured, step-by-step implementation plans; (2) a progressive coding and
prompt optimization module that iteratively refines the code across multiple
representations to enhance correctness and synthesisability for RTL conversion;
and (3) an adaptive reflection module that identifies and traces the source of
errors during generation, ensuring a more robust code generation flow. Instead
of directly generating RTL from natural language, our system strategically
generates synthesizable C++ code, which is then optimized for HLS. This
agent-driven refinement ensures greater correctness and compatibility compared
to naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three
specification documents, showing it generates accurate RTL code with up to 75%
fewer human interventions than existing methods. This highlights its role as
the first fully automated multi-agent system for RTL generation from
unstructured specs, reducing reliance on human effort in hardware design.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanized Metatheory of Forward Reasoning for End-to-End
  Linearizability Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Kent, Ugur Y. Yavuz, Siddhartha Jayanti, Stephanie Balzer, Guy Blelloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past decade, many techniques have been developed to prove
linearizability, the gold standard of correctness for concurrent data
structures. Intuitively, linearizability requires that every operation on a
concurrent data structure appears to take place instantaneously, even when
interleaved with other operations. Most recently, Jayanti et al. presented the
first sound and complete "forward reasoning" technique for proving
linearizability that relates the behavior of a concurrent data structure to a
reference atomic data structure as time moves forward. This technique can be
used to produce machine-checked proofs of linearizability in TLA+. However,
while Jayanti et al.'s approach is shown to be sound and complete, a
mechanization of this important metatheoretic result is still outstanding. As a
result, it is not possible to produce verified end-to-end proofs of
linearizability. To reduce the size of this trusted computing base, we
formalize this forward reasoning technique and mechanize proofs of its
soundness and completeness in Rocq. As a case study, we use the approach to
produce a verified end-to-end proof of linearizability for a simple concurrent
register.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIO: Multiverse Debugging in the Face of Input/Output -- Extended
  Version with Additional Appendices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Lauwaerts, Maarten Steevens, Christophe Scholliers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This extended version provides auxiliary material to the article of
  the same title that will appear in the ACM Digital Library as part of the
  PACMPL issue for OOPSLA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dato: A Task-Based Programming Model for Dataflow Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihan Fang, Hongzheng Chen, Niansong Zhang, Jiajie Li, Han Meng, Adrian Liu, Zhiru Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Termination Analysis of Linear-Constraint Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir M. Ben-Amram, Samir Genaim, Joël Ouaknine, James Worrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This Survey provides an overview of techniques in termination analysis for
programs with numerical variables and transitions defined by linear
constraints. This subarea of program analysis is challenging due to the
existence of undecidable problems, and this Survey systematically explores
approaches that mitigate this inherent difficulty. These include foundational
decidability results, the use of ranking functions, and disjunctive
well-founded transition invariants. The Survey also discusses non-termination
witnesses, used to prove that a program will not halt. We examine the
algorithmic and complexity aspects of these methods, showing how different
approaches offer a trade-off between expressive power and computational
complexity. The Survey does not discuss how termination analysis is performed
on real-world programming languages, nor does it consider more expressive
abstract models that include non-linear arithmetic, probabilistic choice, or
term rewriting systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pacing Types: Safe Monitoring of Asynchronous Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kohn, Arthur Correnson, Jan Baumeister, Bernd Finkbeiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stream-based monitoring is a real-time safety assurance mechanism for complex
cyber-physical systems such as unmanned aerial vehicles. In this context, a
monitor aggregates streams of input data from sensors and other sources to give
real-time statistics and assessments of the system's health. Since monitors are
safety-critical components, it is crucial to ensure that they are free of
potential runtime errors. One of the central challenges in designing reliable
stream-based monitors is to deal with the asynchronous nature of data streams:
in concrete applications, the different sensors being monitored produce values
at different speeds, and it is the monitor's responsibility to correctly react
to the asynchronous arrival of different streams of values. To ease this
process, modern frameworks for stream-based monitoring such as RTLola feature
an expressive specification language that allows to finely specify data
synchronization policies. While this feature dramatically simplifies the design
of monitors, it can also lead to subtle runtime errors. To mitigate this issue,
this paper presents pacing types, a novel type system implemented in RTLola to
ensure that monitors for asynchronous streams are well-behaved at runtime. We
formalize the essence of pacing types for a core fragment of RTLola, and
present a soundness proof of the pacing type system using a new logical
relation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Software Model Checking via Summary-Guided Search (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.15137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.15137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Fang, Zachary Kincaid, Thomas Reps
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a diverse suite of benchmarks including programs from the Software
Verification Competition (SV-COMP), from prior literature, as well as synthetic
programs based on examples in this paper. We found that our implementation of
GPS outperforms state-of-the-art software model checkers (including the top
performers in SV-COMP ReachSafety-Loops category), both in terms of the number
of benchmarks solved and in terms of running time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of paper in OOPSLA 2025 (with typo and stylistic
  fixes compared to v2 manuscript). 37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Specification-Guided Repair of Arithmetic Errors in Dafny Programs using
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03659v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03659v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentina Wu, Alexandra Mendes, Alexandre Abreu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Debugging and repairing faults when programs fail to formally verify can be
complex and time-consuming. Automated Program Repair (APR) can ease this burden
by automatically identifying and fixing faults. However, traditional APR
techniques often rely on test suites for validation, but these may not capture
all possible scenarios. In contrast, formal specifications provide strong
correctness criteria, enabling more effective automated repair.
  In this paper, we present an APR tool for Dafny, a verification-aware
programming language that uses formal specifications - including
pre-conditions, post-conditions, and invariants - as oracles for fault
localization and repair. Assuming the correctness of the specifications and
focusing on arithmetic bugs, we localize faults through a series of steps,
which include using Hoare logic to determine the state of each statement within
the program, and applying Large Language Models (LLMs) to synthesize candidate
fixes. The models considered are GPT-4o mini, Llama 3, Mistral 7B, and Llemma
7B.
  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny
programs. Our tool achieves 89.6% fault localization coverage and GPT-4o mini
yields the highest repair success rate of 74.18%. These results highlight the
potential of combining formal reasoning with LLM-based program synthesis for
automated program repair.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Data Structures and Algorithms <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering Select Support for Hybrid Bitvectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Chiu, Dominik Kempa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the central problems in the design of compressed data structures is
the efficient support for rank and select queries on bitvectors. These two
operations form the backbone of more complex data structures (such as wavelet
trees) used for the compact representation of texts, trees, graphs, or grids.
Their efficient implementation is one of the most frequently studied problems
in compressed data structures.
  One effective solution is the so-called hybrid bitvector implementation,
which partitions the input bitvector into blocks and adaptively selects an
encoding method, such as run-length, plain, or minority encoding, based on
local redundancy. Experiments have shown that hybrid bitvectors achieve
excellent all-around performance on repetitive and non-repetitive inputs.
  However, current implementations support only rank queries (i.e., counting
the number of ones up to a given position) and lack support for select queries.
This limitation significantly restricts their applicability. In this paper, we
propose a method to add support for select queries to hybrid bitvectors, and we
conduct an extensive set of experiments. Our results show that hybrid
bitvectors offer excellent performance, matching the speed of the fastest and
the space efficiency of the most compact existing bitvectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Steiner Shortest Path Tree Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Asher, Yefim Dinitz, Shlomi Dolev, Li-on Raviv, Baruch Schieber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce and study a novel problem of computing a shortest path tree with
a minimum number of non-terminals. It can be viewed as an (unweighted) Steiner
Shortest Path Tree (SSPT) that spans a given set of terminal vertices by
shortest paths from a given source while minimizing the number of nonterminal
vertices included in the tree. This problem is motivated by applications where
shortest-path connections from a source are essential, and where reducing the
number of intermediate vertices helps limit cost, complexity, or overhead. We
show that the SSPT problem is NP-hard. To approximate it, we introduce and
study the shortest path subgraph of a graph. Using it, we show an
approximation-preserving reduction of SSPT to the uniform vertex-weighted
variant of the Directed Steiner Tree (DST) problem, termed UVDST. Consequently,
the algorithm of [Grandoni et al., 2023] approximating DST implies a
quasi-polynomial polylog-approximation algorithm for SSPT. We present a
polynomial polylog-approximation algorithm for UVDST, and thus for SSPT, for a
restricted class of graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Freeness is All You Need: A Weitz-Type FPTAS for the Entire
  Lee-Yang Zero-Free Region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Shao, Ke Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Weitz-type FPTAS for the ferromagnetic Ising model across the
entire Lee-Yang zero-free region, without relying on the strong spatial mixing
(SSM) property. Our algorithm is Weitz-type for two reasons. First, it
expresses the partition function as a telescoping product of ratios, with the
key being to approximate each ratio. Second, it uses Weitz's self-avoiding walk
tree, and truncates it at logarithmic depth to give a good and efficient
approximation. The key difference from the standard Weitz algorithm is that we
approximate a carefully designed edge-deletion ratio instead of the marginal
probability of a vertex's spin, ensuring our algorithm does not require SSM.
  Furthermore, by establishing local dependence of coefficients (LDC), we
indeed prove a novel form of SSM for these edge-deletion ratios, which, in
turn, implies the standard SSM for the random cluster model. This is the first
SSM result for the random cluster model on general graphs, beyond lattices. We
prove LDC using a new division relation, and remarkably, such relations hold
quite universally. As a result, we establish LDC for a variety of models.
Combined with existing zero-freeness results for these models, we derive new
SSM results for them. Our work suggests that both Weitz-type FPTASes and SSM
can be derived from zero-freeness, while zero-freeness alone suffices for
Weitz-type FPTASes, SSM additionally requires LDC, a combinatorial property
independent of zero-freeness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal streaming algorithm for detecting $\ell_2$ heavy hitters in
  random order streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santhoshini Velusamy, Huacheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a stream $x_1,x_2,\dots,x_n$ of items from a Universe $U$ of size
$\mathsf{poly}(n)$, and a parameter $\epsilon>0$, an item $i\in U$ is said to
be an $\ell_2$ heavy hitter if its frequency $f_i$ in the stream is at least
$\sqrt{\epsilon F_2}$, where $F_2=\sqrt{\sum_{i\in U} f_i^2}$. The classical
$\mathsf{CountSketch}$ algorithm due to Charikar, Chen, and Farach-Colton
[2004], was the first algorithm to detect $\ell_2$ heavy hitters using
$O\left(\frac{\log^2 n}{\epsilon}\right)$ bits of space, and their algorithm is
optimal for streams with deletions. For insertion-only streams, Braverman,
Chestnut, Ivkin, Nelson, Wang, and Woodruff [2017] gave the $\mathsf{BPTree}$
algorithm which requires only $O\left(\frac{\log(1/\epsilon)}{\epsilon}\log n
\right)$ space. Note that any algorithm requires at least
$O\left(\frac{1}{\epsilon} \log n\right)$ space to output $O(1/\epsilon)$ heavy
hitters in the worst case. So for constant $\epsilon$, the space usage of the
$\mathsf{BPTree}$ algorithm is optimal but their bound could be sub-optimal for
$\epsilon=o(1)$. In this work, we show that for random order streams, where the
stream elements can be adversarial but their order of arrival is uniformly
random, it is possible to achieve the optimal space bound of
$O\left(\frac{1}{\epsilon} \log n\right)$ for every $\epsilon =
\Omega\left(\frac{1}{2^{\sqrt{\log n}}}\right)$. We also show that for
partially random order streams where only the heavy hitters are required to be
uniformly distributed in the stream, it is possible to achieve the same space
bound, but with an additional assumption that the algorithm is given a constant
approximation to $F_2$ in advance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum algorithms for general nonlinear dynamics based on the Carleman
  embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Jennings, Kamil Korzekwa, Matteo Lostaglio, Andrew T Sornborger, Yigit Subasi, Guoming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Important nonlinear dynamics, such as those found in plasma and fluid
systems, are typically hard to simulate on classical computers. Thus, if
fault-tolerant quantum computers could efficiently solve such nonlinear
problems, it would be a transformative change for many industries. In a recent
breakthrough [Liu et al., PNAS 2021], the first efficient quantum algorithm for
solving nonlinear differential equations was constructed, based on a single
condition $R<1$, where $R$ characterizes the ratio of nonlinearity to
dissipation. This result, however, is limited to the class of purely
dissipative systems with negative log-norm, which excludes application to many
important problems. In this work, we correct technical issues with this and
other prior analysis, and substantially extend the scope of nonlinear dynamical
systems that can be efficiently simulated on a quantum computer in a number of
ways. Firstly, we extend the existing results from purely dissipative systems
to a much broader class of stable systems, and show that every quadratic
Lyapunov function for the linearized system corresponds to an independent
$R$-number criterion for the convergence of the Carlemen scheme. Secondly, we
extend our stable system results to physically relevant settings where
conserved polynomial quantities exist. Finally, we provide extensive results
for the class of non-resonant systems. With this, we are able to show that
efficient quantum algorithms exist for a much wider class of nonlinear systems
than previously known, and prove the BQP-completeness of nonlinear oscillator
problems of exponential size. In our analysis, we also obtain several results
related to the Poincar\'{e}-Dulac theorem and diagonalization of the Carleman
matrix, which could be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>70+78 pages, 4 figures. Comments welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resident fitness computation in linear time and other algorithmic
  aspects of interacting trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katalin Friedl, Viktória Nemkin, András Tóbiás
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The notion of a system of interacting trajectories was recently introduced by
Hermann, Gonz\'alez Casanova, Soares dos Santos, T\'obi\'as and Wakolbinger.
Such a system of $[0,1]$-valued piecewise linear trajectories arises as a
scaling limit of the system of logarithmic subpopulation sizes in a
population-genetic model (more precisely, a Moran model) with mutation and
selection. By definition, the resident fitness is initially 0 and afterwards it
increases by the ultimate slope of each trajectory that reaches height 1.
  We show that although the interaction of $n$ trajectories may yield
$\Omega(n^2)$ slope changes in total, the resident fitness function can be
computed algorithmically in $O(n)$ time. Our algorithm uses the so-called
continued lines representation of the system of interacting trajectories. In
the special case of Poissonian interacting trajectories where the birth times
of the trajectories form a Poisson process and the initial slopes are random
and i.i.d., we provide a linear bound on the expected total number of slope
changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved sampling algorithms and Poincaré inequalities for
  non-log-concave distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11236v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11236v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen He, Zhehan Lei, Jianan Shao, Chihao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of sampling from a distribution $\mu$ with density
$\propto e^{-V}$ for some potential function $V:\mathbb R^d\to \mathbb R$ with
query access to $V$ and $\nabla V$. We start with the following standard
assumptions:
  (1) The potential function $V$ is $L$-smooth.
  (2) The second moment $\mathbf{E}_{X\sim \mu}[\|X\|^2]\leq M$.
  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling
from such distributions is at least
$\left(\frac{LM}{d\epsilon}\right)^{\Omega(d)}$ where $\epsilon$ is the desired
accuracy in total variation distance, and the Poincar\'e constant can be
arbitrarily large.
  Meanwhile, another common assumption in the study of diffusion based samplers
(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))
strengthens the smoothness condition (1) to the following:
  (1*) The potential function of *every* distribution along the
Ornstein-Uhlenbeck process starting from $\mu$ is $L$-smooth.
  We show that under the assumptions (1*) and (2), the query complexity of
sampling from $\mu$ can be $\mathrm{poly}(L,d)\cdot
\left(\frac{Ld+M}{\epsilon^2}\right)^{\mathcal{O}(L+1)}$, which is polynomial
in $d$ and $\frac{1}{\epsilon}$ when $L=\mathcal{O}(1)$ and
$M=\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query
complexity developed by Huang et al. (COLT'24). Our results imply that the
seemly moderate strengthening of the smoothness condition (1) to (1*) can lead
to an exponential gap in the query complexity of sampling algorithms.
  Moreover, we show that together with the assumption (1*) and the stronger
moment assumption that $\|X\|$ is $\lambda$-sub-Gaussian for $X\sim\mu$, the
Poincar\'e constant of $\mu$ is at most $\mathcal{O}(\lambda)^{2(L+1)}$. As an
application of our technique, we obtain improved estimate of the Poincar\'e
constant for mixture of Gaussians with the same covariance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ With a Little Help From My Friends: Exploiting Probability Distribution
  Advice in Algorithm Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément L. Canonne, Kenny Chen, Julián Mestre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study online algorithms with predictions using distributional advice, a
type of prediction that arises when leveraging expert knowledge or historical
data. To demonstrate the usefulness and versatility of this framework, we focus
on the fundamental problem of online metric matching, considering both the
fractional and integral variants. Our main positive result is, for the former,
an algorithm achieving the optimal cost under perfect advice, while smoothly
defaulting to competitive ratios comparable to advice-free algorithms as the
prediction's quality degrades. For the integral matching, we are able to
provide an algorithm with essentially the same guarantees, up to an additive
sublinear term. We conclude by discussing how our algorithmic framework can be
extended to other online optimization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extending the results on online matching, and removing the warmup
  section on prophet inequalities after being made aware of a gap in one of the
  earlier proofs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constructive l2-Discrepancy Minimization with Additive Deviations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21423v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21423v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The \emph{signed series} problem in the $\ell_2$ norm asks, given set of
vectors $v_1,\ldots,v_n\in \mathbf{R}^d$ having at most unit $\ell_2$ norm,
does there always exist a series $(\varepsilon_i)_{i\in [n]}$ of $\pm 1$ signs
such that for all $i\in [n]$, $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i
v_i\|_2 = O(\sqrt{d})$. A result of Banaszczyk [2012, \emph{Rand. Struct.
Alg.}] states that there exist signs $\varepsilon_i\in \{-1,1\},\; i\in [n]$
such that $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i v_i\|_2 =
O(\sqrt{d+\log n})$. The best constructive bound known so far is of
$O(\sqrt{d\log n})$, by Bansal and Garg [2017, \emph{STOC.}, 2019, \emph{SIAM
J. Comput.}]. We give a polynomial-time randomized algorithm to find signs
$x(i) \in \{-1,1\},\; i\in [n]$ such that \[ \max_{i\in [n]} \|\sum_{j=1}^i
x(i)v_i\|_2 = O(\sqrt{d + \log^2 n}) = O(\sqrt{d}+\log n).\] By the
constructive reduction of Harvey and Samadi [\emph{COLT}, 2014], this also
yields a constructive bound of $O(\sqrt{d}+\log n)$ for the Steinitz problem in
the $\ell_2$-norm. Thus, we algorithmically achieve Banaszczyk's bounds for
both problems when $d \geq \log^2n$, which also matches the conjectured bounds.
Our algorithm is based on the framework on Bansal and Garg, together with a new
analysis involving $(i)$ additional linear and spectral orthogonality
constraints during the construction of the covariance matrix of the random walk
steps, which allow us to control the quadratic variation in the linear as well
as the quadratic components of the discrepancy increment vector, alongwith
$(ii)$ a ``Freedman-like" version of the Hanson-Wright concentration
inequality, for filtration-dependent sums of subgaussian chaoses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preliminary version was submitted to a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Average-Value Allocation Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshipra Bhawalkar, Zhe Feng, Anupam Gupta, Aranyak Mehta, David Wajc, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of centralized algorithms for welfare-maximizing
allocation of goods to buyers subject to average-value constraints. We show
that this problem is NP-hard to approximate beyond a factor of $\frac{e}{e-1}$,
and provide a $\frac{4e}{e-1}$-approximate offline algorithm. For the online
setting, we show that no non-trivial approximations are achievable under
adversarial arrivals. Under i.i.d. arrivals, we present a polytime online
algorithm that provides a constant approximation of the optimal
(computationally-unbounded) online algorithm. In contrast, we show that no
constant approximation of the ex-post optimum is achievable by an online
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDPs and Robust Satisfiability of Promise CSP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08373v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08373v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Brakensiek, Venkatesan Guruswami, Sai Sandeep
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a constraint satisfaction problem (CSP), a robust satisfaction algorithm
is one that outputs an assignment satisfying most of the constraints on
instances that are near-satisfiable. It is known that the CSPs that admit
efficient robust satisfaction algorithms are precisely those of bounded width,
i.e., CSPs whose satisfiability can be checked by a simple local consistency
algorithm (eg., 2-SAT or Horn-SAT in the Boolean case). While the exact
satisfiability of a bounded width CSP can be checked by combinatorial
algorithms, the robust algorithm is based on rounding a canonical Semidefinite
Programming (SDP) relaxation.
  In this work, we initiate the study of robust satisfaction algorithms for
promise CSPs, which are a vast generalization of CSPs that have received much
attention recently. The motivation is to extend the theory beyond CSPs, as well
as to better understand the power of SDPs. We present robust SDP rounding
algorithms under some general conditions, namely the existence of particular
high-dimensional Boolean symmetries known as majority or alternating threshold
polymorphisms. On the hardness front, we prove that the lack of such
polymorphisms makes the PCSP hard for all pairs of symmetric Boolean
predicates. Our approach relies on SDP integrality gaps argued via the absence
of certain colorings of the sphere, with connections to sphere Ramsey theory.
  We conjecture that PCSPs with robust satisfaction algorithms are precisely
those for which the feasibility of the canonical SDP implies (exact)
satisfiability. We also give a precise algebraic condition, known as a minion
characterization, of which PCSPs have the latter property.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Discrete Analysis</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling <span class="highlight-title">Transformer</span>-Based Novel View Synthesis Models with Token
  Disentanglement and Synthetic Data <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nithin Gopalakrishnan Nair, Srinivas Kaza, Xuan Luo, Vishal M. Patel, Stephen Lombardi, Jungyeon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large transformer-based models have made significant progress in
generalizable novel view synthesis (NVS) from sparse input views, generating
novel viewpoints without the need for test-time optimization. However, these
models are constrained by the limited diversity of publicly available scene
datasets, making most real-world (in-the-wild) scenes out-of-distribution. To
overcome this, we incorporate synthetic training data generated from diffusion
models, which improves generalization across unseen domains. While synthetic
data offers scalability, we identify artifacts introduced during data
generation as a key bottleneck affecting reconstruction quality. To address
this, we propose a token disentanglement process within the transformer
architecture, enhancing feature separation and ensuring more effective
learning. This refinement not only improves reconstruction quality over
standard transformers but also enables scalable training with synthetic data.
As a result, our method outperforms existing models on both in-dataset and
cross-dataset evaluations, achieving state-of-the-art results across multiple
benchmarks while significantly reducing computational costs. Project page:
https://scaling3dnvs.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital
  Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marilyn Keller, Keenon Werling, Soyong Shin, Scott Delp, Sergi Pujades, C. Karen Liu, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Great progress has been made in estimating 3D human pose and shape from
images and video by training neural networks to directly regress the parameters
of parametric human models like SMPL. However, existing body models have
simplified kinematic structures that do not correspond to the true joint
locations and articulations in the human skeletal system, limiting their
potential use in biomechanics. On the other hand, methods for estimating
biomechanically accurate skeletal motion typically rely on complex motion
capture systems and expensive optimization methods. What is needed is a
parametric 3D human model with a biomechanically accurate skeletal structure
that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL
body model with a biomechanics skeleton. To enable this, we need training data
of skeletons inside SMPL meshes in diverse poses.
  We build such a dataset by optimizing biomechanically accurate skeletons
inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL
mesh vertices to the optimized joint locations and bone rotations. Finally, we
re-parametrize the SMPL mesh with the new kinematic parameters. The resulting
SKEL model is animatable like SMPL but with fewer, and
biomechanically-realistic, degrees of freedom. We show that SKEL has more
biomechanically accurate joint locations than SMPL, and the bones fit inside
the body surface better than previous methods. By fitting SKEL to SMPL meshes
we are able to "upgrade" existing human pose and shape datasets to include
biomechanical parameters. SKEL provides a new tool to enable biomechanics in
the wild, while also providing vision and graphics researchers with a better
constrained and more realistic model of human articulation. The model, code,
and data are available for research at https://skel.is.tue.mpg.de..
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of
  Hand-Drawn Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhou, Linzi Qu, Miu-Ling Lam, Hongbo Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-drawn character animation is a vibrant field in computer graphics,
presenting challenges in achieving geometric consistency while conveying
expressive motion. Traditional skeletal animation methods maintain geometric
consistency but struggle with complex non-rigid elements like flowing hair and
skirts, leading to unnatural deformation. Conversely, video diffusion models
synthesize realistic dynamics but often create geometric distortions in
stylized drawings due to domain gaps. This work proposes a hybrid animation
system that combines skeletal animation and video diffusion. Initially, coarse
images are generated from characters retargeted with skeletal animations for
geometric guidance. These images are then enhanced in texture and secondary
dynamics using video diffusion priors, framing this enhancement as an
inpainting task. A domain-adapted diffusion model refines user-masked regions
needing improvement, especially for secondary dynamics. To enhance motion
realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in
the denoising process, incorporating features from a pre-trained diffusion
model enriched with human motion priors. Additionally, to tackle unnatural
deformations from low-poly single-mesh character modeling, we present a Hair
Layering Modeling (HLM) technique that uses segmentation maps to separate hair
from the body, allowing for more natural animation of long-haired characters.
Extensive experiments show that our system outperforms state-of-the-art methods
in both quantitative and qualitative evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Computation of Voronoi Diagrams Using Point-in-Cell Tests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyang Xiao, Yao Li, Juan Cao, Zhonggui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the Voronoi diagram appears in many applications, the topic of
improving its computational efficiency remains attractive. We propose a novel
yet efficient method to compute Voronoi diagrams bounded by a given domain,
i.e., the clipped or restricted Voronoi diagrams. The intersection of the
domain and a Voronoi cell (domain-cell intersection) is generated by removing
the part outside the cell from the domain, which can be accomplished by several
clippings. Different from the existing methods, we present an edge-based search
scheme to find clipping planes (bisectors). A test called point-in-cell is
first set up to tell whether a space point is in a target Voronoi cell or not.
Then, for each edge of the intermediate domain-cell intersection, we will
launch a clipping only if its two endpoints are respectively inside and outside
the corresponding Voronoi cell, where the bisector for the clipping can be
found by using a few times of point-in-cell tests. Therefore, our method only
involves the clippings that contribute to the final results, which is a great
advantage over the state-of-the-art methods. Additionally, because each
domain-cell intersection can be generated independently, we extend the proposed
method to the GPUs for computing Voronoi diagrams in parallel. The experimental
results show the best performance of our method compared to state-of-the-art
ones, regardless of site distribution. This paper was first submitted to
SIGGRAPH Asia 2025.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVGauge: Towards Human-Aligned Evaluation for SVG Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Zini, Elia Frigieri, Sebastiano Aloscari, Marcello Generali, Lorenzo Dodi, Robert Dosen, Lorenzo Baraldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generated Scalable Vector Graphics (SVG) images demand evaluation criteria
tuned to their symbolic and vectorial nature: criteria that existing metrics
such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce
SVGauge, the first human-aligned, reference based metric for text-to-SVG
generation. SVGauge jointly measures (i) visual fidelity, obtained by
extracting SigLIP image embeddings and refining them with PCA and whitening for
domain alignment, and (ii) semantic consistency, captured by comparing
BLIP-2-generated captions of the SVGs against the original prompts in the
combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark
shows that SVGauge attains the highest correlation with human judgments and
reproduces system-level rankings of eight zero-shot LLM-based generators more
faithfully than existing metrics. Our results highlight the necessity of
vector-specific evaluation and provide a practical tool for benchmarking future
text-to-SVG generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 23rd edition of International Conference on Image
  Analysis and Processing 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On design, analysis, and hybrid manufacturing of microstructured
  blade-like geometries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Antolin, Michael Barton, Georges-Pierre Bonneau, Annalisa Buffa, Amaia Calleja-Ochoa, Gershon Elber, Stefanie Elgeti, Gaizka Gómez Escudero, Alicia Gonzalez, Haizea González Barrio, Stefanie Hahmann, Thibaut Hirschler, Q Youn Honga, Konstantin Key, Myung-Soo Kim, Michael Kofler, Norberto Lopez de Lacalle, Silvia de la Maza, Kanika Rajain, Jacques Zwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the evolution of new manufacturing technologies such as multi-material
3D printing, one can think of new type of objects that consist of considerably
less, yet heterogeneous, material, consequently being porous, lighter and
cheaper, while having the very same functionality as the original object when
manufactured from one single solid material. We aim at questioning five decades
of traditional paradigms in geometric CAD and focus at new generation of CAD
objects that are not solid, but contain heterogeneous free-form internal
microstructures. We propose a unified manufacturing pipeline that involves all
stages, namely design, optimization, manufacturing, and inspection of
microstructured free-form geometries. We demonstrate our pipeline on an
industrial test case of a blisk blade that sustains the desired pressure
limits, yet requires significantly less material when compared to the solid
counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The best approximation pair problem relative to two subsets in a normed
  space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18767v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18767v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Reem, Yair Censor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the classical best approximation pair (BAP) problem, one is given two
nonempty, closed, convex and disjoint subsets in a finite- or an
infinite-dimensional Hilbert space, and the goal is to find a pair of points,
each from each subset, which realizes the distance between the subsets. We
discuss the problem in more general normed spaces and with possibly non-convex
subsets, and focus our attention on the issues of uniqueness and existence of
the solution to the problem. As far as we know, these fundamental issues have
not received much attention. We present several sufficient geometric conditions
for the (at most) uniqueness of a BAP. These conditions are related to the
structure and the relative orientation of the boundaries of the subsets and to
the norm. We also present many sufficient conditions for the existence of a
BAP. Our results significantly extend the horizon of a recent algorithm for
solving the BAP problem [Censor, Mansour, Reem, J. Approx. Theory (2024)]. The
paper also shows, perhaps for the first time, how wide is the scope of the BAP
problem in terms of the scientific communities which are involved in it
(frequently independently) and in terms of its applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correction of a misprint in the Acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network
  Weight Space Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Guoxing Sun, Marc Habermann, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating human avatars is a highly desirable yet challenging task. Recent
advancements in radiance field rendering have achieved unprecedented
photorealism and real-time performance for personalized dynamic human avatars.
However, these approaches are typically limited to person-specific rendering
models trained on multi-view video data for a single individual, limiting their
ability to generalize across different identities. On the other hand,
generative approaches leveraging prior knowledge from pre-trained 2D diffusion
models can produce cartoonish, static human avatars, which are animated through
simple skeleton-based articulation. Therefore, the avatars generated by these
methods suffer from lower rendering quality compared to person-specific
rendering methods and fail to capture pose-dependent deformations such as cloth
wrinkles. In this paper, we propose a novel approach that unites the strengths
of person-specific rendering and diffusion-based generative modeling to enable
dynamic human avatar generation with both high photorealism and realistic
pose-dependent deformations. Our method follows a two-stage pipeline: first, we
optimize a set of person-specific UNets, with each network representing a
dynamic human avatar that captures intricate pose-dependent deformations. In
the second stage, we train a hyper diffusion model over the optimized network
weights. During inference, our method generates network weights for real-time,
controllable rendering of dynamic human avatars. Using a large-scale,
cross-identity, multi-view video dataset, we demonstrate that our approach
outperforms state-of-the-art human avatar generation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://vcai.mpi-inf.mpg.de/projects/HDA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for
  Modeling and Rendering Scattering and Emissive Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Condor, Sebastien Speierer, Lukas Bode, Aljaz Bozic, Simon Green, Piotr Didyk, Adrian Jarabo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient scene representations are essential for many computer graphics
applications. A general unified representation that can handle both surfaces
and volumes simultaneously, remains a research challenge. Inspired by recent
methods for scene reconstruction that leverage mixtures of 3D Gaussians to
model radiance fields, we formalize and generalize the modeling of scattering
and emissive media using mixtures of simple kernel-based volumetric primitives.
We introduce closed-form solutions for transmittance and free-flight distance
sampling for different kernels, and propose several optimizations to use our
method efficiently within any off-the-shelf volumetric path tracer. We
demonstrate our method as a compact and efficient alternative to other forms of
volume modeling for forward and inverse rendering of scattering media.
Furthermore, we adapt and showcase our method in radiance field optimization
and rendering, providing additional flexibility compared to current state of
the art given its ray-tracing formulation. We also introduce the Epanechnikov
kernel and demonstrate its potential as an efficient alternative to the
traditionally-used Gaussian kernel in scene reconstruction tasks. The
versatility and physically-based nature of our approach allows us to go beyond
radiance fields and bring to kernel-based modeling and rendering any
path-tracing enabled functionality such as scattering, relighting and complex
camera models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-07T00:00:00Z">2025-09-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Systems and Control <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 20 Years in Life of a Smart Building: A retrospective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karolina Skrivankova, Mark Handley, Stephen Hailes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operating an intelligent smart building automation system in 2025 is met with
many challenges: hardware failures, vendor obsolescence, evolving security
threats and more. None of these have been comprehensibly addressed by the
industrial building nor home automation industries, limiting feasibility of
operating large, truly smart automation deployments. This paper introduces
KaOS, a distributed control platform for constructing robust and evolvable
smart building automation systems using affordable, off-the-shelf IoT hardware.
Supporting control applications and distributed system operations by leveraging
containerisation and managed resource access, KaOS seeks to achieve
flexibility, security, and fault tolerance without sacrificing
cost-effectiveness. Initial evaluation confirms the practical feasibility of
our approach, highlighting its potential to sustainably maintain and
incrementally evolve building control functionalities over extended timeframes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ignore Drift, Embrace Simplicity: Constrained Nonlinear Control through
  Driftless Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Padmanabhan, Melkior Ornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel technique to drive a nonlinear system to reach a target
state under input constraints. The proposed controller consists only of
piecewise constant inputs, generated from a simple linear driftless
approximation to the original nonlinear system. First, we construct this
approximation using only the effect of the control input at the initial state.
Next, we partition the time horizon into successively shorter intervals and
show that optimal controllers for the linear driftless system result in a
bounded error from a specified target state in the nonlinear system. We also
derive conditions under which the input constraint is guaranteed to be
satisfied. On applying the optimal control inputs, we show that the error
monotonically converges to zero as the intervals become successively shorter,
thus achieving arbitrary closeness to the target state with time. Using
simulation examples on classical nonlinear systems, we illustrate how the
presented technique is used to reach a target state while still satisfying
input constraints. In particular, we show that our method completes the task
even when assumptions of the underlying theory are violated or when classical
linearization-based methods may fail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored
  Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pradyumna Kaushal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures. Whitepaper submission; LaTeX source with
  compiled .bbl. Includes architecture diagrams, tables, and code listings
  (TypeScript & Solidity)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Xu, Lihao Zhang, Yuyang Du, Qun Yang, Soung Chang Liew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in robotics has underscored the demand for real-time control
in applications such as manufacturing, healthcare, and autonomous systems,
where the timely delivery of mission-critical commands under heterogeneous
robotic traffic is paramount for operational efficacy and safety. In these
scenarios, mission-critical traffic follows a strict deadline-constrained
communication pattern: commands must arrive within defined QoS deadlines,
otherwise late arrivals can degrade performance or destabilize control loops.In
this work, we demonstrate on a real-time SDR platform that CSMA, widely adopted
in robotic communications,suffers severe degradation under high robot traffic
loads, with contention-induced collisions and delays disrupting the on-time
arrival of mission-critical packets. To address this problem, we propose an
IEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's
deterministic slot scheduling with CSMA's adaptability for heterogeneous robot
traffic.The protocol achieves collision-free, low-latency mission-critical
command delivery and IEEE 802.11 compatibility through the synergistic
integration of sub-microsecond PTP-based slot synchronization-essential for
establishing precise timing for TDMA, a three-session superframe with dynamic
TDMA allocation for structured and adaptable traffic management,and beacon-NAV
protection to preemptively secure these critical communication sessions from
interference. Emulation experiments on real-time SDR testbed and Robot
Operating System (ROS) simulation show that the proposed protocol reduces
missed-deadline errors by 93% compared to the CSMA baseline. In high-speed
robot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)
trajectory error by up to 90% compared with a CSMA baseline, all while
maintaining throughput for non-critical traffic within +-2%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid A* Path Planning with Multi-Modal Motion Extension for Four-Wheel
  Steering Mobile Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runjiao Bao, Lin Zhang, Tianwei Niu, Haoyu Yuan, Shoukun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Four-wheel independent steering (4WIS) systems provide mobile robots with a
rich set of motion modes, such as Ackermann steering, lateral steering, and
parallel movement, offering superior maneuverability in constrained
environments. However, existing path planning methods generally assume a single
kinematic model and thus fail to fully exploit the multi-modal capabilities of
4WIS platforms. To address this limitation, we propose an extended Hybrid A*
framework that operates in a four-dimensional state space incorporating both
spatial states and motion modes. Within this framework, we design multi-modal
Reeds-Shepp curves tailored to the distinct kinematic constraints of each
motion mode, develop an enhanced heuristic function that accounts for
mode-switching costs, and introduce a terminal connection strategy with
intelligent mode selection to ensure smooth transitions between different
steering patterns. The proposed planner enables seamless integration of
multiple motion modalities within a single path, significantly improving
flexibility and adaptability in complex environments. Results demonstrate
significantly improved planning performance for 4WIS robots in complex
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mutual Support by Sensor-Attacker Team for a Passive Target 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajakta Surve, Shaunak D. Bopardikar, Alexander Von Moll, Isaac Weintraub, David W. Casbeer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a pursuit game played between a team of a sensor and an attacker
and a mobile target in the unbounded Euclidean plane. The target is faster than
the sensor, but slower than the attacker. The sensor's objective is to keep the
target within a sensing radius so that the attacker can capture the target,
whereas the target seeks to escape by reaching beyond the sensing radius from
the sensor without getting captured by the attacker. We assume that as long as
the target is within the sensing radius from the sensor, the sensor-attacker
team is able to measure the target's instantaneous position and velocity. We
pose and solve this problem as a \emph{game of kind} in which the target uses
an open-loop strategy (passive target). Aside from the novel formulation, our
contributions are four-fold. First, we present optimal strategies for both the
sensor and the attacker, according to their respective objectives.
Specifically, we design a sensor strategy that maximizes the duration for which
the target remains within its sensing range, while the attacker uses
proportional navigation to capture the target. Second, we characterize the
\emph{sensable region} -- the region in the plane in which the target remains
within the sensing radius of the sensor during the game -- and show that
capture is guaranteed {if and only if} the Apollonius circle between the
attacker and the target is fully contained within this region. Third, we
{derive a lower bound} on the target's speed below which capture is guaranteed,
and an upper bound on the target speed above which there exists an escape
strategy for the target, from an arbitrary initial orientation between the
agents. Fourth, for a given initial orientation between the agents, we present
a sharper upper bound on the target speed above which there exists an escape
strategy for the target.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certifying the Nonexistence of Feasible Path Between Power System
  Operating Points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Rasoul Narimani, Katherine R. Davis, Daniel K. Molzahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By providing the optimal operating point that satisfies both the power flow
equations and engineering limits, the optimal power flow (OPF) problem is
central to the operation of electric power systems. While extensive research
efforts have focused on reliably computing high-quality OPF solutions,
assessing the feasibility of transitioning between operating points remains
challenging since the feasible spaces of OPF problems may consist of multiple
disconnected components. It is not possible to transition between operating
points in different disconnected components without violating OPF constraints.
To identify such situations, this paper introduces an algorithm for certifying
the infeasibility of transitioning between two operating points within an OPF
feasible space. As an indication of potential disconnectedness, the algorithm
first seeks an infeasible point on the line connecting a pair of feasible
points. The algorithm then certifies disconnectedness by using convex
relaxation and bound tightening techniques to show that all points on the plane
that is normal to this line are infeasible. Using this algorithm, we provide
the first certifications of disconnected feasible spaces for a variety of OPF
test cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smoothed Online Optimization for Target Tracking: Robust and
  Learning-Augmented Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zeynali, Mahsa Sahebdel, Qingsong Liu, Mohammad Hajiesmaili, Ramesh K. Sitaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 14 pages appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamic Programming Framework for Vehicular Task Offloading with
  Successive Action Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianren Li, Yuncong Hong, Bojie Lv, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, task offloading from vehicles with random velocities is
optimized via a novel dynamic programming framework. Particularly, in a
vehicular network with multiple vehicles and base stations (BSs), computing
tasks of vehicles are offloaded via BSs to an edge server. Due to the random
velocities, the exact locations of vehicles versus time, namely trajectories,
cannot be determined in advance. Hence, instead of deterministic optimization,
the cell association, uplink time, and throughput allocation of multiple
vehicles during a period of task offloading are formulated as a finite-horizon
Markov decision process. In order to derive a low-complexity solution
algorithm, a two-time-scale framework is proposed. The scheduling period is
divided into super slots, each super slot is further divided into a number of
time slots. At the beginning of each super slot, we first obtain a reference
scheduling scheme of cell association, uplink time and throughput allocation
via deterministic optimization, yielding an approximation of the optimal value
function. Within the super slot, the actual scheduling action of each time slot
is determined by making improvement to the approximate value function according
to the system state. Due to the successive improvement framework, a non-trivial
average cost upper bound could be derived. In the simulation, the random
trajectories of vehicles are generated from a high-fidelity traffic simulator.
It is shown that the performance gain of the proposed scheduling framework over
the baselines is significant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Fundamental Trade-Off Between Age of Information and
  Throughput in Unreliable Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.12185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.12185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Wang, I-Hong Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper characterizes the fundamental trade-off between throughput and Age
of Information (AoI) in wireless networks where multiple devices transmit
status updates to a central base station over unreliable channels. To address
the complexity introduced by stochastic transmission successes, we propose the
throughput-AoI capacity region, which defines all feasible throughput-AoI pairs
achievable under any scheduling policy. Using a second-order approximation that
incorporates both mean and temporal variance, we derive an outer bound and a
tight inner bound for the throughput-AoI capacity region. Furthermore, we
propose a simple and low complexity scheduling policy and prove that it
achieves every interior point within the tight inner bound. This establishes a
systematic and theoretically grounded framework for the joint optimization of
throughput and information freshness in practical wireless communication
scenarios.
  To validate our theoretical framework and demonstrate the utility of the
throughput-AoI capacity region, extensive simulations are implemented.
Simulation results demonstrate that our proposed policy significantly
outperforms conventional methods across various practical network optimization
scenarios. The findings highlight our approach's effectiveness in optimizing
both throughput and AoI, underscoring its applicability and robustness in
practical wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built
  from a Five Bar Linkage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Vyas, Matteo Bottin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A force balanced manipulator design based on the closed chain planar five bar
linkage is developed and experimentally validated. We present 2 variants as a
modular design: Forbal-2, a planar 2-DOF manipulator, and its extension to
5-DOF spatial motion called Forbal-5. The design considerations in terms of
geometric, kinematic, and dynamic design that fulfill the force balance
conditions while maximizing workspace are discussed. Then, the inverse
kinematics of both variants are derived from geometric principles. We validate
the improvements from force balancing the manipulator through comparative
experiments with counter mass balanced and unbalanced configurations. The
results show how the balanced configuration yields a reduction in the average
reaction moments of up to 66%, a reduction of average joint torques of up to
79%, as well as a noticeable reduction in position error for Forbal-2. For
Forbal-5, which has a higher end effector payload mass, the joint torques are
reduced up to 84% for the balanced configuration. Experimental results validate
that the balanced manipulator design is suitable for applications where the
reduction of joint torques and reaction forces/moments helps achieve millimeter
level precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Learning of Decision Functions in Multiplayer Games with
  Expectation Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15724v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15724v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhanqing Huang, Jianghai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a class of stochastic multiplayer games where each player in the
game aims to optimize its objective under uncertainty and adheres to some
expectation constraints. The study employs an offline learning paradigm,
leveraging a pre-existing dataset containing auxiliary features. While prior
research in deterministic and stochastic multiplayer games primarily explored
vector-valued decisions, this work departs by considering function-valued
decisions that incorporate auxiliary features as input. We leverage the law of
large deviations and degree theory to establish the almost sure convergence of
the offline learning solution to the true solution as the number of data
samples increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Robust Optimization for Energy-Aware Safe Motion Planning of
  Electric Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simran Kumari, Ashish R. Hota, Siddhartha Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we simultaneously address the problems of energy optimal and
safe motion planning of electric vehicles (EVs) in a data-driven robust
optimization framework. Safe maneuvers, especially in urban traffic, are
characterized by frequent lateral motions, such as lane changes, overtakes and
turning along curved roads. Motivated by our previous work which shows a 3-10 %
increase in energy consumption due to lateral motion when an electric vehicle
changes its lane once every kilometer while following standard drive cycles, we
incorporate vehicle lateral dynamics in the modeling and control synthesis,
which is in contrast with most prior works. In the context of safety, we
leverage past data of obstacle motion to construct a future occupancy set with
probabilistic guarantees, and formulate robust collision avoidance constraints
with respect to such an occupancy set using convex programming duality.
Consequently, we formulate a finite-horizon optimal control problem subject to
robust collision avoidance constraints while penalizing resulting energy
consumption, and solve it in a receding horizon fashion. Finally, we show the
effectiveness of the proposed approach in reducing energy consumption and
collision avoidance via numerical simulations involving curved roads and
multiple obstacles. A detailed analysis of energy consumption along different
components of EV motion highlights appreciable improvement under the proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Direct Pseudospectral Optimal Control by Orthogonal Polynomial Integral
  Collocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas L. Ahrens, Ian M. Down, Manoranjan Majji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper details a methodology to transcribe an optimal control problem
into a nonlinear program for generation of the trajectories that optimize a
given functional by approximating only the highest order derivatives of a given
system's dynamics. The underlying method uses orthogonal polynomial integral
collocation by which successive integrals are taken to approximate all lower
order states. Hence, one set of polynomial coefficients can represent an entire
coordinate's degree of freedom. Specifically, Chebyshev polynomials of the
first and second kind and Legendre polynomials are used over their associated
common interpolating grids derived from the bases' roots and extrema. Simple
example problems compare different polynomial bases' performance to analytical
solutions. The planar circular orbit raising problem is used to verify the
method with solutions obtained by other pseudospectral methods in literature.
Finally, a rocket landing flip maneuver problem is solved to demonstrate the
ability to solve complex problems with multiple states and control variables
with constraints. Simulations establish this method's performance, and reveal
that the polynomial/node choice for a given problem notably affects the
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to the Journal of Guidance, Control, and Dynamics
  for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ontology Neural Network and ORTSF: A Framework for Topological Reasoning
  and Delay-Robust Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19277v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19277v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of autonomous robotic systems has led to impressive
capabilities in perception, localization, mapping, and control. Yet, a
fundamental gap remains: existing frameworks excel at geometric reasoning and
dynamic stability but fall short in representing and preserving relational
semantics, contextual reasoning, and cognitive transparency essential for
collaboration in dynamic, human-centric environments. This paper introduces a
unified architecture comprising the Ontology Neural Network (ONN) and the
Ontological Real-Time Semantic Fabric (ORTSF) to address this gap. The ONN
formalizes relational semantic reasoning as a dynamic topological process. By
embedding Forman-Ricci curvature, persistent homology, and semantic tensor
structures within a unified loss formulation, ONN ensures that relational
integrity and topological coherence are preserved as scenes evolve over time.
The ORTSF transforms reasoning traces into actionable control commands while
compensating for system delays. It integrates predictive and delay-aware
operators that ensure phase margin preservation and continuity of control
signals, even under significant latency conditions. Empirical studies
demonstrate the ONN + ORTSF framework's ability to unify semantic cognition and
robust control, providing a mathematically principled and practically viable
solution for cognitive robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, includes theoretical proofs and simulation
  results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Bandwidth Estimation for Real-Time Communication with Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Kai, Tianwei Zhang, Zihan Ling, Yang Cao, Can Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate bandwidth estimation (BWE) is critical for real-time communication
(RTC) systems. Traditional heuristic approaches offer limited adaptability
under dynamic networks, while online reinforcement learning (RL) suffers from
high exploration costs and potential service disruptions. Offline RL, which
leverages high-quality data collected from real-world environments, offers a
promising alternative. However, challenges such as out-of-distribution (OOD)
actions, policy extraction from behaviorally diverse datasets, and reliable
deployment in production systems remain unsolved. We propose RBWE, a robust
bandwidth estimation framework based on offline RL that integrates Q-ensemble
(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD
risks and enhance policy learning. A fallback mechanism ensures deployment
stability by switching to heuristic methods under high uncertainty.
Experimental results show that RBWE reduces overestimation errors by 18% and
improves the 10th percentile Quality of Experience (QoE) by 18.6%,
demonstrating its practical effectiveness in real-world RTC applications. The
implementation is publicly available at
https://github.com/jiu2021/RBWE_offline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE GLOBECOM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Categorical semantics of compositional reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Bakirtzis, Michail Savvas, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional knowledge representations in reinforcement learning (RL)
facilitate modular, interpretable, and safe task specifications. However,
generating compositional models requires the characterization of minimal
assumptions for the robustness of the compositionality feature, especially in
the case of functional decompositions. Using a categorical point of view, we
develop a knowledge representation framework for a compositional theory of RL.
Our approach relies on the theoretical study of the category MDP, whose objects
are Markov decision processes (MDPs) acting as models of tasks. The categorical
semantics models the compositionality of tasks through the application of
pushout operations akin to combining puzzle pieces. As a practical application
of these pushout operations, we introduce zig-zag diagrams that rely on the
compositional guarantees engendered by the category MDP. We further prove that
properties of the category MDP unify concepts, such as enforcing safety
requirements and exploiting symmetries, generalizing previous abstraction
theories for RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-Optimal Emission-Aware Online Ride Assignment Algorithm for Peak
  Demand Hours 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zeynali, Mahsa Sahebdel, Noman Bashir, Ramesh K. Sitaraman, Mohammad Hajiesmaili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ridesharing has experienced significant global growth over the past decade
and is becoming an integral component of modern transportation systems.
However, despite their benefits, ridesharing platforms face fundamental
inefficiencies that contribute to negative environmental impacts. A prominent
source of such inefficiency is the deadhead miles. This issue becomes
especially severe during high-demand periods, when the volume of ride requests
exceeds the available driver supply, leading to suboptimal rider-to-driver
assignments, longer deadhead trips, and increased emissions. Although limiting
these unproductive miles can reduce emissions, doing so may increase passenger
wait times due to limited driver availability, thereby degrading the overall
service experience. In this paper, we introduce LARA, an online rider-to-driver
assignment algorithm that dynamically adjusts the maximum allowable distance
between rider and drivers and assigns ride requests accordingly. While LARA is
applicable in general settings, it is particularly effective during peak demand
periods, achieving reductions in both emissions and wait times. We provide
theoretical guarantees showing that LARA achieves near-optimal performance in
online environments, with respect to an optimal offline benchmark. Beside our
theoretical analysis, our empirical evaluations on both synthetic and
real-world datasets show that LARA achieves up to a 34% reduction in carbon
emissions and up to a 50% decrease in rider wait times, compared to
state-of-the-art baselines. While prior work has explored emission-aware ride
assignment, LARA is, to our knowledge, the first algorithm to offer both
rigorous theoretical guarantees and strong empirical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.19153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.19153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinuo Wang, Gavin Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages, 9 figures, Journal paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecture <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Template for Approximate Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Rezaalipour, F. Costa, M. Biasion, R. Otoni, G. A. Constantinides, L. Pozzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCREME: A Scalable Framework for Resilient Memory Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Li, Mimi Xie, Yanan Guo, Huize Li, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuing advancement of memory technology has not only fueled a surge
in performance, but also substantially exacerbate reliability challenges.
Traditional solutions have primarily focused on improving the efficiency of
protection schemes, i.e., Error Correction Codes (ECC), under the assumption
that allocating additional memory space for parity data is always expensive and
therefore not a scalable solution.
  We break the stereotype by proposing an orthogonal approach that provides
additional, cost-effective memory space for resilient memory design. In
particular, we recognize that ECC chips (used for parity storage) do not
necessarily require the same performance level as regular data chips. This
offers two-fold benefits: First, the bandwidth originally provisioned for a
regular-performance ECC chip can instead be used to accommodate multiple
low-performance chips. Second, the cost of ECC chips can be effectively
reduced, as lower performance often correlates with lower expense. In addition,
we observe that server-class memory chips are often provisioned with ample, yet
underutilized I/O resources. This further offers the opportunity to repurpose
these resources to enable flexible on-DIMM interconnections. Based on the above
two insights, we finally propose SCREME, a scalable memory framework leverages
cost-effective, albeit slower, chips -- naturally produced during rapid
technology evolution -- to meet the growing reliability demands driven by this
evolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Hsing Huang, Jianwei Jia, Yuyao Kong, Faaiq Waqar, Tai-Hao Wen, Meng-Fan Chang, Shimeng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an
innovative architectural paradigm capable of replicating conventional deep
neural network (DNN) capabilities while utilizing significantly reduced
parameter counts through the employment of parameterized B-spline functions
with trainable coefficients. Nevertheless, the B-spline functional components
inherent to KAN architectures introduce distinct hardware acceleration
complexities. While B-spline function evaluation can be accomplished through
look-up table (LUT) implementations that directly encode functional mappings,
thus minimizing computational overhead, such approaches continue to demand
considerable circuit infrastructure, including LUTs, multiplexers, decoders,
and related components. This work presents an algorithm-hardware co-design
approach for KAN acceleration. At the algorithmic level, techniques include
Alignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity
aware mapping strategy, and circuit-level techniques include N:1 Time
Modulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)
circuits. This work conducts evaluations on large-scale KAN networks to
validate the proposed methodologies. Non-ideality factors, including partial
sum deviations from process variations, have been evaluated with statistics
measured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally
determined KAN hyperparameters in conjunction with circuit optimizations
fabricated at the 22nm technology node, despite the parameter count for
large-scale tasks in this work increasing by 500Kx to 807Kx compared to
tiny-scale tasks in previous work, the area overhead increases by only 28Kx to
41Kx, with power consumption rising by merely 51x to 94x, while accuracy
degradation remains minimal at 0.11% to 0.23%, demonstrating the scaling
potential of our proposed architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chronus: Understanding and Securing the Cutting-Edge Industry Solutions
  to DRAM Read Disturbance <span class="chip">HPCA'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oğuzhan Canpolat, A. Giray Yağlıkçı, Geraldo F. Oliveira, Ataberk Olgun, Nisa Bostancı, İsmail Emir Yüksel, Haocong Luo, Oğuz Ergin, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We 1) present the first rigorous security, performance, energy, and cost
analyses of the state-of-the-art on-DRAM-die read disturbance mitigation
method, Per Row Activation Counting (PRAC) and 2) propose Chronus, a new
mechanism that addresses PRAC's two major weaknesses. Our analysis shows that
PRAC's system performance overhead on benign applications is non-negligible for
modern DRAM chips and prohibitively large for future DRAM chips that are more
vulnerable to read disturbance. We identify two weaknesses of PRAC that cause
these overheads. First, PRAC increases critical DRAM access latency parameters
due to the additional time required to increment activation counters. Second,
PRAC performs a constant number of preventive refreshes at a time, making it
vulnerable to an adversarial access pattern, known as the wave attack, and
consequently requiring it to be configured for significantly smaller activation
thresholds. To address PRAC's two weaknesses, we propose a new on-DRAM-die
RowHammer mitigation mechanism, Chronus. Chronus 1) updates row activation
counters concurrently while serving accesses by separating counters from the
data and 2) prevents the wave attack by dynamically controlling the number of
preventive refreshes performed. Our performance analysis shows that Chronus's
system performance overhead is near-zero for modern DRAM chips and very low for
future DRAM chips. Chronus outperforms three variants of PRAC and three other
state-of-the-art read disturbance solutions. We discuss Chronus's and PRAC's
implications for future systems and foreshadow future research directions. To
aid future research, we open-source our Chronus implementation at
https://github.com/CMU-SAFARI/Chronus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in HPCA'25. arXiv admin note: text overlap with
  arXiv:2406.19094. Appendix E added that describe the errata and new results</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Help Students Prove Software Correctness? An
  Experimental Study with Dafny 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22370v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22370v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carolina Carreira, Álvaro Silva, Alexandre Abreu, Alexandra Mendes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Students in computing education increasingly use large language models (LLMs)
such as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding
tasks, like deductive program verification, remains poorly understood. This
paper investigates how students interact with an LLM when solving formal
verification exercises in Dafny, a language that supports functional
correctness, by allowing programmers to write formal specifications and
automatically verifying that the implementation satisfies the specification. We
conducted a mixed-methods study with master's students enrolled in a formal
methods course. Each participant completed two verification problems, one with
access to a custom ChatGPT interface that logged all interactions, and the
other without. We identified strategies used by successful students and
assessed the level of trust students place in LLMs. Our findings show that
students perform significantly better when using ChatGPT; however, performance
gains are tied to prompt quality. We conclude with practical recommendations
for integrating LLMs into formal methods courses more effectively, including
designing LLM-aware challenges that promote learning rather than substitution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Challenges Do Developers Face When Using Verification-Aware
  Programming Languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Oliveira, Alexandra Mendes, Carolina Carreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software reliability is critical in ensuring that the digital systems we
depend on function correctly. In software development, increasing software
reliability often involves testing. However, for complex and critical systems,
developers can use Design by Contract (DbC) methods to define precise
specifications that software components must satisfy. Verification-Aware (VA)
programming languages support DbC and formal verification at compile-time or
run-time, offering stronger correctness guarantees than traditional testing.
However, despite the strong guarantees provided by VA languages, their adoption
remains limited. In this study, we investigate the barriers to adopting VA
languages by analyzing developer discussions on public forums using topic
modeling techniques. We complement this analysis with a developer survey to
better understand the practical challenges associated with VA languages. Our
findings reveal key obstacles to adoption, including steep learning curves and
usability issues. Based on these insights, we identify actionable
recommendations to improve the usability and accessibility of VA languages. Our
findings suggest that simplifying tool interfaces, providing better educational
materials, and improving integration with everyday development environments
could improve the usability and adoption of these languages. Our work provides
actionable insights for improving the usability of VA languages and making
verification tools more accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision Procedure for A Theory of String Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denghang Hu, Taolue Chen, Philipp Rümmer, Fu Song, Zhilin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 2 tables, APLAS 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Data Structures and Algorithms <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Catalytic Graph Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Cook, Edward Pyne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give fast, simple, and implementable catalytic logspace algorithms for two
fundamental graph problems.
  First, a randomized catalytic algorithm for $s\to t$ connectivity running in
$\widetilde{O}(nm)$ time, and a deterministic catalytic algorithm for the same
running in $\widetilde{O}(n^3 m)$ time. The former algorithm is the first
algorithmic use of randomization in $\mathsf{CL}$. The algorithm uses one
register per vertex and repeatedly ``pushes'' values along the edges in the
graph.
  Second, a deterministic catalytic algorithm for simulating random walks which
in $\widetilde{O}( m T^2 / \varepsilon )$ time estimates the probability a
$T$-step random walk ends at a given vertex within $\varepsilon$ additive
error. The algorithm uses one register for each vertex and increments it at
each visit to ensure repeated visits follow different outgoing edges.
  Prior catalytic algorithms for both problems did not have explicit runtime
bounds beyond being polynomial in $n$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degree Realization by Bipartite Cactus Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amotz Bar-Noy, Toni Bohnlein, David Peleg, Yingli Ran, Dror Rawitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The \textsc{Degree Realization} problem with respect to a graph family
$\mathcal{F}$ is defined as follows. The input is a sequence $d$ of $n$
positive integers, and the goal is to decide whether there exists a graph $G
\in \mathcal{F}$ whose degrees correspond to $d$. The main challenges are to
provide a precise characterization of all the sequences that admit a
realization in $\mathcal{F}$ and to design efficient algorithms that construct
one of the possible realizations, if one exists.
  This paper studies the problem of realizing degree sequences by bipartite
cactus graphs (where the input is given as a single sequence, without the
bi-partition). A characterization of the sequences that have a cactus
realization is already known [28]. In this paper, we provide a systematic way
to obtain such a characterization, accompanied by a realization algorithm. This
allows us to derive a characterization for bipartite cactus graphs, and as a
byproduct, also for several other interesting sub-families of cactus graphs,
including bridge-less cactus graphs and core cactus graphs, as well as for the
bipartite sub-families of these families.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separable convex optimization over indegree polytopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nóra A. Borsik, Péter Madarasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study egalitarian (acyclic) orientations of undirected graphs under
indegree-based objectives, such as minimizing the $\varphi$-sum of indegrees
for a strictly convex function $\varphi$, decreasing minimization (dec-min),
and increasing maximization (inc-max). In the non-acyclic setting of Frank and
Murota (2022), a single orientation simultaneously optimizes these three
objectives, however, restricting to acyclic orientations confines us to the
corners of the indegree polytope, where these fairness objectives do diverge.
We establish strong hardness results across a broad range of settings:
minimizing the $\varphi$-sum of indegrees is NP-hard for every discrete
strictly convex function $\varphi$; dec-min and inc-max are NP-hard for every
indegree bound $k \geq 2$, as well as without a bound; and the complementary
inc-min and dec-max problems are NP-hard even on $3$-regular graphs. On the
algorithmic side, we give a polynomial-time algorithm for minimizing the
maximum weighted indegree via a weighted smallest-last ordering. We also
provide an exact exponential-time algorithm for minimizing general separable
discrete convex objectives over indegrees, and a polynomial-time algorithm for
the non-acyclic case. Finally, for maximizing the sum of the products of
indegrees and outdegrees, we prove NP-hardness on graphs of maximum degree $4$,
give an algorithm for maximum degree $3$, and provide a $3$-approximation
algorithm. Our results delineate the algorithmic frontier of convex integral
optimization over indegree (base-)polytopes, and highlight both theoretical
consequences and practical implications, notably for scheduling and
deadlock-free routing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameterized Algorithms for Computing Pareto Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Könen, Heiko Röglin, Tarek Stuck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic programming over tree decompositions is a common technique in
parameterized algorithms. In this paper, we study whether this technique can
also be applied to compute Pareto sets of multiobjective optimization problems.
We first derive an algorithm to compute the Pareto set for the multicriteria
s-t cut problem and show how this result can be applied to a polygon
aggregation problem arising in cartography that has recently been introduced by
Rottmann et al. (GIScience 2021). We also show how to apply these techniques to
also compute the Pareto set of the multiobjective minimum spanning tree problem
and for the multiobjective TSP. The running time of our algorithms is
$O(f(w)\cdot\mathrm{poly}(n,p_{\text{max}}))$, where $f$ is some function in
the treewidth $w$, $n$ is the input size, and $p_{\text{max}}$ is an upper
bound on the size of the Pareto sets of the subproblems that occur in the
dynamic program. Finally, we present an experimental evaluation of computing
Pareto sets on real-world instances of polygon aggregation problems. For this
matter we devised a task-specific data structure that allows for efficient
storage and modification of large sets of Pareto-optimal solutions. Throughout
the implementation process, we incorporated several improved strategies and
heuristics that significantly reduced both runtime and memory usage, enabling
us to solve instances with treewidth of up to 22 within reasonable amount of
time. Moreover, we conducted a preprocessing study to compare different tree
decompositions in terms of their estimated overall runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Graph Packing Problems Parameterized by Treewidth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barış Can Esmer, Dániel Marx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $H$-Packing is the problem of finding a maximum number of vertex-disjoint
copies of $H$ in a given graph $G$. $H$-Partition is the special case of
finding a set of vertex-disjoint copies that cover each vertex of $G$ exactly
once. Our goal is to study these problems and some generalizations on
bounded-treewidth graphs. The case of $H$ being a triangle is well understood:
given a tree decomposition of $G$ having treewidth $tw$, the $K_3$-Packing
problem can be solved in time $2^{tw} \cdot n^{O(1)}$, while Lokshtanov et
al.~[{\it ACM Transactions on Algorithms} 2018] showed, under the Strong
Exponential-Time Hypothesis (SETH), that there is no $(2-\epsilon)^{tw}\cdot
n^{O(1)}$ algorithm for any $\epsilon>0$ even for $K_3$-Partition. Similar
results can be obtained for any other clique $K_d$ for $d\ge 3$. We provide
generalizations in two directions:
  - We consider a generalization of the problem where every vertex can be used
at most $c$ times for some $c\ge 1$. When $H$ is any clique $K_d$ with $d\ge
3$, then we give upper and lower bounds showing that the optimal running time
increases to $(c+1)^{tw}\cdot n^{O(1)}$. We consider two variants depending on
whether a copy of $H$ can be used multiple times in the packing.
  - If $H$ is not a clique, then the dependence of the running time on
treewidth may not be even single exponential. Specifically, we show that if $H$
is any fixed graph where not every 2-connected component is a clique, then
there is no $2^{o({tw}\log {tw})}\cdot n^{O(1)}$ algorithm for
\textsc{$H$-Partition}, assuming the Exponential-Time Hypothesis (ETH).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across
  Thousands of Computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Framework for Low Soundness Homomorphism Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushant Mittal, Sourya Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework to design and analyze algorithms for the
problem of testing homomorphisms between finite groups in the low-soundness
regime.
  In this regime, we give the first constant-query tests for various families
of groups. These include tests for: (i) homomorphisms between arbitrary cyclic
groups, (ii) homomorphisms between any finite group and $\mathbb{Z}_p$, (iii)
automorphisms of dihedral and symmetric groups, (iv) inner automorphisms of
non-abelian finite simple groups and extraspecial groups, and (v) testing
linear characters of $\mathrm{GL}_n(\mathbb{F}_q)$, and finite-dimensional Lie
algebras over $\mathbb{F}_q$. We also recover the result of Kiwi [TCS'03] for
testing homomorphisms between $\mathbb{F}_q^n$ and $\mathbb{F}_q$.
  Prior to this work, such tests were only known for abelian groups with a
constant maximal order (such as $\mathbb{F}_q^n$). No tests were known for
non-abelian groups.
  As an additional corollary, our framework gives combinatorial list decoding
bounds for cyclic groups with list size dependence of $O(\varepsilon^{-2})$
(for agreement parameter $\varepsilon$). This improves upon the currently
best-known bound of $O(\varepsilon^{-105})$ due to Dinur, Grigorescu, Kopparty,
and Sudan [STOC'08], and Guo and Sudan [RANDOM'14].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Approximate Maximum Matching in the Distributed Vertex Partition
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Robinson, Xianbin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of approximate maximum matching in the vertex partition
model, for graphs subject to dynamic changes. We assume that the $n$ vertices
of the graph are partitioned among $k$ players, who execute a distributed
algorithm and communicate via message passing. An adaptive adversary may
perform dynamic updates to the graph topology by inserting or removing edges
between the nodes, and the algorithm needs to respond to these changes by
adapting the output of the players, with the goal of maintaining an approximate
maximum matching. The main performance metric in this setting is the
algorithm's update time, which corresponds to the number of rounds required for
updating the solution upon an adversarial change. For the standard setting of
single-edge insertions and deletions, we obtain the following results:
  We give a randomized Las Vegas algorithm with an expected update time of $O(
\frac{\sqrt{m}}{\beta k} )$ rounds that maintains a $\frac{2}{3}$-approximate
maximum matching that is also maximal, where $m$ is the number of edges of the
graph. We also show that any algorithm has a worst case update time of $\Omega(
\frac{n}{\beta k^2\log n} )$, assuming a link bandwidth of $O(\beta\log n)$
bits per round, if it maintains a matching that is maximal and does not have
any 3-augmenting paths. For batch-dynamic updates, where the adversary may
modify up to $\ell\ge 1$ edges at once, we prove the following: There is a
randomized algorithm that succeeds with high probability in maintaining a
$\frac{2}{3}$-approximate maximum matching and has a worst case update time of
$\Omega( \frac{\ell\log n}{\sqrt{\beta k}} )$ rounds. We show that $\Omega(
\frac{\ell}{\beta k \log n} )$ poses a lower bound for maintaining a maximal
matching without 3-augmenting paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strong Linearizability without Compare&Swap: The Case of Bags 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faith Ellen, Gal Sela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Because strongly-linearizable objects provide stronger guarantees than
linearizability, they serve as valuable building blocks for the design of
concurrent data structures. Yet, many objects that have linearizable
implementations from base objects weaker than compare&swap objects do not have
strongly-linearizable implementations from the same base objects. We focus on
one such object: the bag, a multiset from which processes can take unspecified
elements.
  We present the first lock-free, strongly-linearizable implementation of a bag
from interfering objects (specifically, registers, and test&set objects). This
may be surprising, since there are provably no such implementations of stacks
or queues.
  Since a bag can contain arbitrarily many elements, an unbounded amount of
space must be used to implement it. Hence, it makes sense to also consider a
bag with a bound on its capacity. However, like stacks and queues, a bag with
capacity $b$ shared by more than $2b$ processes has no lock-free,
strongly-linearizable implementation from interfering objects. If we further
restrict a bounded bag so that only one process can insert into it, we are able
to obtain a lock-free, strongly-linearizable implementation from $O(b + n)$
interfering objects, where $n$ is the number of processes.
  Our goal is to understand the circumstances under which strongly-linearizable
implementations of bags exist and, more generally, to understand the power of
interfering objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Modular Linear Systems with a Constraint by parallel
  decomposition of the Smith form and extended Euclidean division modulo powers
  of primes divisors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10158v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10158v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virendra Sule
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integral linear systems $Ax=b$ with matrices $A$, $b$ and solutions $x$ are
also required to be in integers, can be solved using invariant factors of $A$
(by computing the Smith Canonical Form of $A$). This paper explores a new
problem which arises in applications, that of obtaining conditions for solving
the Modular Linear System $Ax=b\rem n$ given $A,b$ in $\zz_n$ for $x$ in
$\zz_n$ along with the constraint that the value of the linear function
$\phi(x)=\la w,x\ra$ is coprime to $n$ for some solution $x$. In this paper we
develop decomposition of the system to coprime moduli $p^{r(p)}$ which are
divisors of $n$ and show how such a decomposition simplifies the computation of
Smith form. This extends the well known index calculus method of computing the
discrete logarithm where the moduli over which the linear system is reduced
were assumed to be prime (to solve the reduced systems over prime fields) to
the case when the factors of the modulus are prime powers $p^{r(p)}$. It is
shown how this problem can be addressed effciently using the invariant factors
and Smith form of the augmented matrix $[A,-p^{r(p)}I]$ and conditions modulo
$p$ satisfied by $w$, where $p^{r(p)}$ vary over all divisors of $n$ with $p$
prime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grassroots Consensus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19216v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19216v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idit Keidar, Andrew Lewis-Pye, Ehud Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider people with smartphones operating without external authorities or
global resources other than the network itself. In this setting, high-end
applications supporting sovereign democratic digital communities, community
banks, and digital cooperatives require consensus executed by community
members, which must be reconfigurable to support community dynamics.
  The Constitutional Consensus protocol aims to address this need by
introducing constitutional self-governance to consensus: participants
dynamically amend the participant set, supermajority threshold, and timeout
parameter through the consensus protocol itself. We achieve this by enhancing a
DAG-based protocol (like Cordial Miners) with participant-controlled
reconfiguration, while also supporting both high- and low-throughput operation
(like Morpheus), remaining quiescent when idle. This three-way synthesis
uniquely combines: (1) constitutional amendments for self-governance, (2) a
cryptographic DAG structure for simplicity, parallelism, and throughput, and
(3) both high- and low-throughput operation. The protocol achieves consensus in
$3\delta$, maintains O(n) amortized communication complexity during high
throughput, and seamlessly transitions between modes. The basic protocol
(without constitutional amendments) realizes these features in 25 lines of
pseudocode, making it one of the most concise consensus protocols for eventual
synchrony.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distance Estimation for High-Dimensional Discrete Distributions <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gunjan Kumar, Kuldeep S. Meel, Yash Pote
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given two distributions $\mathcal{P}$ and $\mathcal{Q}$ over a
high-dimensional domain $\{0,1\}^n$, and a parameter $\varepsilon$, the goal of
distance estimation is to determine the statistical distance between
$\mathcal{P}$ and $\mathcal{Q}$, up to an additive tolerance $\pm \varepsilon$.
Since exponential lower bounds (in $n$) are known for the problem in the
standard sampling model, research has focused on richer query models where one
can draw conditional samples. This paper presents the first polynomial query
distance estimator in the conditional sampling model ($\mathsf{COND}$).
  We base our algorithm on the relatively weaker \textit{subcube conditional}
sampling ($\mathsf{SUBCOND}$) oracle, which draws samples from the distribution
conditioned on some of the dimensions. $\mathsf{SUBCOND}$ is a promising model
for widespread practical use because it captures the natural behavior of
discrete samplers. Our algorithm makes $\tilde{\mathcal{O}}(n^3/\varepsilon^5)$
queries to $\mathsf{SUBCOND}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work appears at AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Parallel Complexity of Group Isomorphism via Weisfeiler-Leman 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11487v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11487v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua A. Grochow, Michael Levet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we show that the constant-dimensional Weisfeiler-Leman
algorithm for groups (Brachter & Schweitzer, LICS 2020) can be fruitfully used
to improve parallel complexity upper bounds on isomorphism testing for several
families of groups. In particular, we show:
  - Groups with an Abelian normal Hall subgroup whose complement is
$O(1)$-generated are identified by constant-dimensional Weisfeiler-Leman using
only a constant number of rounds. This places isomorphism testing for this
family of groups into $\textsf{L}$; the previous upper bound for isomorphism
testing was $\textsf{P}$ (Qiao, Sarma, & Tang, STACS 2011).
  - We use the individualize-and-refine paradigm to obtain an isomorphism test
for groups without Abelian normal subgroups by $\textsf{SAC}$ circuits of depth
$O(\log n)$ and size $n^{O(\log \log n)}$, previously only known to be in
$\textsf{P}$ (Babai, Codenotti, \& Qiao, ICALP 2012) and $\mathsf{quasiSAC}^1$
(Chattopadhyay, Tor\'an, \& Wagner, ACM Trans. Comput. Theory, 2013).
  - We extend a result of Brachter \& Schweitzer (ESA, 2022) on direct products
of groups to the parallel setting. Namely, we also show that Weisfeiler--Leman
can identify direct products in parallel, provided it can identify each of the
indecomposable direct factors in parallel. They previously showed the analogous
result for $\textsf{P}$.
  We finally consider the count-free Weisfeiler--Leman algorithm, where we show
that count-free WL is unable to even distinguish Abelian groups in
polynomial-time. Nonetheless, we use count-free WL in tandem with bounded
non-determinism and limited counting to obtain a new upper bound of
$\beta_{1}\textsf{MAC}^{0}(\textsf{FOLL})$ for isomorphism testing of Abelian
groups. This improves upon the previous $\textsf{TC}^{0}(\textsf{FOLL})$ upper
bound due to Chattopadhyay, Tor\'an, \& Wagner (ibid.).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preliminary version appeared in the proceedings of FCT23. The final
  journal version has been accepted to the Journal of Computer and System
  Sciences</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Urban Factors with Autoencoders: Relationship Between Static
  and Dynamic Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.06167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.06167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximena Pocco, Waqar Hassan, Karelia Salinas, Vladimir Molchanov, Luis G. Nonato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-06T00:00:00Z">2025-09-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Systems and Control <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Genesis: A Spiking Neuromorphic Accelerator With On-chip Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedant Karia, Abdullah Zyarah, Dhireesha Kudithipudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning, the ability to acquire and transfer knowledge through a
models lifetime, is critical for artificial agents that interact in real-world
environments. Biological brains inherently demonstrate these capabilities while
operating within limited energy and resource budgets. Achieving continual
learning capability in artificial systems considerably increases memory and
computational demands, and even more so when deploying on platforms with
limited resources. In this work, Genesis, a spiking continual learning
accelerator, is proposed to address this gap. The architecture supports
neurally inspired mechanisms, such as activity-dependent metaplasticity, to
alleviate catastrophic forgetting. It integrates low-precision continual
learning parametersand employs a custom data movement strategy to accommodate
the sparsely distributed spikes. Furthermore, the architecture features a
memory mapping technique that places metaplasticity parameters and synaptic
weights in a single address location for faster memory access. Results show
that the mean classification accuracy for Genesis is 74.6% on a task-agnostic
split-MNIST benchmark with power consumption of 17.08mW in a 65nm technology
node.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Single-Iteration Model Predictive Control for Wave Energy
  Converters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Pirrera, Nicolas Faedo, Sophie M. Fosson, Diego Regruto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel real-time algorithm for controlling wave energy
converters (WECs). We begin with the economic model predictive control (MPC)
problem formulation and apply a novel, first-order optimization algorithm
inspired by recently developed control-based algorithms for constrained
optimization to define the controller dynamics according to the
single-iteration MPC approach. We theoretically analyse the convergence of the
employed algorithm and the computational complexity of the obtained controller.
Results from simulations using a benchmark WEC system indicate that the
proposed approach significantly outperforms standard MPC, thanks to the
inherent ability to handle faster sampling rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Decision-Making in Population Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Wen Chen, Nuno C. Martins, Murat Arcak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a hierarchical framework for population games, where
individuals delegate decision-making to proxies that act within their own
strategic interests. This framework extends classical population games, where
individuals are assumed to make decisions directly, to capture various
real-world scenarios involving multiple decision layers. We establish
equilibrium properties and provide convergence results for the proposed
hierarchical structure. Additionally, based on these results, we develop a
systematic approach to analyze population games with general convex
constraints, without requiring individuals to have full knowledge of the
constraints as in existing methods. We present a navigation application with
capacity constraints as a case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically
  Constrained Humanoids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arturo Flores Alvarez, Fatemeh Zargarbashi, Havel Liu, Shiqi Wang, Liam Edwards, Jessica Anz, Alex Xu, Fan Shi, Stelian Coros, Dennis W. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a
custom-built humanoid robot designed for entertainment applications. Unlike
traditional humanoids, entertainment robots present unique challenges due to
aesthetic-driven design choices. Cosmo embodies these with a disproportionately
large head (16% of total mass), limited sensing, and protective shells that
considerably restrict movement. To address these challenges, we apply
Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking
movements while maintaining physical stability. We develop tailored domain
randomization techniques and specialized reward structures to ensure safe
sim-to-real, protecting valuable hardware components during deployment. Our
experiments demonstrate that AMP generates stable standing and walking
behaviors despite Cosmo's extreme mass distribution and movement constraints.
These results establish a promising direction for robots that balance aesthetic
appeal with functional performance, suggesting that learning-based methods can
effectively adapt to aesthetic-driven design constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, accepted at IEEE-RAS International Conference on
  Humanoid Robots (Humanoids) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Concept of the Psyche 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Kolonin, Vladimir Kryukov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The article provides an overview of approaches to modeling the human psyche
in the perspective of building an artificial one. Based on the review, a
concept of cognitive architecture is proposed, where the psyche is considered
as an operating system of a living or artificial subject, including a space of
needs that determines its life meanings in connection with stimuli from the
external world, and intelligence as a decision-making system for actions in
relation to this world in order to satisfy these needs. Based on the concept, a
computational formalization is proposed for creating artificial intelligence
systems through learning from experience in the space of a space of needs,
taking into account their biological or existential significance for an
intelligent agent. Thus, the problem of building general artificial
intelligence as a system for making optimal decisions in the space of
agent-specific needs under conditions of uncertainty is formalized, with
maximization of success in achieving goals, minimization of existential risks
and maximization of energy efficiency. A minimal experimental implementation of
the model is also provided.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, in Russian, 2 figures, submitted to Neuroinformatics-2025
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Port-Hamiltonian Differential Algebraic Equations for
  Compositional Learning of Electrical Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11215v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11215v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cyrus Neary, Nathan Tsao, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop compositional learning algorithms for coupled dynamical systems,
with a particular focus on electrical networks. While deep learning has proven
effective at modeling complex relationships from data, compositional couplings
between system components typically introduce algebraic constraints on state
variables, posing challenges to many existing data-driven approaches to
modeling dynamical systems. Towards developing deep learning models for
constrained dynamical systems, we introduce neural port-Hamiltonian
differential algebraic equations (N-PHDAEs), which use neural networks to
parameterize unknown terms in both the differential and algebraic components of
a port-Hamiltonian DAE. To train these models, we propose an algorithm that
uses automatic differentiation to perform index reduction, automatically
transforming the neural DAE into an equivalent system of neural ordinary
differential equations (N-ODEs), for which established model inference and
backpropagation methods exist. Experiments simulating the dynamics of nonlinear
circuits exemplify the benefits of our approach: the proposed N-PHDAE model
achieves an order of magnitude improvement in prediction accuracy and
constraint satisfaction when compared to a baseline N-ODE over long prediction
time horizons. We also validate the compositional capabilities of our approach
through experiments on a simulated DC microgrid: we train individual N-PHDAE
models for separate grid components, before coupling them to accurately predict
the behavior of larger-scale networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A QoS Framework for Service Provision in Multi-Infrastructure-Sharing
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang Minh Nguyen, Eytan Modiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a framework for resource provisioning with QoS guarantees in
shared infrastructure networks. Our novel framework provides tunable
probabilistic service guarantees for throughput and delay. Key to our approach
is a Modified Dirft-plus-Penalty (MDP) policy that ensures long-term stability
while capturing short-term probabilistic service guarantees using linearized
upper-confidence bounds. We characterize the feasible region of service
guarantees and show that our MDP procedure achieves mean rate stability and an
optimality gap that vanishes with the frame size over which service guarantees
are provided. Finally, empirical simulations validate our theory and
demonstrate the favorable performance of our algorithm in handling QoS in
multi-infrastructure networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MobiHoc '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery
  Systems with Data-Driven Formal Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudi Coppola, Hovsep Touloujian, Pierfrancesco Ombrini, Manuel Mazo Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of
modern technology. In the last decades, the production and design of such
batteries and their adjacent embedded charging and safety protocols, denoted by
Battery Management Systems (BMS), has taken central stage. A fundamental
challenge to be addressed is the trade-off between the speed of charging and
the ageing behavior, resulting in the loss of capacity in the battery cell. We
rely on a high-fidelity physics-based battery model and propose an approach to
data-driven charging and safety protocol design. Following a
Counterexample-Guided Inductive Synthesis scheme, we combine Reinforcement
Learning (RL) with recent developments in data-driven formal methods to obtain
a hybrid control strategy: RL is used to synthesise the individual controllers,
and a data-driven abstraction guides their partitioning into a switched
structure, depending on the initial output measurements of the battery. The
resulting discrete selection among RL-based controllers, coupled with the
continuous battery dynamics, realises a hybrid system. When a design meets the
desired criteria, the abstraction provides probabilistic guarantees on the
closed-loop performance of the cell.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Ground Cost for Optimal Transport of Angular Velocity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Elamvazhuthi, Abhishek Halder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the optimal transport problem over angular velocity dynamics given
by the controlled Euler equation. The solution of this problem enables
stochastic guidance of spin states of a rigid body (e.g., spacecraft) over a
hard deadline constraint by transferring a given initial state statistics to a
desired terminal state statistics. This is an instance of generalized optimal
transport over a nonlinear dynamical system. While prior work has reported
existence-uniqueness and numerical solution of this dynamical optimal transport
problem, here we present structural results about the equivalent Kantorovich
a.k.a. optimal coupling formulation. Specifically, we focus on deriving the
ground cost for the associated Kantorovich optimal coupling formulation. The
ground cost is equal to the cost of transporting unit amount of mass from a
specific realization of the initial or source joint probability measure to a
realization of the terminal or target joint probability measure, and determines
the Kantorovich formulation. Finding the ground cost leads to solving a
structured deterministic nonlinear optimal control problem, which is shown to
be amenable to an analysis technique pioneered by Athans et al. We show that
such techniques have broader applicability in determining the ground cost (thus
Kantorovich formulation) for a class of generalized optimal mass transport
problems involving nonlinear dynamics with translated norm-invariant drift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability of Polling Systems for a Large Class of Markovian Switching
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Avrachenkov, Kousik Das, Veeraruna Kavitha, Vartika Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a polling system with two queues, where a single server is
attending the queues in a cyclic order and requires non-zero switching times to
switch between the queues. Our aim is to identify a fairly general and
comprehensive class of Markovian switching policies that renders the system
stable. Potentially a class of policies that can cover the Pareto frontier
related to individual-queue-centric performance measures like the stationary
expected number of waiting customers in each queue; for instance, such a class
of policies is identified recently for a polling system near the fluid regime
(with large arrival and departure rates), and we aim to include that class. We
also aim to include a second class that facilitates switching between the
queues at the instance the occupancy in the opposite queue crosses a threshold
and when that in the visiting queue is below a threshold (this inclusion
facilitates design of `robust' polling systems). Towards this, we consider a
class of two-phase switching policies, which includes the above mentioned
classes. In the maximum generality, our policies can be represented by eight
parameters, while two parameters are sufficient to represent the aforementioned
classes. We provide simple conditions to identify the sub-class of switching
policies that ensure system stability. By numerically tuning the parameters of
the proposed class, we illustrate that the proposed class can cover the Pareto
frontier for the stationary expected number of customers in the two queues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Iridescent" Reflective Tags to Enable Radar-based Orientation
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onel L. A. López, Zhu Han, Ashutosh Sabharwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate orientation estimation of objects can aid in scene understanding in
many applications. In this paper, we consider use cases where passive tags
could be deployed to assist radar systems in estimating object orientation.
Towards that end, we propose the concept of passive iridescent reflective tags
that selectively reflect different wavelengths in different directions. We
propose a conceptual tag design based on leaky-wave antennas. We develop a
framework for signal modeling and orientation estimation with a multi-tone
radar. We analyze the impact of imperfect tag location information, revealing
that it minimally impacts orientation estimation accuracy. To reduce estimator
complexity, we propose a radiation pointing angle-based estimator with
near-optimal performance. We derive its feasible orientation estimation region
and show that it depends mainly on the system bandwidth. Monte Carlo
simulations validate our analytical results while evincing that the
low-complexity estimator achieves near-optimal accuracy and that its feasible
orientation estimation region closely matches that of the other estimators.
Finally, we show that the optimal number of tones increases with the sensing
time under a power budget constraint, multipath effects may be negligible,
signal-to-noise ratio gains rise with the number of tones, and many radar
antennas can hurt estimation performance when the signal contains very few
tones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 tables, 11 figs. Accepted as IEEE JSAC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hypergraph Approach to Distributed Broadcast 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Cao, Yulin Shao, Fan Yang, Octavia A. Dobre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the distributed broadcast problem within the context of
network communications, a critical challenge in decentralized information
dissemination. We put forth a novel hypergraph-based approach to address this
issue, focusing on minimizing the number of broadcasts to ensure comprehensive
data sharing among all network users. The key contributions of this work
include the establishment of a general lower bound for the problem using the
min-cut capacity of hypergraphs, and a distributed broadcast for quasi-trees
(DBQT) algorithm tailored for the unique structure of quasi-trees, which is
proven to be optimal. This paper advances both network communication strategies
and hypergraph theory, with implications for a wide range of real-world
applications, from vehicular and sensor networks to distributed storage
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Z Transform Laplace Inversion: To Correct flaws in Signal and
  System Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Yang, Hang Zhou, Chaojie Li, Xin Li, Yingyi Yan, Mingyang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the classical formulation of the Z-transform and its
relationship to the inverse Laplace transform (L-1), originally developed by
Ragazzini in sampled-data theory. It identifies a longstanding mathematical
oversight in standard derivations, which typically neglect the contribution
from the infinite arc in the complex plane during inverse Laplace evaluation.
This omission leads to inconsistencies, especially at discontinuities such as t
= 0. By incorporating the full Bromwich contour, including all boundary
contributions, we restore internal consistency between L-1 and the Z-transform,
aligning the corrected L-1 with results from Discrete-Time Fourier Transform
(DTFT) aliasing theory. Consequently, this necessitates a structural revision
of the Z-transform, inverse Laplace transform, and the behavior of the
Heaviside step function at discontinuities, providing a more accurate
foundation for modeling and analysis of sampled-data systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is to be submitted to IEEE transactions on automatic
  control This is revision2 of the manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Feedback Optimization with Model Uncertainty: A Regularization
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.24151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.24151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Winnie Chan, Zhiyu He, Keith Moffat, Saverio Bolognani, Michael Muehlebach, Florian Dörfler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feedback optimization optimizes the steady state of a dynamical system by
implementing optimization iterations in closed loop with the plant. It relies
on online measurements and limited model information, namely, the input-output
sensitivity. In practice, various issues including inaccurate modeling, lack of
observation, or changing conditions can lead to sensitivity mismatches, causing
closed-loop sub-optimality or even instability. To handle such uncertainties,
we pursue robust feedback optimization, where we optimize the closed-loop
performance against all possible sensitivities lying in specific uncertainty
sets. We provide tractable reformulations for the corresponding min-max
problems via regularizations and characterize the online closed-loop
performance through the tracking error in case of time-varying optimal
solutions. Simulations on a distribution grid illustrate the effectiveness of
our robust feedback optimization controller in addressing sensitivity
mismatches in a non-stationary environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proc. 64th IEEE Conference on Decision and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComplexVCoder: An LLM-Driven Framework for Systematic Generation of
  Complex Verilog Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Zuo, Junzhe Liu, Xianyong Wang, Yicheng Liu, Navya Goli, Tong Xu, Hao Zhang, Umamaheswara Rao Tida, Zhenge Jia, Mengying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have demonstrated the promising capabilities of large
language models (LLMs) in generating register-transfer level (RTL) code, such
as Verilog. However, existing LLM-based frameworks still face significant
challenges in accurately handling the complexity of real-world RTL designs,
particularly those that are large-scale and involve multi-level module
instantiations. To address this issue, we present ComplexVCoder, an open-source
LLM-driven framework that enhances both the generation quality and efficiency
of complex Verilog code. Specifically, we introduce a two-stage generation
mechanism, which leverages an intermediate representation to enable a more
accurate and structured transition from natural language descriptions to
intricate Verilog designs. In addition, we introduce a rule-based alignment
method and a domain-specific retrieval-augmented generation (RAG) to further
improve the correctness of the synthesized code by incorporating relevant
design knowledge during generation. To evaluate our approach, we construct a
comprehensive dataset comprising 55 complex Verilog designs derived from
real-world implementations. We also release an open-source benchmark suite for
systematically assessing the quality of auto-generated RTL code together with
the ComplexVCoder framework. Experimental results show that ComplexVCoder
outperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%,
respectively, in terms of function correctness on complex Verilog benchmarks.
Furthermore, ComplexVcoder achieves comparable generation performances in terms
of functionality correctness using a lightweight 32B model (Qwen2.5), rivaling
larger-scale models such as GPT-3.5 and DeepSeek-V3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Withdrawn due to an error in the experimental setup that affected the
  results. A corrected version is in progress</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecture <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Utilization Energy-Aware Real-Time Inference Deep Convolutional
  Neural Network Accelerator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Ting Lin, Ching-Te Chiu, Jheng-Yi Chang, Shi-Zong Huang, Yu-Ting Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolution Neural Network (DCNN) has been widely used in computer
vision tasks. However, for edge devices even inference has too large
computational complexity and data access amount. The inference latency of
state-of-the-art models are impractical for real-world applications. In this
paper, we propose a high utilization energy-aware real-time inference deep
convolutional neural network accelerator, which improves the performance of the
current accelerators. First, we use the 1x1 size convolution kernel as the
smallest unit of the computing unit. Then we design suitable computing unit
based on the requirements of each model. Secondly, we use Reuse Feature SRAM to
store the output of the current layer in the chip and use the value as the
input of the next layer. Moreover, we import Output Reuse Strategy and Ring
Stream Dataflow to reduce the amount of data exchange between chips and DRAM.
Finally, we present On-fly Pooling Module to let the calculation of the Pooling
layer directly complete in the chip. With the aid of the proposed method, the
implemented acceleration chip has an extremely high hardware utilization rate.
We reduce a generous amount of data transfer on the specific module, ECNN.
Compared to the methods without reuse strategy, we can reduce 533 times of data
access amount. At the same time, we have enough computing power to perform
real-time execution of the existing image classification model, VGG16 and
MobileNet. Compared with the design in VWA, we can speed up 7.52 times and have
1.92x energy efficiency
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlimited Vector Processing for Wireless Baseband Based on RISC-V
  Extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Limin Jiang, Yi Shi, Yihao Shen, Shan Cao, Zhiyuan Jiang, Sheng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless baseband processing (WBP) serves as an ideal scenario for utilizing
vector processing, which excels in managing data-parallel operations due to its
parallel structure. However, conventional vector architectures face certain
constraints such as limited vector register sizes, reliance on power-of-two
vector length multipliers, and vector permutation capabilities tied to specific
architectures. To address these challenges, we have introduced an instruction
set extension (ISE) based on RISC-V known as unlimited vector processing (UVP).
This extension enhances both the flexibility and efficiency of vector
computations. UVP employs a novel programming model that supports
non-power-of-two register groupings and hardware strip-mining, thus enabling
smooth handling of vectors of varying lengths while reducing the software
strip-mining burden. Vector instructions are categorized into symmetric and
asymmetric classes, complemented by specialized load/store strategies to
optimize execution. Moreover, we present a hardware implementation of UVP
featuring sophisticated hazard detection mechanisms, optimized pipelines for
symmetric tasks such as fixed-point multiplication and division, and a robust
permutation engine for effective asymmetric operations. Comprehensive
evaluations demonstrate that UVP significantly enhances performance, achieving
up to 3.0$\times$ and 2.1$\times$ speedups in matrix multiplication and fast
Fourier transform (FFT) tasks, respectively, when measured against lane-based
vector architectures. Our synthesized RTL for a 16-lane configuration using
SMIC 40nm technology spans 0.94 mm$^2$ and achieves an area efficiency of 21.2
GOPS/mm$^2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue
  and Anagram Agnostic Data Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Zheng Han, Umang Mathur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifying linearizability of concurrent data structures is NP-hard, even for
simple types. We present fixed-parameter tractable algorithms for monitoring
stacks, queues, and anagram-agnostic data types (AADTs), parameterized by the
maximum concurrency. Our approach leverages frontier graphs and partition
states to bound the search space. For AADTs, equivalence of linearizations
enables monitoring in log-linear time. For stacks, we introduce a grammar-based
method with a sub-cubic reduction to matrix multiplication, and for queues, a
split-sequence transition system supporting efficient dynamic programming.
These results unify tractability guarantees for both order-sensitive and
anagram-agnostic data types under bounded concurrency.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Data Structures and Algorithms <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple and Robust Protocol for Distributed Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edith Cohen, Moshe Shechner, Uri Stemmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the distributed counting problem, where a server must continuously
approximate the total number of events occurring across $k$ sites while
minimizing communication. The communication complexity of this problem is known
to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang,
Yi, and Zhang (2012) showed that randomization can reduce this to
$\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to
the {\em oblivious setting}, where the stream of events is independent of the
protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed
counting that removes the oblivious assumption. However, their communication
complexity is suboptimal by a $polylog(k)$ factor and their protocol is
substantially more complex than the oblivious protocol of Huang et al. (2012).
This left open a natural question: could it be that the simple protocol of
Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the
protocol of Huang et al. (2012) is itself not robust by constructing an
explicit adaptive attack that forces it to lose its accuracy. Second, we
present a new, surprisingly simple, robust protocol for distributed counting
that achieves the optimal communication complexity of
$O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of
Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and
is the first to match the optimal oblivious complexity in the adaptive setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subsequence Covers of Words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Charalampopoulos, Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter, Tomasz Waleń, Wiktor Zuba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce subsequence covers (s-covers, in short), a new type of covers of
a word. A word $C$ is an s-cover of a word $S$ if the occurrences of $C$ in $S$
as subsequences cover all the positions in $S$.
  The s-covers seem to be computationally much harder than standard covers of
words (cf. Apostolico et al., Inf. Process. Lett. 1991), but, on the other
hand, much easier than the related shuffle powers (Warmuth and Haussler, J.
Comput. Syst. Sci. 1984).
  We give a linear-time algorithm for testing if a candidate word $C$ is an
s-cover of a word $S$ over a polynomially-bounded integer alphabet. We also
give an algorithm for finding a shortest s-cover of a word $S$, which in the
case of a constant-sized alphabet, also runs in linear time.
  The words without proper s-cover are called s-primitive. We complement our
algorithmic results with explicit lower and an upper bound on the length of a
longest s-primitive word. Both bounds are exponential in the size of the
alphabet. The upper bound presented here improves the bound given in the
conference version of this paper [SPIRE 2022].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Simplified algorithm in section 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Learning of One-Counter Automata via State-Merging Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibashis Guha, Anirban Majumdar, Prince Mathew, A. V. Sreejith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose One-counter Positive Negative Inference (OPNI), a passive learning
algorithm for deterministic real-time one-counter automata (DROCA). Inspired by
the RPNI algorithm for regular languages, OPNI constructs a DROCA consistent
with any given valid sample set.
  We further present a method for combining OPNI with active learning of DROCA,
and provide an implementation of the approach. Our experimental results
demonstrate that this approach scales more effectively than existing
state-of-the-art algorithms. We also evaluate the performance of the proposed
approach for learning visibly one-counter automata.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 24 figures, 3 procedures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Efficient and Scalable Design of In-Memory Graph-Based Vector
  Search <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Azizi, Karima Echihab, Themis Palpanas, Vassilis Christophides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector data is prevalent across business and scientific applications, and its
popularity is growing with the proliferation of learned embeddings. Vector data
collections often reach billions of vectors with thousands of dimensions, thus,
increasing the complexity of their analysis. Vector search is the backbone of
many critical analytical tasks, and graph-based methods have become the best
choice for analytical tasks that do not require guarantees on the quality of
the answers. Although several paradigms (seed selection, incremental insertion,
neighborhood propagation, neighborhood diversification, and divide-and-conquer)
have been employed to design in-memory graph-based vector search algorithms, a
systematic comparison of the key algorithmic advances is still missing. We
conduct an exhaustive experimental evaluation of twelve state-of-the-art
methods on seven real data collections, with sizes up to 1 billion vectors. We
share key insights about the strengths and limitations of these methods; e.g.,
the best approaches are typically based on incremental insertion and
neighborhood diversification, and the choice of the base graph can hurt
scalability. Finally, we discuss open research directions, such as the
importance of devising more sophisticated data adaptive seed selection and
diversification strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICML 2025 VecDB Workshop; an extended version appeared
  in ACM SIGMOD 2025 ('Graph-Based Vector Search: An Experimental Evaluation of
  the State-of-the-Art')</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unweighted One-Sided Code Sparsifiers and Thin Subgraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Oveis Gharan, Arvin Sahami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a linear code $\mathcal{C} \subseteq \mathbb{F}_2^n$ and $\alpha \in
[0,1]$, call a set $S \subseteq [n]$ an (unweighted) one-sided
$\alpha$-sparsifier of $\mathcal{C}$ if for all $c \in \mathcal{C}$,
$\mathrm{wt}(c_S)\geq \alpha \cdot \mathrm{wt}(c)$, where $c_S$ is the
projection of $c$ onto the coordinates in $S$ and $\mathrm{wt}(c)$ is the
Hamming weight of $c$. \\ We show that every $k$-dimensional linear code
$\mathcal{C}\subseteq \mathbb{F}_2^n$ has at least $2^{n - k}$ many unweighted
one-sided $1/2$-sparsifiers and hence one of size at most $n/2 + O(\sqrt{n
k})$. As an application, letting $\mathcal{C} \subseteq \mathbb{F}_2^E$ denote
the cut-space of a graph $G=(V, E)$, we show a lower bound of $2^{\lvert E
\rvert- (\lvert V \rvert - 1)}$ on the number of $1/2$-thin subgraphs of $G$
and the existence of a $1/2$-thin subgraph with at least $\lvert E \rvert
/2-O(\sqrt{\lvert E \rvert \cdot \lvert V \rvert})$ edges. In contrast to
previous results on thin subgraphs, our proofs are purely "combinatorial".
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sorting and Selection in Rounds with Adversarial Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Trevisan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We continue the study of selection and sorting of $n$ numbers under the
adversarial comparator model, where comparisons can be adversarially tampered
with if the arguments are sufficiently close.
  We derive a randomized sorting algorithm that does $O(n \log^2 n)$
comparisons and gives a correct answer with high probability, addressing an
open problem of Ajtai, Feldman, Hassadim, and Nelson [AFHN15]. Our algorithm
also implies a selection algorithm that does $O(n \log n)$ comparisons and
gives a correct answer with high probability. Both of these results are a
$\log$ factor away from the naive lower bound. [AFHN15] shows an
$\Omega(n^{1+\varepsilon})$ lower bound for both sorting and selection in the
deterministic case, so our results also prove a discrepancy between what is
possible with deterministic and randomized algorithms in this setting.
  We also consider both sorting and selection in rounds, exploring the tradeoff
between accuracy, number of comparisons, and number of rounds. Using results
from sorting networks, we give general algorithms for sorting in $d$ rounds
where the number of comparisons increases with $d$ and the accuracy decreases
with $d$. Using these algorithms, we derive selection algorithms in $d+O(\log
d)$ rounds that use the same number of comparisons as the corresponding sorting
algorithm, but have a constant accuracy. Notably, this gives selection
algorithms in $d$ rounds that use $n^{1 + o(1)}$ comparisons and have constant
accuracy for all $d = \omega(1)$, which still beats the deterministic lower
bound of $\Omega(n^{1+\varepsilon})$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SODA 2024; minor revisions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quasi-optimal hierarchically semi-separable matrix approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noah Amsel, Tyler Chen, Feyza Duman Keles, Diana Halikias, Cameron Musco, Christopher Musco, David Persson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a randomized algorithm for producing a quasi-optimal
hierarchically semi-separable (HSS) approximation to an $N\times N$ matrix $A$
using only matrix-vector products with $A$ and $A^T$. We prove that, using $O(k
\log(N/k))$ matrix-vector products and ${O}(N k^2 \log(N/k))$ additional
runtime, the algorithm returns an HSS matrix $B$ with rank-$k$ blocks whose
expected Frobenius norm error $\mathbb{E}[\|A - B\|_F^2]$ is at most
$O(\log(N/k))$ times worse than the best possible approximation error by an HSS
rank-$k$ matrix. In fact, the algorithm we analyze in a simple modification of
an empirically effective method proposed by [Levitt & Martinsson, SISC 2024].
As a stepping stone towards our main result, we prove two results that are of
independent interest: a similar guarantee for a variant of the algorithm which
accesses $A$'s entries directly, and explicit error bounds for near-optimal
subspace approximation using projection-cost-preserving sketches. To the best
of our knowledge, our analysis constitutes the first polynomial-time
quasi-optimality result for HSS matrix approximation, both in the explicit
access model and the matrix-vector product query model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FPT approximations for Capacitated Sum of Radii and Diameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnold Filtser, Ameet Gadekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Capacitated Sum of Radii problem involves partitioning a set of points
$P$, where each point $p\in P$ has capacity $U_p$, into $k$ clusters that
minimize the sum of cluster radii, such that the number of points in the
cluster centered at point $p$ is at most $U_p$. We begin by showing that the
problem is APX-hard, and that under gap-ETH there is no parameterized
approximation scheme (FPT-AS). We then construct a $\approx5.83$-approximation
algorithm in FPT time (improving a previous $\approx7.61$ approximation in FPT
time). Our results also hold when the objective is a general monotone symmetric
norm of radii. We also improve the approximation factors for the uniform
capacity case, and for the closely related problem of Capacitated Sum of
Diameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>improved hardness of approximation factor and minor edits</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Programming tension in 3D printed networks inspired by spiderwebs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thijs Masmeijer, Caleb Swain, Jeff Hill, Ed Habtour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Each element in tensioned structural networks -- such as tensegrity,
architectural fabrics, or medical braces/meshes -- requires a specific tension
level to achieve and maintain the desired shape, stability, and compliance.
These structures are challenging to manufacture, 3D print, or assemble because
flattening the network during fabrication introduces multiplicative
inaccuracies in the network's final tension gradients. This study overcomes
this challenge by offering a fabrication algorithm for direct 3D printing of
such networks with programmed tension gradients, an approach analogous to the
spinning of spiderwebs. The algorithm: (i) defines the desired network and
prescribes its tension gradients using the force density method; (ii) converts
the network into an unstretched counterpart by numerically optimizing vertex
locations toward target element lengths and converting straight elements into
arcs to resolve any remaining error; and (iii) decomposes the network into
printable toolpaths; Optional additional steps are: (iv) flattening curved 2D
networks or 3D networks to ensure 3D printing compatibility; and (v)
automatically resolving any unwanted crossings introduced by the flattening
process. The proposed method is experimentally validated using 2D unit cells of
viscoelastic filaments, where accurate tension gradients are achieved with an
average element strain error of less than 1.0\%. The method remains effective
for networks with element minimum length and maximum stress of 5.8 mm and 7.3
MPa, respectively. The method is used to demonstrate the fabrication of three
complex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The
programmable tension gradient algorithm can be utilized to produce compact,
integrated cable networks, enabling novel applications such as moment-exerting
structures in medical braces and splints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaMO: Parallel Mesh Optimization for Intersection-Free Low-Poly Modeling
  on the GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghun Oh, Xiaodi Yuan, Xinyue Wei, Ruoxi Shi, Fanbo Xiang, Minghua Liu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing the triangle count in complex 3D models is a basic geometry
preprocessing step in graphics pipelines such as efficient rendering and
interactive editing. However, most existing mesh simplification methods exhibit
a few issues. Firstly, they often lead to self-intersections during decimation,
a major issue for applications such as 3D printing and soft-body simulation.
Second, to perform simplification on a mesh in the wild, one would first need
to perform re-meshing, which often suffers from surface shifts and losses of
sharp features. Finally, existing re-meshing and simplification methods can
take minutes when processing large-scale meshes, limiting their applications in
practice. To address the challenges, we introduce a novel GPU-based mesh
optimization approach containing three key components: (1) a parallel
re-meshing algorithm to turn meshes in the wild into watertight, manifold, and
intersection-free ones, and reduce the prevalence of poorly shaped triangles;
(2) a robust parallel simplification algorithm with intersection-free
guarantees; (3) an optimization-based safe projection algorithm to realign the
simplified mesh with the input, eliminating the surface shift introduced by
re-meshing and recovering the original sharp features. The algorithm
demonstrates remarkable efficiency, simplifying a 2-million-face mesh to 20k
triangles in 3 seconds on RTX4090. We evaluated the approach on the Thingi10K
dataset and showcased its exceptional performance in geometry preservation and
speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemLayoutDiff: Semantic Layout Generation with Diffusion Model for
  Indoor Scene Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.18597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.18597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Sun, Divyam Goel, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor
scenes across multiple room types. The model introduces a scene layout
representation combining a top-down semantic map and attributes for each
object. Unlike prior approaches, which cannot condition on architectural
constraints, SemLayoutDiff employs a categorical diffusion model capable of
conditioning scene synthesis explicitly on room masks. It first generates a
coherent semantic map, followed by a cross-attention-based network to predict
furniture placements that respect the synthesized layout. Our method also
accounts for architectural elements such as doors and windows, ensuring that
generated furniture arrangements remain practical and unobstructed. Experiments
on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent,
realistic, and varied scenes, outperforming previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://3dlg-hcvc.github.io/SemLayoutDiff/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-05T00:00:00Z">2025-09-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Xu, Junxi Xia, Weisi Yang, Yueyuan Sui, Stephen Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mainframe-Style Channel Controllers for Modern Disaggregated Memory
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Liu, Jasmin Schult, Pengcheng Xu, Timothy Roscoe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promise of alleviating the main memory bottleneck, and the
existence of commercial hardware implementations, techniques for Near-Data
Processing have seen relatively little real-world deployment. The idea has
received renewed interest with the appearance of disaggregated or "far" memory,
for example in the use of CXL memory pools.
  However, we argue that the lack of a clear OS-centric abstraction of
Near-Data Processing is a major barrier to adoption of the technology. Inspired
by the channel controllers which interface the CPU to disk drives in mainframe
systems, we propose memory channel controllers as a convenient, portable, and
virtualizable abstraction of Near-Data Processing for modern disaggregated
memory systems.
  In addition to providing a clean abstraction that enables OS integration
while requiring no changes to CPU architecture, memory channel controllers
incorporate another key innovation: they exploit the cache coherence provided
by emerging interconnects to provide a much richer programming model, with more
fine-grained interaction, than has been possible with existing designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready authors' version for APSys'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Estimation for Linear Systems with Non-Gaussian Measurement Noise
  via Dynamic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hussein Yoosefian Nooshabadi, Laurent Lessard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new recursive estimator for linear dynamical systems under
Gaussian process noise and non-Gaussian measurement noise. Specifically, we
develop an approximate maximum a posteriori (MAP) estimator using dynamic
programming and tools from convex analysis. Our approach does not rely on
restrictive noise assumptions and employs a Bellman-like update instead of a
Bayesian update. Our proposed estimator is computationally efficient, with only
modest overhead compared to a standard Kalman filter. Simulations demonstrate
that our estimator achieves lower root mean squared error (RMSE) than the
Kalman filter and has comparable performance to state-of-the-art estimators,
while requiring significantly less computational power.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fully Analog Implementation of Model Predictive Control with
  Application to Buck Converters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Pirrera, Lorenzo Calogero, Francesco Gabriele, Diego Regruto, Alessandro Rizzo, Gianluca Setti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to design analog electronic circuits
that implement Model Predictive Control (MPC) policies for plants described by
affine models. The combination of state-of-the-art approaches to define
reduced-complexity Explicit MPC (EMPC) is employed to realize an analog circuit
characterized by a limited amount of low-latency and commercially available
components. The practical feasibility and effectiveness of the proposed
approach are demonstrated through its application in the design of an advanced
controller for DC-DC Buck converters. We formally analyze the stability of the
obtained system and conduct extensive numerical simulations to demonstrate that
it is capable of achieving outstanding load disturbance rejection performance,
outclassing standard approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in
  AGC Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jehad Jilan, Niranjana Naveen Nambiar, Ahmad Mohammad Saber, Alok Paranjape, Amr Youssef, Deepa Kundur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Generation Control (AGC) is essential for power grid stability but
remains vulnerable to stealthy cyberattacks, such as False Data Injection
Attacks (FDIAs), which can disturb the system's stability while evading
traditional detection methods. Unlike previous works that relied on blackbox
approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an
interpretable and accurate method for FDIA detection in AGC systems,
considering the system nonlinearities. KAN models include a method for
extracting symbolic equations, and are thus able to provide more
interpretability than the majority of machine learning models. The proposed KAN
is trained offline to learn the complex nonlinear relationships between the AGC
measurements under different operating scenarios. After training, symbolic
formulas that describe the trained model's behavior can be extracted and
leveraged, greatly enhancing interpretability. Our findings confirm that the
proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for
the initial model and the symbolic formula, respectively, with a low false
alarm rate, offering a reliable approach to enhancing AGC cybersecurity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Peer-reviewed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Model Predictive Control Design for Autonomous Vehicles with
  Perception-based Observers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nariman Niknejad, Gokul S. Sankar, Bahare Kiumarsi, Hamidreza Modares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedback Linearisation with State Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Jin, Yuanbo Nie, Morgan Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feedback Linearisation (FBL) is a widely used technique that applies feedback
laws to transform input-affine nonlinear dynamical systems into linear
dynamical systems, allowing for the use of linear controller design methods
such as pole placement. However, for problems with state constraints,
controlling the linear system induced by FBL can be more challenging than
controlling the original system. This is because simple state constraints in
the original nonlinear system become complex nonlinear constraints in the FBL
induced linearised system, thereby diminishing the advantages of linearisation.
To avoid increasing the complexity of state constraints under FBL, this paper
introduces a method to first augment system dynamics to capture state
constraints before applying FBL. We show that our proposed augmentation method
leads to ill-defined relative degrees at state constraint boundaries. However,
we show that ill-defined relative degrees can be overcome by using a switching
FBL controller. Numerical experiments illustrate the capabilities of this
method for handling state constraints within the FBL framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collective decision-making dynamics in hypernetworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angela Fontan, Silun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work describes a collective decision-making dynamical process in a
multiagent system under the assumption of cooperative higher-order interactions
within the community, modeled as a hypernetwork. The nonlinear interconnected
system is characterized by saturated nonlinearities that describe how agents
transmit their opinion state to their neighbors in the hypernetwork, and by a
bifurcation parameter representing the community's social effort. We show that
the presence of higher-order interactions leads to the unfolding of a pitchfork
bifurcation, introducing an interval for the social effort parameter in which
the system exhibits bistability. With equilibrium points representing
collective decisions, this implies that, depending on the initial conditions,
the community will either remain in a deadlock state (with the origin as the
equilibrium point) or reach a nontrivial decision. A numerical example is given
to illustrate the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model predictive quantum control: A modular approach for efficient and
  robust quantum optimal control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eya Guizani, Julian Berberich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model predictive control (MPC) is one of the most successful modern control
methods. It relies on repeatedly solving a finite-horizon optimal control
problem and applying the beginning piece of the optimal input. In this paper,
we develop a modular framework for improving efficiency and robustness of
quantum optimal control (QOC) via MPC. We first provide a tutorial introduction
to basic concepts of MPC from a QOC perspective. We then present multiple MPC
schemes, ranging from simple approaches to more sophisticated schemes which
admit stability guarantees. This yields a modular framework which can be used
1) to improve efficiency of open-loop QOC and 2) to improve robustness of
closed-loop quantum control by incorporating feedback. We demonstrate these
benefits with numerical results, where we benchmark the proposed methods
against competing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StimulHeat: a Low-Energy Wearable Thermal Feedback Device Using Peltier
  Elements with Heat Flow Controlled Loop for Hand Interactions in Virtual
  Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Mesnage, Sophie Villenave, Bertrand Massot, Matthieu Blanchard, Pierre Raimbaud, Guillaume Lavoué, Claudine Gehin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, the majority of wearable thermal feedback systems designed for use
in virtual reality applications are not compatible or not integrated to
standard controllers and are based on temperature control. The objectives of
the present work is to enable integration with existing controllers, in this
case Valve Index controllers, and to propose an alternative approach to
managing thermal stimulation with Peltier modules by controlling heat flow
instead of temperature. We introduce StimulHeat as a wireless, low power
thermal feedback system, based on the continuous relationship between heat and
current injection in thermoelectric device (TED). First, we designed an
optimized TED driver capable of injecting a continuous, bidirectional current
into the TED, thereby driving it as a heater or cooler. Subsequently, this
driver was implemented in an electronic board to include temperature and heat
flow control loops, as well as Bluetooth Low Energy interface for remote
control. A mechanical integration was conducted, in the form of a controller
extension which is non-intrusive and can be clipped to Valve Index controllers
to enclose the TED, temperature sensors and electronics. Finally, we present a
user study validating StimulHeat for use in Virtual Reality, utilizing a
Unity-built virtual environment with our open-source package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Cellular Network Delays in Finnish Railways: A Machine
  Learning Enhanced Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeideh Mansouri, Mohamed Shamekh, Simon Indola, Petri Mahonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing interest in using public cellular networks for specialized
communication applications, replacing standalone sector-specific networks. One
such application is transitioning from the aging GSM-R railway network to
public 4G and 5G networks. Finland is modernizing its railway communication
system through the Digirail project, leveraging public cellular networks. To
evaluate network performance, a nationwide measurement campaign was conducted
in two modes: Best Quality and Packet Replication. However, Best Quality mode
introduces artificial delays, making it unsuitable for real-world assessments.
In this paper, railway network delays are modeled using machine learning based
on measurements from the Packet Replication mode. The best-performing model is
then employed to generate a dataset estimating network delays across Finland's
railway network. This dataset provides a more accurate representation of
network performance. Machine learning based network performance prediction is
shown to be feasible, and the results indicate that Finland's public cellular
network can meet the stringent performance requirements of railway network
control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at IEEE PIMRC 2025. 6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Analysis of Pinching-Antenna-Enabled Internet of Things
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Bingxin Zhang, Yizhe Zhao, Kun Yang, Guopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pinching-antenna systems (PASS), which activate small dielectric
particles along a dielectric waveguide, has recently emerged as a promising
paradigm for flexible antenna deployment in next-generation wireless
communication networks. While most existing studies assume rectangular indoor
layouts with full coverage waveguide, practical deployments may involve
geometric constraints, partial coverage, and non-negligible waveguide
attenuation. This paper presents the first analytical investigation of PASS in
a circular indoor environment, encompassing both full coverage and partial
coverage waveguide configurations with/without propagation loss. A unified
geometric-propagation framework is developed that jointly captures
pinching-antenna placement, Internet of Things (IoT) device location
distribution, and waveguide attenuation. Closed-form expressions for the outage
probability and average achievable rate are derived for four scenarios, with
accuracy validated via extensive Monte-Carlo simulations. The analysis reveals
that, under the partial coverage waveguide scenario with propagation loss, the
system performance demonstrates a non-monotonic trend with respect to the
waveguide length, and the optimal length decreases as the attenuation
coefficient increases. Numerical results further quantify the interplay between
deployment strategy, waveguide propagation loss, and coverage geometry,
offering practical guidelines for performance-oriented PASS design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling, Observability, and Inertial Parameter Estimation of a Planar
  Multi-Link System with Thrusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas B. Andrews, Kristi A. Morgansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research provides a theoretical foundation for modeling and real-time
estimation of both the pose and inertial parameters of a free-floating
multi-link system with link thrusters, which are essential for safe and
effective controller design and performance. First, we adapt a planar nonlinear
multi-link snake robot model to represent a planar chain of bioinspired salp
robots by removing joint actuators, introducing link thrusters, and allowing
for non-uniform link lengths, masses, and moments of inertia. Second, we
conduct a nonlinear observability analysis of the multi-link system with link
thrusters, proving that the link angles, angular velocities, masses, and
moments of inertia are locally observable when equipped with inertial
measurement units and operating under specific thruster conditions. The
analytical results are demonstrated in simulation with a three-link system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Community-Centric Multi-Criteria Assessment Framework for Energy
  Transition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayashree Yadav, Ingemar Mathiasson, Bindu Panikkar, Mads Almassalkhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transition to low-carbon energy systems demands comprehensive technical,
economic, environmental, and social evaluation tools. While numerous studies
address specific aspects of energy transition, few provide an integrated
framework to capture the full spectrum of impacts. This work developed a
community-collaborative assessment framework that integrates intelligent energy
devices with optimization-based coordination of energy assets. The proposed
framework uses techno-economic, environmental, and social criteria to evaluate
transition pathways. A detailed case study is performed for a remote community
in Alaska to assess its applicability, where the feasibility of renewable
energy transitions remains underexplored. Three distinct pathways, including
heat pump and battery integration, resource coordination, and expanded
community solar PV, are analyzed using a year-long dataset of demand, renewable
energy, and transformer data. The analysis revealed that using heat pumps
lowers the overall energy costs by 30% and carbon emissions by 28%. In
addition, the share of the population spending more than 10% of their income on
energy falls from 74% in the existing scenario to 40% with heat pump adoption,
indicating significant affordability improvements. By combining a general,
community-centric assessment framework with a data-driven case study, this work
offers a practical tool for utilities, community stakeholders, and policymakers
to work toward equitable and sustainable energy transitions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NISE-PE Constraint: Data-Driven Predictive Control with Persistence of
  Excitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucca Heinze Faro, Yuanbo Nie, Paul Trodden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persistence of excitation (PE) is an important requirement for the successful
operation of data-driven predictive control, as it ensures that the
input-output data contains sufficient information about the underlying system
dynamics. Nonetheless, this property is usually assumed rather than guaranteed.
This paper introduces a novel data-driven predictive control formulation that
maintains PE. The technical development that allows this is the
characterisation of the nonexciting input set (NIS), i.e., the set of inputs
that lead to loss of PE, and the consequent derivation of a pair of disjoint,
linear inequality constraints on the input, termed NIS exclusion PE (NIS-PE)
constraint, that, if satisfied, maintain PE. When used in a predictive control
formulation, these constraints lead to a mixed-integer optimal control problem
with a single binary variable or, equivalently, a pair of disjoint quadratic
programming problems that can be efficiently and reliably solved. Numerical
examples show how these constraints are able to maintain PE during the
controller's operation, resulting in improved performance over conventional
approaches for both time-invariant and time-varying systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. Accepted for presentation at, and publication in
  the proceedings of, the 2025 64th IEEE Conference on Decision and Control
  (CDC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Coordinate: Distributed Meta-Trajectory Optimization Via
  Differentiable ADMM-DDP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingheng Wang, Yichao Gao, Tianchen Sun, Lin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed trajectory optimization via ADMM-DDP is a powerful approach for
coordinating multi-agent systems, but it requires extensive tuning of tightly
coupled hyperparameters that jointly govern local task performance and global
coordination. In this paper, we propose Learning to Coordinate (L2C), a general
framework that meta-learns these hyperparameters, modeled by lightweight
agent-wise neural networks, to adapt across diverse tasks and agent
configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in
a distributed manner. It also enables efficient meta-gradient computation by
reusing DDP components such as Riccati recursions and feedback gains. These
gradients correspond to the optimal solutions of distributed matrix-valued LQR
problems, coordinated across agents via an auxiliary ADMM framework that
becomes convex under mild assumptions. Training is further accelerated by
truncating iterations and meta-learning ADMM penalty parameters optimized for
rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a
challenging cooperative aerial transport task, L2C generates dynamically
feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures
quadrotor formations for safe 6-DoF load manipulation in tight spaces, and
adapts robustly to varying team sizes and task conditions, while achieving up
to $88\%$ faster gradient computation than state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attitude Control of Rigid Bodies: A <span class="highlight-title">Survey</span> of Representations,
  Topological Obstructions, and Stabilization Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Su, Dandan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the attitude control problems for rigid-body systems,
starting from the attitude representation for rigid body kinematics. Highly
redundant rotation matrix defines the attitude orientation globally and
uniquely by 9 parameters, which is the most fundamental one, without any
singularities; minimum 3-parameter Euler angles or (modified) Rodrigues
parameters define the attitude orientation neither globally nor uniquely, but
the former exhibits kinematical singularity and Gimbal lock, while the latter
two exhibit geometrical singularity; once-redundant axis-angle or unit
quaternion globally define the attitude rotation but not uniquely using 4
parameters, but the former is not appropriate to define very small or very
large rotations, while the latter shows unwinding phenomenon despite of the
reduced computation burden. In addition, we explore the relationships among
those attitude representations, including the connections among Gimbal lock,
unwinding phenomenon and a nowhere dense set of zero Lebesgue measure. Based on
attitude representations, we analyze different attitude control laws, almost
global control and global attitude control, nominal and general robustness, as
well as the technique tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sensing environmental physical interaction to traverse cluttered
  obstacles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13062v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13062v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqing Wang, Ling Xu, Chen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The long-standing, dominant approach to robotic obstacle negotiation relies
on mapping environmental geometry to avoid obstacles. However, this approach
does not allow for traversal of cluttered obstacles, hindering applications
such as search and rescue operations through earthquake rubble and exploration
across lunar and Martian rocks. To overcome this challenge, robots must further
sense and utilize environmental physical interactions to control themselves to
traverse obstacles. Recently, a physics-based approach has been established
towards this vision. Self-propelled robots interacting with obstacles results
in a potential energy landscape. On this landscape, to traverse obstacles, a
robot must escape from certain landscape basins that attract it into failure
modes, to reach other basins that lead to successful modes. Thus, sensing the
potential energy landscape is crucial. Here, we developed new methods and
performed systematic experiments to demonstrate that the potential energy
landscape can be estimated by sensing environmental physical interaction. We
developed a minimalistic robot capable of sensing obstacle contact forces and
torques for systematic experiments over a wide range of parameter space.
Surprisingly, although these forces and torques are not fully conservative,
they match the potential energy landscape gradients that are conservative
forces and torques, enabling an accurate estimation of the potential energy
landscape. Additionally, a bio-inspired strategy further enhanced estimation
accuracy. Our results provided a foundation for further refining these methods
for use in free-locomoting robots. Our study is a key step in establishing a
new physics-based approach for robots to traverse clustered obstacles to
advance their mobility in complex, real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Barrier Certificates for Unknown Systems with Latent States and
  Polynomial Dynamics using Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Lefringhausen, Sami Leon Noel Aziz Hanna, Elias August, Sandra Hirche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Certifying safety in dynamical systems is crucial, but barrier certificates -
widely used to verify that system trajectories remain within a safe region -
typically require explicit system models. When dynamics are unknown,
data-driven methods can be used instead, yet obtaining a valid certificate
requires rigorous uncertainty quantification. For this purpose, existing
methods usually rely on full-state measurements, limiting their applicability.
This paper proposes a novel approach for synthesizing barrier certificates for
unknown systems with latent states and polynomial dynamics. A Bayesian
framework is employed, where a prior in state-space representation is updated
using output data via a targeted marginal Metropolis-Hastings sampler. The
resulting samples are used to construct a barrier certificate through a
sum-of-squares program. Probabilistic guarantees for its validity with respect
to the true, unknown system are obtained by testing on an additional set of
posterior samples. The approach and its probabilistic guarantees are
illustrated through a numerical simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Proceedings of the 64th IEEE
  Conference on Decision and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability Analysis for Stochastic Hybrid Inclusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.14360v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.14360v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Su, Dandan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic hybrid inclusions (SHIs) address situations with the stochastic
continuous evolution in a stochastic differential inclusions and random jumps
in the difference inclusions due to the forced (the state reaching a boundary
in the state space) and/or spontaneous (the state vector may occur
spontaneously) transitions. An obvious characteristic of SHIs is the
non-uniqueness of random solutions, which can be ensured by the mild regularity
conditions, as well as nominal robustness. Basic sufficient conditions for
stability/recurrence in probability are usually expressed based on different
types of Lyapunov functions, including Lagrange/Lyapunov/Lyapunov-Forster
functions respectively for Lagrange/Lyapunov/asymptotical stability in
probability and Foster/Lagrange-Forster functions for recurrence, (weaker)
relaxed Lyapunov-based sufficient conditions including Matrosov-Foster
functions and the stochastic invariance principle, as well as Lyapunov-based
necessary and sufficient conditions for asymptotical stability in probability
or recurrence (i.e.,converse theorems), etc. The converse theorems involving
smooth Lyapunov functions are guaranteed by the sequential compactness and thus
robustness. In addition, the uniformity property and causality are analyzed for
the stabilities in probability. Hence, serving as a partial roadmap for the
theoretical development of SHIs, also serving as inspiration, we anticipate
that many of the open questions, including the prediction problem, the
filtering problem and the control problem, will be resolved based on the
techniques of SHIs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation of Parameters in Heterogeneous Multi-agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungbo Shim, Jin Gyu Lee, B. D. O. Anderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an adaptation mechanism for heterogeneous multi-agent
systems to align the agents' internal parameters, based on enforced consensus
through strong couplings. Unlike homogeneous systems, where exact consensus is
attainable, the heterogeneity in node dynamics precludes perfect
synchronization. Nonetheless, previous work has demonstrated that strong
coupling can induce approximate consensus, whereby the agents exhibit emergent
collective behavior governed by the so-called blended dynamics. Building on
this observation, we introduce an adaptation law that gradually aligns the
internal parameters of agents without requiring direct parameter communication.
The proposed method reuses the same coupling signal employed for state
synchronization, which may result in a biologically or sociologically plausible
adaptation process. Under a persistent excitation condition, we prove that the
linearly parametrized vector fields of the agents converge to each other,
thereby making the dynamics asymptotically homogeneous, and leading to exact
consensus of the state variables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, IEEE Conf. on Decision and Control 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Error-In-Variables Methods for Efficient System Identification with
  Finite-Sample Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.09057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.09057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Zhang, Xinhe Zhang, Jia Liu, Na Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of learning linear dynamical systems from
noisy observations. In this setting, existing algorithms either yield biased
parameter estimates or have large sample complexities. We resolve these issues
by adapting the instrumental variable method and the bias compensation method,
originally proposed for error-in-variables models, to our setting. We provide
refined non-asymptotic analysis for both methods. Under mild conditions, our
algorithms achieve superior sample complexities that match the best-known
sample complexity for learning a fully observable system without observation
noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05946v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05946v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Wu, Jiahao Ai, Tongxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Predictive Control (MPC) is a powerful control strategy widely utilized
in domains like energy management, building control, and autonomous systems.
However, its effectiveness in real-world settings is challenged by the need to
incorporate context-specific predictions and expert instructions, which
traditional MPC often neglects. We propose InstructMPC, a novel framework that
addresses this gap by integrating real-time human instructions through a Large
Language Model (LLM) to produce context-aware predictions for MPC. Our method
employs a Language-to-Distribution (L2D) module to translate contextual
information into predictive disturbance trajectories, which are then
incorporated into the MPC optimization. Unlike existing context-aware and
language-based MPC models, InstructMPC enables dynamic human-LLM interaction
and fine-tunes the L2D module in a closed loop with theoretical performance
guarantees, achieving a regret bound of $O(\sqrt{T\log T})$ for linear dynamics
when optimized via advanced fine-tuning methods such as Direct Preference
Optimization (DPO) using a tailored loss function.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecture <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Methods for the Cross-Level Verification of SystemC
  Peripherals with Symbolic Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Aaron Rudkowski, Sallar Ahmadi-Pour, Rolf Drechsler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing and Optimizing Realistic Workloads on a Commercial
  Compute-in-SRAM Device <span class="chip">MICRO 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niansong Zhang, Wenbo Zhu, Courtney Golden, Dan Ilan, Hongzheng Chen, Christopher Batten, Zhiru Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compute-in-SRAM architectures offer a promising approach to achieving higher
performance and energy efficiency across a range of data-intensive
applications. However, prior evaluations have largely relied on simulators or
small prototypes, limiting the understanding of their real-world potential. In
this work, we present a comprehensive performance and energy characterization
of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.
We compare the GSI APU against established architectures, including CPUs and
GPUs, to quantify its energy efficiency and performance potential. We introduce
an analytical framework for general-purpose compute-in-SRAM devices that
reveals fundamental optimization principles by modeling performance trade-offs,
thereby guiding program optimizations.
  Exploiting the fine-grained parallelism of tightly integrated memory-compute
architectures requires careful data management. We address this by proposing
three optimizations: communication-aware reduction mapping, coalesced DMA, and
broadcast-friendly data layouts. When applied to retrieval-augmented generation
(RAG) over large corpora (10GB--200GB), these optimizations enable our
compute-in-SRAM system to accelerate retrieval by 4.8$\times$--6.6$\times$ over
an optimized CPU baseline, improving end-to-end RAG latency by
1.1$\times$--1.8$\times$. The shared off-chip memory bandwidth is modeled using
a simulated HBM, while all other components are measured on the real
compute-in-SRAM device. Critically, this system matches the performance of an
NVIDIA A6000 GPU for RAG while being significantly more energy-efficient
(54.4$\times$-117.9$\times$ reduction). These findings validate the viability
of compute-in-SRAM for complex, real-world applications and provide guidance
for advancing the technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICRO 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed-HISQ: A Distributed Quantum Control Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zhao, Kangding Zhao, Peng Zhou, Dingdong Liu, Tingyu Luo, Yuzhen Zheng, Peng Luo, Shun Hu, Jin Lin, Cheng Guo, Yinhe Han, Ying Wang, Mingtang Deng, Junjie Wu, X. Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design of a scalable Quantum Control Architecture (QCA) faces two primary
challenges. First, the continuous growth in qubit counts has rendered
distributed QCA inevitable, yet the nondeterministic latencies inherent in
feedback loops demand cycleaccurate synchronization across multiple
controllers. Existing synchronization strategies -- whether lock-step or
demand-driven -- introduce significant performance penalties. Second, existing
quantum instruction set architectures are polarized, being either too abstract
or too granular. This lack of a unifying design necessitates recurrent hardware
customization for each new control requirement, which limits the system's
reconfigurability and impedes the path toward a scalable and unified digital
microarchitecture.
  Addressing these challenges, we propose Distributed-HISQ, featuring: (i)
HISQ, A universal instruction set that redefines quantum control with a
hardware-agnostic design. By decoupling from quantum operation semantics, HISQ
provides a unified language for control sequences, enabling a single
microarchitecture to support various control methods and enhancing system
reconfigurability. (ii) BISP, a booking-based synchronization protocol that can
potentially achieve zero-cycle synchronization overhead. The feasibility and
adaptability of Distributed-HISQ are validated through its implementation on a
commercial quantum control system targeting superconducting qubits. We
performed a comprehensive evaluation using a customized quantum software stack.
Our results show that BISP effectively synchronizes multiple control boards,
leading to a 22.8% reduction in average program execution time and a
$\sim$5$\times$ reduction in infidelity when compared to an existing lock-step
synchronization scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Intelligence: Designing Data Centers for Next-Gen Language
  Models <span class="chip">SC25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesmin Jahan Tithi, Hanjiang Wu, Avishaii Abuhatzera, Fabrizio Petrini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8
trillion parameters, demands a fundamental rethinking of data center
architecture to ensure scalability, efficiency, and cost-effectiveness. Our
work provides a comprehensive co-design framework that jointly explores FLOPS,
HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat
optical), the size of the scale-out domain, and popular
parallelism/optimization strategies used in LLMs. We introduce and evaluate
FullFlat network architectures, which provide uniform high-bandwidth,
low-latency connectivity between all nodes, and demonstrate their
transformative impact on performance and scalability. Through detailed
sensitivity analyses, we quantify the benefits of overlapping compute and
communication, leveraging hardware-accelerated collectives, widening the
scale-out domain, and increasing memory capacity. Our study spans both sparse
(mixture of experts) and dense transformer-based LLMs, revealing how system
design choices affect Model FLOPS Utilization (MFU = Model FLOPS per token *
Observed tokens per second / Peak FLOPS of the hardware) and overall
throughput. For the co-design study, we utilized an analytical performance
modeling tool capable of predicting LLM runtime within 10% of real-world
measurements. Our findings offer actionable insights and a practical roadmap
for designing AI data centers that can efficiently support trillion-parameter
models, reduce optimization complexity, and sustain the rapid evolution of AI
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, submitted to SC25 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mainframe-Style Channel Controllers for Modern Disaggregated Memory
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Liu, Jasmin Schult, Pengcheng Xu, Timothy Roscoe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promise of alleviating the main memory bottleneck, and the
existence of commercial hardware implementations, techniques for Near-Data
Processing have seen relatively little real-world deployment. The idea has
received renewed interest with the appearance of disaggregated or "far" memory,
for example in the use of CXL memory pools.
  However, we argue that the lack of a clear OS-centric abstraction of
Near-Data Processing is a major barrier to adoption of the technology. Inspired
by the channel controllers which interface the CPU to disk drives in mainframe
systems, we propose memory channel controllers as a convenient, portable, and
virtualizable abstraction of Near-Data Processing for modern disaggregated
memory systems.
  In addition to providing a clean abstraction that enables OS integration
while requiring no changes to CPU architecture, memory channel controllers
incorporate another key innovation: they exploit the cache coherence provided
by emerging interconnects to provide a much richer programming model, with more
fine-grained interaction, than has been possible with existing designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready authors' version for APSys'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Methods for the Cross-Level Verification of SystemC
  Peripherals with Symbolic Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Aaron Rudkowski, Sallar Ahmadi-Pour, Rolf Drechsler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The compact double category $\mathbf{Int}(\mathbf{Poly}_*)$ models
  control flow and data transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grigory Kondyrev, David I. Spivak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hasegawa showed that control flow in programming languages -- while loops and
if-then-else statements -- can be modeled using traced cocartesian categories,
such as the category $\mathbf{Set}_*$ of pointed sets. In this paper we define
an operad $\mathscr{W}$ of wiring diagrams that provides syntax for categories
whose control flow moreover includes data transformations, including deleting,
duplicating, permuting, and applying pre-specified functions to variables. In
the most basic version, the operad underlies $\mathbf{Int}(\mathbf{Poly}_*)$,
where $\mathbf{Int}(\mathscr{T})$ denotes the free compact category on a traced
category $\mathscr{T}$, as defined by Joyal, Street, and Verity; to do so, we
show that $\mathbf{Poly}_*$, as well as any multivariate version of it, is
traced. We show moreover that whenever $\mathscr{T}$ is uniform -- a condition
also defined by Hasegawa and satisfied by $\mathbf{Int}(\mathscr{T})$ -- the
resulting $\mathbf{Int}$-construction extends to a double category
$\mathbb{I}\mathbf{nt}(\mathscr{T})$, which is compact in the sense of
Patterson. Finally, we define a universal property of the double category
$\mathbb{I}\mathbf{nt}(\mathbf{Poly}_*)$ and
$\mathbb{I}\mathbf{nt}(\mathbf{Set}_*)$ by which one can track trajectories as
they move through the control flow associated to a wiring diagram.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages including many diagrams</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Termination Proving: 100 Million LoC and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Vanegue, Jules Villard, Peter O'Hearn, Azalea Raad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Assisted Modeling: DSL-Driven AI Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Smyth, Daniel Busch, Moez Ben Haj Hmida, Edward A. Lee, Bernhard Steffen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale Study of Floating-Point Usage in Statically Typed
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Gilot, Tobias Wrigstad, Eva Darulova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forall-Exists Relational Verification by Filtering to Forall-Forall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramana Nagasamudram, Anindya Banerjee, David A. Naumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational verification encompasses research directions such as reasoning
about data abstraction, reasoning about security and privacy, secure
compilation, and functional specificaton of tensor programs, among others.
Several relational Hoare logics exist, with accompanying tool support for
compositional reasoning of $\forall\forall$ (2-safety) properties and,
generally, k-safety properties of product programs. In contrast, few logics and
tools exist for reasoning about $\forall\exists$ properties which are critical
in the context of nondeterminism.
  This paper's primary contribution is a methodology for verifying a
$\forall\exists$ judgment by way of a novel filter-adequacy transformation.
This transformation adds assertions to a product program in such a way that the
desired $\forall\exists$ property (of a pair of underlying unary programs) is
implied by a $\forall\forall$ property of the transformed product. The paper
develops a program logic for the basic $\forall\exists$ judgement extended with
assertion failures; develops bicoms, a form of product programs that represents
pairs of executions and that caters for direct translation of $\forall\forall$
properties to unary correctness; proves (using the logic) a soundness theorem
that says successful $\forall\forall$ verification of a transformed bicom
implies the $\forall\exists$ spec for its underlying unary commands; and
implements a proof of principle prototype for auto-active relational
verification which has been used to verify all examples in the paper. The
methodology thereby enables a user to work with ordinary assertions and
assumptions, and a standard assertion language, so that existing tools
including auto-active verifiers can be used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ veScale: Consistent and Efficient Tensor Programming with Eager-Mode
  SPMD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.07003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.07003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have scaled rapidly in size and complexity,
requiring increasingly intricate parallelism for distributed training, such as
3D parallelism. This sophistication motivates a shift toward simpler, more
debuggable programming paradigm like Single Program Multiple Data (SPMD).
However, SPMD in eager execution introduces two key challenges: ensuring
consistency with single-device execution and achieving high performance at
scale. In this paper, we introduce veScale, an eager-mode training system that
fully embraces SPMD paradigm to democratize distributed tensor programming.
veScale addresses the prevalent issue of inconsistent results in systems like
PyTorch by introducing a novel algorithm of distributed Random Number
Generation (RNG) compatible with arbitrary sharded operators. veScale also
significantly boosts training performance by reducing PyTorch primitive's
overhead and improving communication efficiency. Evaluations show that veScale
delivers up to 2.2x speedup over the state-of-the-art training systems, like
TorchTitan, and cuts code complexity by 78.4%, while preserving
single-device-equivalent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 16 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Special Delivery: Programming with Mailbox Types (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Fowler, Duncan Paul Attard, Danielle Marshall, Simon J. Gay, Phil Trinder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The asynchronous and unidirectional communication model supported by
mailboxes is a key reason for the success of actor languages like Erlang and
Elixir for implementing reliable and scalable distributed systems. While many
actors may send messages to some actor, only the actor may receive from its
mailbox. Although actors eliminate many of the issues stemming from shared
memory concurrency, they remain vulnerable to communication errors such as
protocol violations and deadlocks.
  Mailbox types are a novel behavioural type system for mailboxes first
introduced for a process calculus by de'Liguoro and Padovani in 2018, which
capture the contents of a mailbox as a commutative regular expression. Due to
aliasing and nested evaluation contexts, moving from a process calculus to a
programming language is challenging. This paper presents Pat, the first
programming language design incorporating mailbox types, and describes an
algorithmic type system. We make essential use of quasi-linear typing to tame
some of the complexity introduced by aliasing. Our algorithmic type system is
necessarily co-contextual, achieved through a novel use of backwards
bidirectional typing, and we prove it sound and complete with respect to our
declarative type system. We extend Pat with sums, products and higher-order
functions, and also interfaces that allow finer-grained reasoning about mailbox
contents. We implement a prototype type checker, and use it to demonstrate the
expressiveness of Pat on a factory automation case study and a series of
examples from the Savina actor benchmark suite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised and extended version of paper accepted to ICFP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPDL: Automatic <span class="highlight-title">Prompt</span> Optimization for LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of large language models (LLMs) depends on how they are
prompted, with choices spanning both the high-level prompting pattern (e.g.,
Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and
few-shot demonstrations). Manually tuning this combination is tedious,
error-prone, and specific to a given LLM and task. Therefore, this paper
proposes AutoPDL, an automated approach to discovering good LLM agent
configurations. Our approach frames this as a structured AutoML problem over a
combinatorial space of agentic and non-agentic prompting patterns and
demonstrations, using successive halving to efficiently navigate this space. We
introduce a library implementing common prompting patterns using the PDL prompt
programming language. AutoPDL solutions are human-readable, editable, and
executable PDL programs that use this library. This approach also enables
source-to-source optimization, allowing human-in-the-loop refinement and reuse.
Evaluations across three tasks and seven LLMs (ranging from 3B to 70B
parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points),
up to 67.5pp, and reveal that selected prompting strategies vary across models
and tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRANE: Reasoning with constrained LLM generation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09061v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09061v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debangshu Banerjee, Tarun Suresh, Shubham Ugare, Sasa Misailovic, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation, symbolic math reasoning, and other tasks require LLMs to
produce outputs that are both syntactically and semantically correct.
Constrained LLM generation is a promising direction to enforce adherence to
formal grammar, but prior works have empirically observed that strict
enforcement of formal constraints often diminishes the reasoning capabilities
of LLMs. In this work, we first provide a theoretical explanation for why
constraining LLM outputs to very restrictive grammars that only allow
syntactically valid final answers reduces the reasoning capabilities of the
model. Second, we demonstrate that by augmenting the output grammar with
carefully designed additional rules, it is always possible to preserve the
reasoning capabilities of the LLM while ensuring syntactic and semantic
correctness in its outputs. Building on these theoretical insights, we propose
a reasoning-augmented constrained decoding algorithm, CRANE, which effectively
balances the correctness of constrained generation with the flexibility of
unconstrained generation. Experiments on multiple open-source LLMs and
benchmarks show that CRANE significantly outperforms both state-of-the-art
constrained decoding strategies and standard unconstrained decoding, showing up
to 10% points accuracy improvement over baselines on challenging symbolic
reasoning benchmarks GSM-symbolic and FOLIO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025, Code at:
  https://github.com/uiuc-focal-lab/CRANE</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Data Structures and Algorithms <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vertex-ordering and arc-partitioning problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nóra A. Borsik, Péter Madarasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study vertex-ordering problems in loop-free digraphs subject to
constraints on the left-going arcs, focusing on existence conditions and
computational complexity. As an intriguing special case, we explore
vertex-specific lower and upper bounds on the left-outdegrees and
right-indegrees. We show, for example, that deciding whether the left-going
arcs can form an in-branching is solvable in polynomial time and provide a
necessary and sufficient condition, while the analogous problem for an
in-arborescence turns out to be NP-complete. We also consider a weighted
variant that enforces vertex-specific lower and upper bounds on the weighted
left-outdegrees, which is particularly relevant in applications. Furthermore,
we investigate the connection between ordering problems and their
arc-partitioning counterparts, where one seeks to partition the arcs into a
subgraph from a specific digraph family and an acyclic subgraph --
equivalently, one seeks to cover all directed cycles with a subgraph belonging
to a specific family. For the family of in-branchings, unions of disjoint
dipaths, and matchings, the two formulations coincide, whereas for
in-arborescences, dipaths, Hamiltonian dipaths, and perfect matchings the
formulations diverge. Our results yield a comprehensive complexity landscape,
unify diverse special cases and variants, clarify the algorithmic boundaries of
ordered digraphs, and relate them to broader topics including graph degeneracy,
acyclic orientations, influence propagation, and rank aggregation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ List Decoding Expander-Based Codes via Fast Approximation of Expanding
  CSPs: I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Granha Jeronimo, Aman Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present near-linear time list decoding algorithms (in the block-length
$n$) for expander-based code constructions. More precisely, we show that
  (i) For every $\delta \in (0,1)$ and $\epsilon > 0$, there is an explicit
family of good Tanner LDPC codes of (design) distance $\delta$ that is $(\delta
- \epsilon, O_\varepsilon(1))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size $O_\delta(1)$,
  (ii) For every $R \in (0,1)$ and $\epsilon > 0$, there is an explicit family
of AEL codes of rate $R$, distance $1-R -\varepsilon$ that is $(1-R-\epsilon,
O_\varepsilon(1))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size
$\text{exp}(\text{poly}(1/\epsilon))$, and
  (iii) For every $R \in (0,1)$ and $\epsilon > 0$, there is an explicit family
of AEL codes of rate $R$, distance $1-R-\varepsilon$ that is $(1-R-\epsilon,
O(1/\epsilon))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size
$\text{exp}(\text{exp}(\text{poly}(1/\epsilon)))$ using recent near-optimal
list size bounds from [JMST25].
  Our results are obtained by phrasing the decoding task as an agreement CSP
[RWZ20,DHKNT19] on expander graphs and using the fast approximation algorithm
for $q$-ary expanding CSPs from [Jer23], which is based on weak regularity
decomposition [JST21,FK96]. Similarly to list decoding $q$-ary Ta-Shma's codes
in [Jer23], we show that it suffices to enumerate over assignments that are
constant in each part (of the constantly many) of the decomposition in order to
recover all codewords in the list.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Contractions of Dynamic Graphs -- with Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monika Henzinger, Evangelos Kosinas, Robin Münk, Harald Räcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A non-trivial minimum cut (NMC) sparsifier is a multigraph $\hat{G}$ that
preserves all non-trivial minimum cuts of a given undirected graph $G$. We
introduce a flexible data structure for fully dynamic graphs that can
efficiently provide an NMC sparsifier upon request at any point during the
sequence of updates. We employ simple dynamic forest data structures to achieve
a fast from-scratch construction of the sparsifier at query time. Based on the
strength of the adversary and desired type of time bounds, the data structure
comes with different guarantees. Specifically, let $G$ be a fully dynamic
simple graph with $n$ vertices and minimum degree $\delta$. Then our data
structure supports an insertion/deletion of an edge to/from $G$ in $n^{o(1)}$
worst-case time. Furthermore, upon request, it can return w.h.p. an NMC
sparsifier of $G$ that has $O(n/\delta)$ vertices and $O(n)$ edges, in
$\hat{O}(n)$ time. The probabilistic guarantees hold against an adaptive
adversary. Alternatively, the update and query times can be improved to
$\tilde{O}(1)$ and $\tilde{O}(n)$ respectively, if amortized-time guarantees
are sufficient, or if the adversary is oblivious.
  We discuss two applications of our data structure. First, it can be used to
efficiently report a cactus representation of all minimum cuts of a fully
dynamic simple graph. Using the NMC sparsifier we can w.h.p. build this cactus
in worst-case time $\hat{O}(n)$ against an adaptive adversary. Second, our data
structure allows us to efficiently compute the maximal $k$-edge-connected
subgraphs of undirected simple graphs, by repeatedly applying a minimum cut
algorithm on the NMC sparsifier. Specifically, we can compute w.h.p. the
maximal $k$-edge-connected subgraphs of a simple graph with $n$ vertices and
$m$ edges in $\tilde{O}(m+n^2/k)$ time which is an improvement for $k =
\Omega(n^{1/8})$ and works for fully dynamic graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Depth First Search Numbering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur Czumaj, Christian Sohler, Stefan Walzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Property Testing is a formal framework to study the computational power and
complexity of sampling from combinatorial objects. A central goal in standard
graph property testing is to understand which graph properties are testable
with sublinear query complexity. Here, a graph property P is testable with a
sublinear query complexity if there is an algorithm that makes a sublinear
number of queries to the input graph and accepts with probability at least 2/3,
if the graph has property P, and rejects with probability at least 2/3 if it is
$\varepsilon$-far from every graph that has property P.
  In this paper, we introduce a new variant of the bounded degree graph model.
In this variant, in addition to the standard representation of a bounded degree
graph, we assume that every vertex $v$ has a unique label num$(v)$ from $\{1,
\dots, |V|\}$, and in addition to the standard queries in the bounded degree
graph model, we also allow a property testing algorithm to query for the label
of a vertex (but not for a vertex with a given label).
  Our new model is motivated by certain graph processes such as a DFS
traversal, which assign consecutive numbers (labels) to the vertices of the
graph. We want to study which of these numberings can be tested in sublinear
time. As a first step in understanding such a model, we develop a
\emph{property testing algorithm for discovery times of a DFS traversal} with
query complexity $O(n^{1/3}/\varepsilon)$ and for constant $\varepsilon>0$ we
give a matching lower bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Exact Resistance Distance Computation on Small-Treewidth
  Graphs: a Labelling Approach <span class="chip">SIGMOD 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meihao Liao, Yueyang Pan, Rong-Hua Li, Guoren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resistance distance computation is a fundamental problem in graph analysis,
yet existing random walk-based methods are limited to approximate solutions and
suffer from poor efficiency on small-treewidth graphs (e.g., road networks). In
contrast, shortest-path distance computation achieves remarkable efficiency on
such graphs by leveraging cut properties and tree decompositions. Motivated by
this disparity, we first analyze the cut property of resistance distance. While
a direct generalization proves impractical due to costly matrix operations, we
overcome this limitation by integrating tree decompositions, revealing that the
resistance distance $r(s,t)$ depends only on labels along the paths from $s$
and $t$ to the root of the decomposition. This insight enables compact
labelling structures. Based on this, we propose \treeindex, a novel index
method that constructs a resistance distance labelling of size $O(n \cdot
h_{\mathcal{G}})$ in $O(n \cdot h_{\mathcal{G}}^2 \cdot d_{\max})$ time, where
$h_{\mathcal{G}}$ (tree height) and $d_{\max}$ (maximum degree) behave as small
constants in many real-world small-treewidth graphs (e.g., road networks). Our
labelling supports exact single-pair queries in $O(h_{\mathcal{G}})$ time and
single-source queries in $O(n \cdot h_{\mathcal{G}})$ time. Extensive
experiments show that TreeIndex substantially outperforms state-of-the-art
approaches. For instance, on the full USA road network, it constructs a $405$
GB labelling in $7$ hours (single-threaded) and answers exact single-pair
queries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the
first exact method scalable to such large graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGMOD 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Bounds for Twin-Width Parameter Variants with Algorithmic
  Applications to Counting Graph Colorings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambroise Baril, Miguel Couceiro, Victor Lagerkvist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The $H$-Coloring problem is a well-known generalization of the classical
NP-complete problem $k$-Coloring where the task is to determine whether an
input graph admits a homomorphism to the template graph $H$. This problem has
been the subject of intense theoretical research and in this article we study
the complexity of $H$-Coloring with respect to the parameters clique-width and
the more recent component twin-width, which describe desirable computational
properties of graphs. We give two surprising linear bounds between these
parameters, thus improving the previously known exponential and double
exponential bounds. Our constructive proof naturally extends to related
parameters and as a showcase we prove that total twin-width and linear
clique-width can be related via a tight quadratic bound. These bounds naturally
lead to algorithmic applications. The linear bounds between component
twin-width and clique-width entail natural approximations of component
twin-width, by making use of the results known for clique-width. As for
computational aspects of graph coloring, we target the richer problem of
counting the number of homomorphisms to $H$ (#$H$-Coloring). The first
algorithm that we propose uses a contraction sequence of the input graph $G$
parameterized by the component twin-width of $G$. This leads to a positive FPT
result for the counting version. The second uses a contraction sequence of the
template graph $H$ and here we instead measure the complexity with respect to
the number of vertices in the input graph. Using our linear bounds we show that
our algorithms are always at least as fast as the previously best #$H$-Coloring
algorithms (based on clique-width) and for several interesting classes of
graphs (e.g., cographs, cycles of length $\ge 7$, or distance-hereditary
graphs) are in fact strictly faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing an Invisible Robber using Separators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Potapov, Tymofii Prokopenko, John Sylvester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the zero-visibility cops and robbers game, where the robber is
invisible to the cops until they are caught. This differs from the classic game
where full information about the robber's location is known at any time. A
previously known solution for capturing a robber in the zero-visibility case is
based on the pathwidth decomposition. We provide an alternative solution based
on a separation hierarchy, improving capture time and space complexity without
asymptotically increasing the zero-visibility cop number in most cases. In
addition, we provide a better bound on the approximate zero-visibility cop
number for various classes of graphs, where approximate refers to the
restriction to polynomial time computable strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On approximating the $f$-divergence between two Ising models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Feng, Yucheng Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The $f$-divergence is a fundamental notion that measures the difference
between two distributions. In this paper, we study the problem of approximating
the $f$-divergence between two Ising models, which is a generalization of
recent work on approximating the TV-distance. Given two Ising models $\nu$ and
$\mu$, which are specified by their interaction matrices and external fields,
the problem is to approximate the $f$-divergence $D_f(\nu\,\|\,\mu)$ within an
arbitrary relative error $\mathrm{e}^{\pm \varepsilon}$. For
$\chi^\alpha$-divergence with a constant integer $\alpha$, we establish both
algorithmic and hardness results. The algorithm works in a parameter regime
that matches the hardness result. Our algorithm can be extended to other
$f$-divergences such as $\alpha$-divergence, Kullback-Leibler divergence,
R\'enyi divergence, Jensen-Shannon divergence, and squared Hellinger distance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Reconstruction with a Connected Components Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.05002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.05002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juha Harviainen, Pekka Parviainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Graph Reconstruction (GR) problem, the goal is to recover a hidden
graph by utilizing some oracle that provides limited access to the structure of
the graph. The interest is in characterizing how strong different oracles are
when the complexity of an algorithm is measured in the number of performed
queries. We study a novel oracle that returns the set of connected components
(CC) on the subgraph induced by the queried subset of vertices. Our main
contributions are as follows:
  1. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$,
and treewidth $k$, GR can be solved in $O(\min\{m, \Delta^2, k^2\} \cdot \log
n)$ CC queries by an adaptive randomized algorithm.
  2. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$,
and treewidth $k$, no algorithm can solve GR in $o(\min\{m, \Delta^2, k^2\})$
CC queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameterized Approximability for Modular Linear Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konrad K. Dabrowski, Peter Jonsson, Sebastian Ordyniak, George Osipov, Magnus Wahlström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the Min-$r$-Lin$(Z_m)$ problem: given a system $S$ of length-$r$
linear equations modulo $m$, find $Z \subseteq S$ of minimum cardinality such
that $S-Z$ is satisfiable. The problem is NP-hard and UGC-hard to approximate
in polynomial time within any constant factor even when $r = m = 2$. We focus
on parameterized approximation with solution size as the parameter. Dabrowski
et al. showed that Min-$2$-Lin$(Z_m)$ is in FPT if $m$ is prime (i.e. $Z_m$ is
a field), and it is W[1]-hard if $m$ is not a prime power. We show that
Min-$2$-Lin$(Z_{p^n})$ is FPT-approximable within a factor of $2$ for every
prime $p$ and integer $n \geq 2$. This implies that Min-$2$-Lin$(Z_m)$, $m \in
Z^+$, is FPT-approximable within a factor of $2\omega(m)$ where $\omega(m)$
counts the number of distinct prime divisors of $m$. The idea behind the
algorithm is to solve ever tighter relaxations of the problem, decreasing the
set of possible values for the variables at each step. Working over $Z_{p^n}$
and viewing the values in base-$p$, one can roughly think of a relaxation as
fixing the number of trailing zeros and the least significant nonzero digits of
the values assigned to the variables. To solve the relaxed problem, we
construct a certain graph where solutions can be identified with a particular
collection of cuts. The relaxation may hide obstructions that will only become
visible in the next iteration of the algorithm, which makes it difficult to
find optimal solutions. To deal with this, we use a strategy based on shadow
removal to compute solutions that (1) cost at most twice as much as the optimum
and (2) allow us to reduce the set of values for all variables simultaneously.
We complement the algorithmic result with two lower bounds, ruling out
constant-factor FPT-approximation for Min-$3$-Lin$(R)$ over any nontrivial ring
$R$ and for Min-$2$-Lin$(R)$ over some finite commutative rings $R$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2410.09932</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Variance and Covariance Estimation under Differential Privacy in
  the Add-Remove Model and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shokichi Takakura, Seng Pei Liew, Satoshi Hasegawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of estimating the variance and covariance
of datasets under differential privacy in the add-remove model. While
estimation in the swap model has been extensively studied in the literature,
the add-remove model remains less explored and more challenging, as the dataset
size must also be kept private. To address this issue, we develop efficient
mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier
mechanism}, a novel moment-release framework that leverages Bernstein bases. We
prove that our proposed mechanisms are minimax optimal in the high-privacy
regime by establishing new minimax lower bounds. Moreover, beyond worst-case
scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based
estimator consistently achieves better utility compared to alternative
mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier
mechanism beyond variance and covariance estimation, showcasing its
applicability to other statistical tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality control in sublinear time: a case study via random graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.16531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.16531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cassandra Marcussen, Ronitt Rubinfeld, Madhu Sudan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many algorithms are designed to work well on average over inputs. When
running such an algorithm on an arbitrary input, we must ask: Can we trust the
algorithm on this input? We identify a new class of algorithmic problems
addressing this, which we call "Quality Control Problems." These problems are
specified by a (positive, real-valued) "quality function" $\rho$ and a
distribution $D$ such that, with high probability, a sample drawn from $D$ is
"high quality," meaning its $\rho$-value is near $1$. The goal is to accept
inputs $x \sim D$ and reject potentially adversarially generated inputs $x$
with $\rho(x)$ far from $1$. The objective of quality control is thus weaker
than either component problem: testing for "$\rho(x) \approx 1$" or testing if
$x \sim D$, and offers the possibility of more efficient algorithms.
  In this work, we consider the sublinear version of the quality control
problem, where $D \in \Delta(\{0,1\}^N)$ and the goal is to solve the $(D
,\rho)$-quality problem with $o(N)$ queries and time. As a case study, we
consider random graphs, i.e., $D = G_{n,p}$ (and $N = \binom{n}2$), and the
$k$-clique count function $\rho_k := C_k(G)/\mathbb{E}_{G' \sim
G_{n,p}}[C_k(G')]$, where $C_k(G)$ is the number of $k$-cliques in $G$. Testing
if $G \sim G_{n,p}$ with one sample, let alone with sublinear query access to
the sample, is of course impossible. Testing if $\rho_k(G)\approx 1$ requires
$p^{-\Omega(k^2)}$ samples. In contrast, we show that the quality control
problem for $G_{n,p}$ (with $n \geq p^{-ck}$ for some constant $c$) with
respect to $\rho_k$ can be tested with $p^{-O(k)}$ queries and time, showing
quality control is provably superpolynomially more efficient in this setting.
More generally, for a motif $H$ of maximum degree $\Delta(H)$, the respective
quality control problem can be solved with $p^{-O(\Delta(H))}$ queries and
running time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>70 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Broader Landscape of Robustness in Algorithmic Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02670v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02670v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gautam Kamath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The last decade has seen a number of advances in computationally efficient
algorithms for statistical methods subject to robustness constraints. An
estimator may be robust in a number of different ways: to contamination of the
dataset, to heavy-tailed data, or in the sense that it preserves privacy of the
dataset. We survey recent results in these areas with a focus on the problem of
mean estimation, drawing technical and conceptual connections between the
various forms of robustness, showing that the same underlying algorithmic ideas
lead to computationally efficient estimators in all these settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE BITS the Information Theory Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prophet Inequalities over Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Abels, Elias Pitschmann, Daniel Schmand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an over-time variant of the well-known prophet
inequality with i.i.d. random variables. Instead of stopping with one realized
value at some point in the process, we decide for each step how long we select
the value. Then we cannot select another value until this period is over. The
goal is to maximize the expectation of the sum of selected values. We describe
the structure of the optimal stopping rule and give upper and lower bounds on
the prophet inequality. In online algorithms terminology, this corresponds to
bounds on the competitive ratio of an online algorithm.
  We give a surprisingly simple algorithm with a single threshold that results
in a prophet inequality of $\approx 0.396$ for all input lengths $n$.
Additionally, as our main result, we present a more advanced algorithm
resulting in a prophet inequality of $\approx 0.598$ when the number of steps
tends to infinity. We complement our results by an upper bound that shows that
the best possible prophet inequality is at most $1/\varphi \approx 0.618$,
where $\varphi$ denotes the golden ratio.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-09-13T04:24:24.187282652Z">
            2025-09-13 04:24:24 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
